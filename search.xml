<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2022云计算国赛容器云</title>
    <url>/2023/04/03/2022%E4%BA%91%E8%AE%A1%E7%AE%97%E5%9B%BD%E8%B5%9B%E5%AE%B9%E5%99%A8%E4%BA%91/</url>
    <content><![CDATA[<h1 id="2022-年全国职业院校技能大赛高职组云计算赛项试卷"><a href="#2022-年全国职业院校技能大赛高职组云计算赛项试卷" class="headerlink" title="2022 年全国职业院校技能大赛高职组云计算赛项试卷"></a>2022 年全国职业院校技能大赛高职组云计算赛项试卷<span id="more"></span></h1><h2 id="云计算赛项第二场-容器云"><a href="#云计算赛项第二场-容器云" class="headerlink" title="云计算赛项第二场-容器云"></a>云计算赛项第二场-容器云</h2><h3 id="【任务-1】容器云平台搭建-5-分"><a href="#【任务-1】容器云平台搭建-5-分" class="headerlink" title="【任务 1】容器云平台搭建[5 分]"></a>【任务 1】容器云平台搭建[5 分]</h3><p>【适用平台】私有云</p>
<h4 id="【题目-1】平台部署–部署容器云平台-1-5-分"><a href="#【题目-1】平台部署–部署容器云平台-1-5-分" class="headerlink" title="【题目 1】平台部署–部署容器云平台[1.5 分]"></a>【题目 1】平台部署–部署容器云平台[1.5 分]</h4><p>登录 OpenStack 私有云平台，使用 CentOS7.9 镜像创建两台云主机，使用 kubeeasy 工具完成 Kubernetes 1.22.1 集群的搭建。然后使用 nginx 镜像在 default 命名空间下创建一个名为exam 的 Pod，并为该 Pod 设置环境变量 exam，其值为 2022。完成后提交 master 节点的用户名、密码和 IP 到答题框。</p>
<p>1.Kubernetes 集群部署成功得 1 分；<br>2.Pod 创建成功且环境变量设置正确得 0.5 分。</p>
<h4 id="【题目-2】平台部署–部署-Istio-服务网格-0-5-分"><a href="#【题目-2】平台部署–部署-Istio-服务网格-0-5-分" class="headerlink" title="【题目 2】平台部署–部署 Istio 服务网格[0.5 分]"></a>【题目 2】平台部署–部署 Istio 服务网格[0.5 分]</h4><p>在 Kubernetes 集群上完成 Istio 服务网格环境的安装，然后新建命名空间 exam，为该命名空间开启自动注入 Sidecar。完成后提交 master 节点的用户名、密码和 IP 到答题框。<br>1.Istio 所有组件运行成功得 0.3 分；<br>2.命名空间 exam 自动注入成功得 0.2 分。</p>
<h4 id="【题目-3】平台部署–部署-KubeVirt-虚拟化-1-分"><a href="#【题目-3】平台部署–部署-KubeVirt-虚拟化-1-分" class="headerlink" title="【题目 3】平台部署–部署 KubeVirt 虚拟化[1 分]"></a>【题目 3】平台部署–部署 KubeVirt 虚拟化[1 分]</h4><p>在 Kubernetes 集群上完成 KubeVirt 虚拟化环境的安装。完成后提交 master 节点的用户名、密码和 IP 到答题框。<br>1.KubeVirt 所有组件运行成功得 1 分。</p>
<h4 id="【题目-4】平台部署–部署-Harbor-仓库及-Helm-包管理工具-1-分"><a href="#【题目-4】平台部署–部署-Harbor-仓库及-Helm-包管理工具-1-分" class="headerlink" title="【题目 4】平台部署–部署 Harbor 仓库及 Helm 包管理工具[1 分]"></a>【题目 4】平台部署–部署 Harbor 仓库及 Helm 包管理工具[1 分]</h4><p>在 master 节点上完成 Harbor 镜像仓库及 Helm 包管理工具的部署。然后使用 nginx 镜像自定义一个 Chart，Deployment 名称为 nginx，副本数为 1，然后将该 Chart 部署到 default命名空间下，Release 名称为 web。<br>完成后提交 master 节点的用户名、密码和 IP 到答题框。<br>1.Harbor 仓库部署成功得 0.5 分；<br>2.Helm 工具安装成功得 0.2 分；<br>3.Chart 包部署成功得 0.3 分。</p>
<h4 id="【题目-5】集群管理–备份-ETCD-数据-1-分"><a href="#【题目-5】集群管理–备份-ETCD-数据-1-分" class="headerlink" title="【题目 5】集群管理–备份 ETCD 数据[1 分]"></a>【题目 5】集群管理–备份 ETCD 数据[1 分]</h4><p>Kubernetes 使用 ETCD 来存储集群的实时运行数据，为防止服务器宕机导致 Kubernetes集群数据丢失，请将 Kubernetes 集群数据备份到&#x2F;root&#x2F;etcd.db 中。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.etcdctl 工具安装成功得 0.2 分；<br>2.ETCD 数据备份成功得 0.8 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# ETCDCTL_API=3 /usr/local/bin/etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=&quot;https://127.0.0.1:2379&quot; snapshot save /root/etcd.db</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1681819860.8046362,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:68&quot;,&quot;msg&quot;:&quot;created temporary db file&quot;,&quot;path&quot;:&quot;/root/etcd.db.part&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1681819860.8216271,&quot;logger&quot;:&quot;client&quot;,&quot;caller&quot;:&quot;v3/maintenance.go:211&quot;,&quot;msg&quot;:&quot;opened snapshot stream; downloading&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1681819860.8217337,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:76&quot;,&quot;msg&quot;:&quot;fetching snapshot&quot;,&quot;endpoint&quot;:&quot;https://127.0.0.1:2379&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1681819860.8757417,&quot;logger&quot;:&quot;client&quot;,&quot;caller&quot;:&quot;v3/maintenance.go:219&quot;,&quot;msg&quot;:&quot;completed snapshot read; closing&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1681819861.1202412,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:91&quot;,&quot;msg&quot;:&quot;fetched snapshot&quot;,&quot;endpoint&quot;:&quot;https://127.0.0.1:2379&quot;,&quot;size&quot;:&quot;3.0 MB&quot;,&quot;took&quot;:&quot;now&quot;&#125;</span><br><span class="line">&#123;&quot;level&quot;:&quot;info&quot;,&quot;ts&quot;:1681819861.1204019,&quot;caller&quot;:&quot;snapshot/v3_snapshot.go:100&quot;,&quot;msg&quot;:&quot;saved&quot;,&quot;path&quot;:&quot;/root/etcd.db&quot;&#125;</span><br><span class="line">Snapshot saved at /root/etcd.db</span><br></pre></td></tr></table></figure>



<h3 id="【任务-2】容器云应用部署：Docker-Compose-编排部署-7-0-分"><a href="#【任务-2】容器云应用部署：Docker-Compose-编排部署-7-0-分" class="headerlink" title="【任务 2】容器云应用部署：Docker Compose 编排部署[7.0 分]"></a>【任务 2】容器云应用部署：Docker Compose 编排部署[7.0 分]</h3><p>【适用平台】私有云<br>Hyperf 是一个高性能、高灵活性的渐进式 PHP 协程框架，内置协程服务器及大量常用的组件，性能较传统基于 PHP-FPM 的框架有质的提升，提供超高性能的同时，也保持着极其灵活的可扩展性。请根据要求完成数据库服务 MariaDB、缓存服务 Redis、微服务 Hyperf及前端服务 Nginx 按照要求进行容器化。</p>
<h4 id="【题目-1】容器化-MariaDB-服务-1-分"><a href="#【题目-1】容器化-MariaDB-服务-1-分" class="headerlink" title="【题目 1】容器化 MariaDB 服务[1 分]"></a>【题目 1】容器化 MariaDB 服务[1 分]</h4><p>编写 Dockerfile 文件构建 hyperf-mariadb:v1.0 镜像，具体要求如下：（需要用到的软件包：<br>Hyperf.tar.gz）<br>（1）基础镜像：centos:7.9.2009；<br>（2）完成 MariaDB 服务的安装；<br>（3）声明端口：3306；<br>（4）设置数据库 root 用户的密码为 root；<br>（5）将提供的数据库文件 hyperf_admin.sql 导入数据库；<br>（6）设置服务开机自启。<br>完成后构建镜像，并提交 master 节点的用户名、密码和 IP 地址到答题框。<br>1.镜像构建成功得 0.5 分；<br>2.数据库安装且导入数据成功得 0.5 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master Hyperf]# cat init.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">#echo &quot;[mysqld]\ncharacter-set-server=utf8&quot; &gt; /etc/my.cnf</span><br><span class="line">mysql_install_db --user=root</span><br><span class="line">mysqld_safe --user=root &amp;</span><br><span class="line">sleep 5</span><br><span class="line">mysqladmin -u root password &#x27;root&#x27;</span><br><span class="line">mysql -uroot -proot -e &quot;grant all privileges on *.* to root@&#x27;%&#x27; identified by &#x27;root&#x27;;&quot;</span><br><span class="line">mysql -uroot -proot -e &quot;source /opt/hyperf_admin.sql;&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master Hyperf]# cat Dockerfile-mariadb </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">MAINTAINER chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD yum /opt/yum</span><br><span class="line">COPY local.repo /etc/yum.repos.d/</span><br><span class="line">RUN  yum install -y mariadb-server mariadb</span><br><span class="line">COPY sql/hyperf_admin.sql /opt/</span><br><span class="line">COPY init.sh /root</span><br><span class="line">RUN sh /root/init.sh</span><br><span class="line">EXPOSE 3306</span><br><span class="line">CMD [&quot;mysqld_safe&quot;,&quot;--user=root&quot;]</span><br></pre></td></tr></table></figure>



<h4 id="【题目-2】容器化-Redis-服务-1-分"><a href="#【题目-2】容器化-Redis-服务-1-分" class="headerlink" title="【题目 2】容器化 Redis 服务[1 分]"></a>【题目 2】容器化 Redis 服务[1 分]</h4><p>编写 Dockerfile 文件构建 hyperf-redis:v1.0 镜像，具体要求如下：（需要用到的软件包：<br>Hyperf.tar.gz）<br>（1）基础镜像：centos:7.9.2009；<br>（2）安装 Redis 服务；<br>（3）关闭保护模式；<br>（4）声明端口：6379；<br>（5）设置服务开机自启。<br>完成后构建镜像，并提交 master 节点的用户名、密码和 IP 地址到答题框。<br>1.镜像构建成功的 0.5 分；<br>2.Redis 服务安装成功且配置正确得 0.5 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master Hyperf]# cat Dockerfile-redis </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">MAINTAINER chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">COPY local.repo /etc/yum.repos.d/</span><br><span class="line">ADD yum /opt/yum</span><br><span class="line">RUN yum install -y redis</span><br><span class="line">RUN sed -i &quot;s/bind 127.0.0.1/bind 0.0.0.0/g&quot; /etc/redis.conf</span><br><span class="line">RUN sed -i &quot;s/protected-mode yes/protected-mode no/g&quot; /etc/redis.conf</span><br><span class="line">EXPOSE 6379</span><br><span class="line">CMD [&quot;redis-server&quot;,&quot;/etc/redis.conf&quot;]</span><br></pre></td></tr></table></figure>

<h4 id="【题目-3】容器化-Nginx-服务-0-5-分"><a href="#【题目-3】容器化-Nginx-服务-0-5-分" class="headerlink" title="【题目 3】容器化 Nginx 服务[0.5 分]"></a>【题目 3】容器化 Nginx 服务[0.5 分]</h4><p>编写 Dockerfile 文件构建 hyperf-nginx:v1.0 镜像，具体要求如下：（需要用到的软件包：<br>Hyperf.tar.gz）<br>（1）基础镜像：centos:7.9.2009；<br>（2）安装 nginx 服务；<br>（3）声明端口：80；<br>（4）设置服务开机自启。<br>完成后构建镜像，并提交 master 节点的用户名、密码和 IP 地址到答题框。<br>1.镜像构建成功得 0.3 分；<br>2.Nginx 安装成功且配置正确得 0.2 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master Hyperf]# cat Dockerfile-nginx </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">MAINTAINER chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">COPY local.repo /etc/yum.repos.d/</span><br><span class="line">ADD yum /opt/yum</span><br><span class="line">RUN yum install -y nginx</span><br><span class="line">COPY project/frontend /var/www/frontend</span><br><span class="line">COPY project/docker/conf.d/hyperf-admin.conf /etc/nginx/conf.d/default.conf</span><br><span class="line">EXPOSE 80 443</span><br><span class="line">CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]</span><br></pre></td></tr></table></figure>



<h4 id="【题目-4】容器化-Hyperf-服务-1-分"><a href="#【题目-4】容器化-Hyperf-服务-1-分" class="headerlink" title="【题目 4】容器化 Hyperf 服务[1 分]"></a>【题目 4】容器化 Hyperf 服务[1 分]</h4><p>编写 Dockerfile 文件构建 hyperf-service:v1.0 镜像，具体要求如下：（需要用到的软件包：<br>Hyperf.tar.gz）<br>（1）基础镜像：centos:7.9.2009；<br>（2）安装 PHP 及扩展；</p>
<p>（3）使用源码编译安装 Swoole。<br>完成后构建镜像，并提交 master 节点的用户名、密码和 IP 地址到答题框。<br>1.镜像构建成功得 0.5 分；<br>2.PHP 安装成功得 0.2 分；<br>3.Swoole 安装成功得 0.3 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master Hyperf]# cat Dockerfile-hyperf </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">MAINTAINER chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">COPY local.repo /etc/yum.repos.d/</span><br><span class="line">ADD yum /opt/yum</span><br><span class="line">RUN ls /opt/</span><br><span class="line">COPY swoole-v4.8.3.zip /opt/</span><br><span class="line">ADD project/backend/ /opt</span><br><span class="line">RUN yum install -y php php-cli php-fpm php-devel php-gd php-mysqlnd </span><br><span class="line">RUN yum install -y unzip gcc make</span><br><span class="line">WORKDIR /opt</span><br><span class="line">RUN unzip /opt/swoole-v4.8.3.zip &amp;&amp; cd /opt/swoole-v4.8.3 &amp;&amp; phpize &amp;&amp; ./configure &amp;&amp; make &amp;&amp; make install</span><br><span class="line">RUN echo &quot;[swoole]&quot; &gt;&gt; /etc/php.ini &amp;&amp;  \</span><br><span class="line">    echo &quot;extension=swoole.so&quot; &gt;&gt; /etc/php.ini &amp;&amp;  \</span><br><span class="line">    echo &quot;swoole.use_shortname=&#x27;Off&#x27;&quot; &gt;&gt; /etc/php.ini</span><br><span class="line">EXPOSE 9511</span><br><span class="line">RUN chmod +x bin/hyperf.php</span><br><span class="line">CMD php --ri swoole &amp;&amp; tail -f /etc/shadow</span><br></pre></td></tr></table></figure>



<h4 id="【题目-5】编排部署-Hyperf-框架-3-5-分"><a href="#【题目-5】编排部署-Hyperf-框架-3-5-分" class="headerlink" title="【题目 5】编排部署 Hyperf 框架[3.5 分]"></a>【题目 5】编排部署 Hyperf 框架[3.5 分]</h4><p>编写&#x2F;root&#x2F;hyperf&#x2F;project&#x2F;docker-compose.yaml 文件，具体要求如下：<br>（1）容器 1 名称：hyperf-mysql；镜像：hyperf-mariadb:v1.0；端口映射：3306:3306；<br>（2）容器 2 名称：hyperf-redis；镜像：hyperf-redis:v1.0；<br>（3）容器 3 名称：hyperf-ui；镜像：hyperf-nginx:v1.0；端口映射：80:8081；<br>（4）容器 4 名称：hyperf-service；镜像：hyperf-service:v1.0。<br>完成后编排部署 Hyperf 框架，并提交 master 节点的用户名、密码和 IP 地址到答题框。<br>1.docker-compose.yaml 文件编排成功得 1.5 分；<br>2.8081 端口访问服务成功得 1 分；<br>3.Hyperf-service 连接数据库和 Redis 成功得 1 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master Hyperf]# cat docker-compose.yaml </span><br><span class="line">version: &#x27;3.1&#x27;</span><br><span class="line">services:</span><br><span class="line">  redis:</span><br><span class="line">    image: hyperf-redis:v1.0</span><br><span class="line">    container_name: hyperf-redis</span><br><span class="line">  mysql:</span><br><span class="line">    container_name: mysql-test</span><br><span class="line">    image: hyperf-mariadb:v1.0</span><br><span class="line"></span><br><span class="line">    ports:</span><br><span class="line">      - 3306:3306</span><br><span class="line"></span><br><span class="line">  app:</span><br><span class="line">    container_name: hyperf-service</span><br><span class="line">    image: hyperf-hyperf:v1.0</span><br><span class="line">    depends_on:</span><br><span class="line">      - mysql</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9511:9511&quot;</span><br><span class="line">    links:</span><br><span class="line">      - mysql:mysql</span><br><span class="line">    command:</span><br><span class="line">      - sh</span><br><span class="line">      - -c</span><br><span class="line">      - &quot;/opt/bin/hyperf.php start&quot;</span><br><span class="line"></span><br><span class="line">  nginx:</span><br><span class="line">    image: hyperf-nginx:v1.0</span><br><span class="line">    container_name: hyperf-ui</span><br><span class="line">    restart: always</span><br><span class="line">    ports:</span><br><span class="line">      - 8081:80</span><br><span class="line">    links:</span><br><span class="line">      - app</span><br><span class="line">      - mysql:mysql</span><br></pre></td></tr></table></figure>

<h3 id="【任务-3】容器云应用部署：基于-Kubernetes-构建-CICD-8-0-分"><a href="#【任务-3】容器云应用部署：基于-Kubernetes-构建-CICD-8-0-分" class="headerlink" title="【任务 3】容器云应用部署：基于 Kubernetes 构建 CICD[8.0 分]"></a>【任务 3】容器云应用部署：基于 Kubernetes 构建 CICD[8.0 分]</h3><p>该公司决定采用 Kubernetes + GitLab CI 来构建 CICD 环境，以缩短新功能开发上线周期，及时满足客户的需求，实现 DevOps 的部分流程，来减轻部署运维的负担，实现可视化容器生命周期管理、应用发布和版本迭代更新，请完成 GitLab CI + Kubernetes 的 CICD 环境部署（构建持续集成所需要的所有软件包在软件包 CICD-Runner.tar.gz 中）。CICD 应用系统架构如下：</p>
<h4 id="【题目-1】安装-GitLab-环境-1-分"><a href="#【题目-1】安装-GitLab-环境-1-分" class="headerlink" title="【题目 1】安装 GitLab 环境[1 分]"></a>【题目 1】安装 GitLab 环境[1 分]</h4><p>在 Kubernetes 集群中新建命名空间 gitlab-ci，将 GitLab 部署到该命名空间下，Deployment和 Service 名称均为 gitlab，以 NodePort 方式将 80 端口对外暴露为 30880，设置 GitLab 服务root 用户的密码为 admin@123，将项目包 demo-2048.tar.gz 导入到 GitLab 中并命名为demo-2048。完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）<br>1.GitLab 部署正确且能正常访问得 0.5 分；<br>2.项目导入成功得 0.5 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat gitlab.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: gitlab</span><br><span class="line">  name: gitlab</span><br><span class="line">  namespace: gitlab-ci</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: gitlab</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: gitlab</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: gitlab/gitlab-ce:12.9.2-ce.0</span><br><span class="line">        env:</span><br><span class="line">        - name: GITLAB_ROOT_PASSWORD</span><br><span class="line">          value: admin@123</span><br><span class="line">        name: gitlab</span><br><span class="line">        ports: </span><br><span class="line">        - containerPort: 443</span><br><span class="line">          name: gitlab443</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: gitlab80</span><br><span class="line">        - containerPort: 22</span><br><span class="line">          name: gitlab22</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: gitlab-config</span><br><span class="line">          mountPath: /etc/gitlab</span><br><span class="line">        - name: gitlab-logs</span><br><span class="line">          mountPath: /var/log/gitlab</span><br><span class="line">        - name: gitlab-data</span><br><span class="line">          mountPath: /var/opt/gitlab</span><br><span class="line">      volumes:</span><br><span class="line">      - name: gitlab-config</span><br><span class="line">        nfs:</span><br><span class="line">          server: 192.168.20.147</span><br><span class="line">          path: /root/nfs-gitlab/config</span><br><span class="line">      - name: gitlab-logs</span><br><span class="line">        nfs:</span><br><span class="line">          server: 192.168.20.147</span><br><span class="line">          path: /root/nfs-gitlab/logs</span><br><span class="line">      - name: gitlab-data</span><br><span class="line">        nfs:</span><br><span class="line">          server: 192.168.20.147</span><br><span class="line">          path: /root/nfs-gitlab/data</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: gitlab</span><br><span class="line">  namespace: gitlab-ci</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">  - port: 443</span><br><span class="line">    targetPort: 443</span><br><span class="line">    name: gitlab443</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">    nodePort: 30888</span><br><span class="line">    name: gitlab80</span><br><span class="line">  - port: 22</span><br><span class="line">    targetPort: 22</span><br><span class="line">    name: gitlab22</span><br><span class="line">      selector:</span><br><span class="line">    app: gitlab</span><br></pre></td></tr></table></figure>



<h4 id="【题目-2】部署-GitLab-Runner-2-分"><a href="#【题目-2】部署-GitLab-Runner-2-分" class="headerlink" title="【题目 2】部署 GitLab Runner[2 分]"></a>【题目 2】部署 GitLab Runner[2 分]</h4><p>将 GitLab Runner 部署到 gitlab-ci 命名空间下，Release 名称为 gitlab-runner，为 GitLabRunner 创建持久化构建缓存目录&#x2F;home&#x2F;gitlab-runner&#x2F;ci-build-cache 以加速构建速度，并将其注册到 GitLab 中。完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）<br>1.GitLab Runner 部署成功得 0.5 分；<br>2.GitLab Runner 注册成功得 0.5 分；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat gitlab-runner.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: gitlab-runner</span><br><span class="line">  name: gitlab-runner</span><br><span class="line">  namespace: gitlab-ci</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: gitlab-runner</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: gitlab-runner</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: gitlab/gitlab-runner:latest</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: gitlab-runner</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: gitlab-config</span><br><span class="line">          mountPath: /etc/gitlab-runner</span><br><span class="line">      volumes:</span><br><span class="line">      - name: gitlab-config</span><br><span class="line">        nfs:</span><br><span class="line">          server: 192.168.20.147</span><br><span class="line">          path: /root/nfs-gitlab/runner-config</span><br></pre></td></tr></table></figure>



<h4 id="【题目-3】配置-GitLab-1-5-分"><a href="#【题目-3】配置-GitLab-1-5-分" class="headerlink" title="【题目 3】配置 GitLab[1.5 分]"></a>【题目 3】配置 GitLab[1.5 分]</h4><p>将 Kubernetes 集群添加到 demo-2048 项目中，并命名为 kubernetes-agent，项目命名空间选择 gitlab-ci。完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）<br>1.GitLab Agent 安装成功得 1 分；<br>2.Kubernetes 连接成功得 0.5 分。</p>
<h4 id="【题目-4】构建-CICD-3-5-分"><a href="#【题目-4】构建-CICD-3-5-分" class="headerlink" title="【题目 4】构建 CICD[3.5 分]"></a>【题目 4】构建 CICD[3.5 分]</h4><p>编写流水线脚本.gitlab-ci.yml 触发自动构建，具体要求如下：<br>（1）基于镜像 maven:3.6-jdk-8 构建项目的 drone 分支；<br>（2）构建镜像的名称：demo:latest；<br>（3）将镜像推送到 Harbor 仓库 demo 项目中；<br>（4）将 demo-2048 应用自动发布到 Kubernetes 集群 gitlab-ci 命名空间下。<br>完成后提交 master 节点的用户名、密码和 IP 地址到答题框。（需要用到的软件包路径CICD-Runner.tar.gz）<br>1.项目变异成功得 0.5 分；<br>2.镜像构建成功得 1 分；<br>3.服务发布成功得 1 分；<br>4.服务能正常访问得 1 分。</p>
<h3 id="【任务-4】容器云服务运维：Kubernetes-基于容器的运维-6-分"><a href="#【任务-4】容器云服务运维：Kubernetes-基于容器的运维-6-分" class="headerlink" title="【任务 4】容器云服务运维：Kubernetes 基于容器的运维[6 分]"></a>【任务 4】容器云服务运维：Kubernetes 基于容器的运维[6 分]</h3><p>【适用平台】私有云</p>
<h4 id="【题目-1】Pod-管理–创建-Pod-0-5-分"><a href="#【题目-1】Pod-管理–创建-Pod-0-5-分" class="headerlink" title="【题目 1】Pod 管理–创建 Pod[0.5 分]"></a>【题目 1】Pod 管理–创建 Pod[0.5 分]</h4><p>在 default 命名空间下使用 nginx:latest 镜像创建一个 QoS 类为 Guaranteed 的 Pod，名称为 qos-demo。<br>完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.Pod 创建成功得 0.2 分；<br>2.Pod QoS 类型为 Guaranteed 得 0.3 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat nginx.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    run: qos-demo</span><br><span class="line">  name: qos-demo</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: nginx:latest</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: qos-demo</span><br><span class="line">    resources: </span><br><span class="line">      requests:</span><br><span class="line">        memory: 200Mi</span><br><span class="line">        cpu: 700m</span><br><span class="line">      limits:</span><br><span class="line">        memory: 200Mi</span><br><span class="line">        cpu: 700m</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# kubectl get pods qos-demo --output=yaml | grep qosClass</span><br><span class="line">  qosClass: Guaranteed</span><br></pre></td></tr></table></figure>

<blockquote>
<p><em>对于 QoS 类为 <code>Guaranteed</code> 的 Pod：</em></p>
<ul>
<li><em>Pod 中的每个容器都必须指定内存限制和内存请求。</em></li>
<li><em>对于 Pod 中的每个容器，内存限制必须等于内存请求。</em></li>
<li><em>Pod 中的每个容器都必须指定 CPU 限制和 CPU 请求。</em></li>
<li><em>对于 Pod 中的每个容器，CPU 限制必须等于 CPU 请求。</em></li>
</ul>
<p><em>如果满足下面条件，Kubernetes 将会指定 Pod 的 QoS 类为 <code>Burstable</code>：</em></p>
<ul>
<li><em>Pod 不符合 <code>Guaranteed</code> QoS 类的标准。</em></li>
<li><em>Pod 中至少一个 Container 具有内存或 CPU 的请求或限制。</em></li>
</ul>
<p><em>对于 QoS 类为 <code>BestEffort</code> 的 Pod，Pod 中的 Container 必须没有设置内存和 CPU 限制或请求。</em></p>
</blockquote>
<h4 id="【题目-2】安全管理–配置-Pod-安全上下文-0-5-分"><a href="#【题目-2】安全管理–配置-Pod-安全上下文-0-5-分" class="headerlink" title="【题目 2】安全管理–配置 Pod 安全上下文[0.5 分]"></a>【题目 2】安全管理–配置 Pod 安全上下文[0.5 分]</h4><p>使用 busybox 镜像启动一个名为 context-demo 的 Pod，为该 Pod 配置安全上下文，要求容器内以用户 1000 和用户组 3000 来运行所有进程，并在启动时执行“sleep 1h”命令。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.Pod 安全上下午配置正确得 0.5 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat context.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    run: context-demo</span><br><span class="line">  name: context-demo</span><br><span class="line">spec:</span><br><span class="line">  securityContext:</span><br><span class="line">    runAsUser: 1000</span><br><span class="line">    runAsGroup: 3000</span><br><span class="line">  containers:</span><br><span class="line">  - image: busybox:latest</span><br><span class="line">    name: context-demo</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    command: [&quot;sh&quot;,&quot;-c&quot;,&quot;sleep 1h&quot;]</span><br></pre></td></tr></table></figure>

<h4 id="【题目-3】CRD-管理–创建自定义资源类型-0-5-分"><a href="#【题目-3】CRD-管理–创建自定义资源类型-0-5-分" class="headerlink" title="【题目 3】CRD 管理–创建自定义资源类型[0.5 分]"></a>【题目 3】CRD 管理–创建自定义资源类型[0.5 分]</h4><p>在 Kubernetes 集群中自定义一种资源类型 Student，API 为 stable.example.com&#x2F;v1，单数形式为 student，复数形式为 students，简写为 stu，作用域为命名空间级，然后在 default 命名空间下创建一个名为 exam 的Student 对象。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.资源类型 Student 定义成功的 0.3 分；<br>2.exam 创建成功得 0.2 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat crd.yaml </span><br><span class="line">apiVersion: apiextensions.k8s.io/v1</span><br><span class="line">kind: CustomResourceDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: students.stable.example.com</span><br><span class="line">spec:</span><br><span class="line">  group: stable.example.com</span><br><span class="line">  names:</span><br><span class="line">    kind: Student #kind 通常是单数形式的驼峰命名（CamelCased）形式。你的资源清单会使用这一形式。</span><br><span class="line">    plural: students #名称的复数形式，用于 URL：/apis/&lt;组&gt;/&lt;版本&gt;/&lt;名称的复数形式&gt;</span><br><span class="line">    singular: student #名称的单数形式，作为命令行使用时和显示时的别名</span><br><span class="line">    shortNames: # shortNames 允许你在命令行使用较短的字符串来匹配资源</span><br><span class="line">    - stu</span><br><span class="line">  scope: Namespaced</span><br><span class="line">  versions:</span><br><span class="line">  - served: true</span><br><span class="line">    storage: true</span><br><span class="line">    name: v1</span><br><span class="line">    schema:</span><br><span class="line">      openAPIV3Schema:</span><br><span class="line">        type: object</span><br><span class="line">[root@k8s-master-node1 ~]# cat student.yaml </span><br><span class="line">apiVersion: stable.example.com/v1</span><br><span class="line">kind: Student</span><br><span class="line">metadata:</span><br><span class="line">  name: exam</span><br></pre></td></tr></table></figure>

<h4 id="【题目-4】解析管理–添加主机别名到-Pod-0-5-分"><a href="#【题目-4】解析管理–添加主机别名到-Pod-0-5-分" class="headerlink" title="【题目 4】解析管理–添加主机别名到 Pod[0.5 分]"></a>【题目 4】解析管理–添加主机别名到 Pod[0.5 分]</h4><p>使用 nginx 镜像在 default 命名空间下创建一个名为 nginx 的 Pod，并在 Pod 的&#x2F;etc&#x2F;hosts中添加 IP 地址 127.0.0.1 与 chinaskills 的解析。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.自定义解析配置正确得 0.5 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat nginx.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  hostAliases:</span><br><span class="line">  - ip: &quot;127.0.0.1&quot;</span><br><span class="line">    hostnames:</span><br><span class="line">    - &quot;chinaskills&quot;</span><br><span class="line">  containers:</span><br><span class="line">  - name: ngnix</span><br><span class="line">    image: nginx:latest</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br></pre></td></tr></table></figure>

<h4 id="【题目-5】HPA-管理–创建-HPA-规则-1-分"><a href="#【题目-5】HPA-管理–创建-HPA-规则-1-分" class="headerlink" title="【题目 5】HPA 管理–创建 HPA 规则[1 分]"></a>【题目 5】HPA 管理–创建 HPA 规则[1 分]</h4><p>默认情况下 HPA 是无法调整伸缩灵敏度的，但不同的业务场景对伸缩灵敏度的要求不一样。要求在 default 命名空间下使用 nginx 镜像创建一个名为 web 的 deployment，自定义HPA 的伸缩灵敏度，为该 deployment 创建一个名为 web 的 HPA，扩容时立即新增当前 9 倍数量的副本数，时间窗口为 5s，伸缩范围为 1–1000。例如一开始只有 1 个 Pod，当 CPU 使用率超过 80%时，Pod 数量变化趋势为：1 → 10 → 100 → 1000。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.HPA 创建成功得 0.2 分；<br>2.HPA 伸缩策略配置正确得 0.8 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat nginx-hpa.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: nginx:latest</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        name: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 20Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 10m</span><br><span class="line">            memory: 20Mi</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat web-hpa.yaml</span><br><span class="line">apiVersion: autoscaling/v2beta2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: web</span><br><span class="line">spec:</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 1000</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: web</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Resource</span><br><span class="line">    resource:</span><br><span class="line">      name: cpu</span><br><span class="line">      target:</span><br><span class="line">        type: Utilization</span><br><span class="line">        averageUtilization: 80</span><br><span class="line">  behavior:</span><br><span class="line">    scaleUp:</span><br><span class="line">      stabilizationWindowSeconds: 5 #在五秒之内进程扩容</span><br><span class="line">      policies:</span><br><span class="line">      - type: Percent  #以百分比形式</span><br><span class="line">        value: 900  #百分之900</span><br><span class="line">        periodSeconds: 5 #每5秒扩容一次</span><br></pre></td></tr></table></figure>

<h4 id="【题目-6】节点亲和性管理–创建硬限制规则的-Pod-0-5-分"><a href="#【题目-6】节点亲和性管理–创建硬限制规则的-Pod-0-5-分" class="headerlink" title="【题目 6】节点亲和性管理–创建硬限制规则的 Pod[0.5 分]"></a>【题目 6】节点亲和性管理–创建硬限制规则的 Pod[0.5 分]</h4><p>在 default 命 名空 间下 使用 nginx 镜像 运行 一个 Pod ，名 称为 nginx ，要求 使用requiredDuringSchedulingIgnoredDuringExecution 策略将 Pod 调度到具有“disktype&#x3D;ssd”标签的节点。<br>完成后提交 master 节点的用户名、密码和 IP 到答题框。<br>1.Pod 调度策略配置正确得 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# kubectl label node k8s-worker-node1 disktype=ssd</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat label-nginx.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata: </span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: nginx:latest</span><br><span class="line">    name: nginx</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">  affinity:</span><br><span class="line">    nodeAffinity:</span><br><span class="line">      requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">        nodeSelectorTerms:</span><br><span class="line">        - matchExpressions:</span><br><span class="line">          - key: disktype</span><br><span class="line">            operator: In</span><br><span class="line">            values:</span><br><span class="line">              - ssd</span><br></pre></td></tr></table></figure>

<h4 id="【题目-7】网络策略管理–创建-Pod-网络策略-0-5-分"><a href="#【题目-7】网络策略管理–创建-Pod-网络策略-0-5-分" class="headerlink" title="【题目 7】网络策略管理–创建 Pod 网络策略[0.5 分]"></a>【题目 7】网络策略管理–创建 Pod 网络策略[0.5 分]</h4><p>创建一个网络策略 network-exam，要求只有 internal 命名空间下的 Pod 可以通过 TCP协议的 8080 端口访问到 mysql 命名空间下的 Pod。<br>完成后提交 master 节点的 IP、用户名和密码到答题框。<br>1.网络策略创建成功得 0.2 分；<br>2.规则配置正确得 0.3 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat network-exam.yaml </span><br><span class="line">apiVersion: networking.k8s.io/v1</span><br><span class="line">kind: NetworkPolicy</span><br><span class="line">metadata:</span><br><span class="line">  name: network-exam</span><br><span class="line">  namespace: internal</span><br><span class="line">spec:</span><br><span class="line">  podSelector: &#123;&#125; #internal命名空间下所有pod</span><br><span class="line">  policyTypes:</span><br><span class="line">    - Egress</span><br><span class="line">  egress:</span><br><span class="line">    - to:</span><br><span class="line">      - namespaceSelector:</span><br><span class="line">          matchLabels:</span><br><span class="line">            kubernetes.io/metadata.name: mysql #mysql命名空间的默认label</span><br><span class="line">      ports:</span><br><span class="line">        - protocol: TCP</span><br><span class="line">          port: 8080</span><br></pre></td></tr></table></figure>

<h4 id="【题目-8】驱逐机制管理–配置节点压力驱逐-0-5-分"><a href="#【题目-8】驱逐机制管理–配置节点压力驱逐-0-5-分" class="headerlink" title="【题目 8】驱逐机制管理–配置节点压力驱逐[0.5 分]"></a>【题目 8】驱逐机制管理–配置节点压力驱逐[0.5 分]</h4><p>设置 kubelet 数据存储在&#x2F;apps&#x2F;data&#x2F;kubelet 目录下，并设置当 kubelet 的存储空间不足 5%，或者当容器运行时文件系统可用存储空间不足 5%时开始驱逐 Pod。</p>
<p>完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.节点压力驱逐配置正确得 0.5 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# mkdir -p /apps/data/kubelet</span><br><span class="line">[root@k8s-master-node1 ~]# cat /etc/sysconfig/kubelet </span><br><span class="line">KUBELET_EXTRA_ARGS=--root-dir=/apps/data/kubelet</span><br><span class="line">[root@k8s-master-node1 ~]# cat /var/lib/kubelet/config.yaml  | grep available</span><br><span class="line">  nodefs.available: 5%</span><br><span class="line">  imagefs.available: 5%</span><br><span class="line">[root@k8s-master-node1 ~]# systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<h4 id="【题目-9】流量管理–创建-Ingress-Gateway-0-5-分"><a href="#【题目-9】流量管理–创建-Ingress-Gateway-0-5-分" class="headerlink" title="【题目 9】流量管理–创建 Ingress Gateway[0.5 分]"></a>【题目 9】流量管理–创建 Ingress Gateway[0.5 分]</h4><p>使用提供的软件包 ServiceMesh.tar.gz 将 Bookinfo 应用部署到 default 命名空间下，使用Istio Gateway 可 以 实 现 应 用 程 序 从 外 部 访 问 ， 请 为 Bookinfo 应 用 创 建 一 个 名 为bookinfo-gateway 的网关，指定所有 HTTP 流量通过 80 端口流入网格，然后将网关绑定到虚拟服务 bookinfo 上。<br>完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.Bookinfo 应用部署成功得 0.2 分；<br>2.Bookinfo 能通过网关访问得 0.3 分。</p>
<p>解压软件包并导入镜像：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">k8s</span>-<span class="type">master</span>-<span class="type">node1</span> ~]<span class="comment"># tar -zxf ServiceMesh.tar.gz</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">k8s</span>-<span class="type">master</span>-<span class="type">node1</span> ~]<span class="comment"># cd ServiceMesh/</span></span><br><span class="line">[<span class="type">root</span>@<span class="type">k8s</span>-<span class="type">master</span>-<span class="type">node1</span> <span class="type">ServiceMesh</span>]<span class="comment"># docker load -i images/image.tar</span></span><br></pre></td></tr></table></figure>

<p>部署应用到Kubernetes集群：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">k8s</span>-<span class="type">master</span>-<span class="type">node1</span> <span class="type">ServiceMesh</span>]<span class="comment"># kubectl apply -f bookinfo/bookinfo.yaml</span></span><br><span class="line">service/details created</span><br><span class="line">serviceaccount/bookinfo<span class="literal">-details</span> created</span><br><span class="line">deployment.apps/details<span class="literal">-v1</span> created</span><br><span class="line">service/ratings created</span><br><span class="line">serviceaccount/bookinfo<span class="literal">-ratings</span> created</span><br><span class="line">deployment.apps/ratings<span class="literal">-v1</span> created</span><br><span class="line">service/reviews created</span><br><span class="line">serviceaccount/bookinfo<span class="literal">-reviews</span> created</span><br><span class="line">deployment.apps/reviews<span class="literal">-v1</span> created</span><br><span class="line">service/productpage created</span><br><span class="line">serviceaccount/bookinfo<span class="literal">-productpage</span> created</span><br><span class="line">deployment.apps/productpage<span class="literal">-v1</span> created</span><br></pre></td></tr></table></figure>

<p>Gateway配置文件如下：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">k8s</span>-<span class="type">master</span>-<span class="type">node1</span> <span class="type">ServiceMesh</span>]<span class="comment"># cat bookinfo-gateway.yaml</span></span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: Gateway</span><br><span class="line">metadata:</span><br><span class="line">name: bookinfo<span class="literal">-gateway</span></span><br><span class="line">spec:</span><br><span class="line">selector:</span><br><span class="line">  istio: ingressgateway <span class="comment"># use istio default controller</span></span><br><span class="line">servers:</span><br><span class="line"> - port:</span><br><span class="line">    number: <span class="number">80</span></span><br><span class="line">    name: http</span><br><span class="line">    protocol: HTTP</span><br><span class="line">  hosts:</span><br><span class="line">   - <span class="string">&quot;*&quot;</span></span><br><span class="line"><span class="literal">---</span></span><br><span class="line">apiVersion: networking.istio.io/v1alpha3</span><br><span class="line">kind: VirtualService</span><br><span class="line">metadata:</span><br><span class="line">name: bookinfo</span><br><span class="line">spec:</span><br><span class="line">hosts:</span><br><span class="line"> - <span class="string">&quot;*&quot;</span></span><br><span class="line">gateways:</span><br><span class="line"> - bookinfo<span class="literal">-gateway</span></span><br><span class="line">http:</span><br><span class="line"> - match:</span><br><span class="line">   - uri:</span><br><span class="line">      exact: /productpage</span><br><span class="line">   - uri:</span><br><span class="line">      prefix: /<span class="keyword">static</span></span><br><span class="line">   - uri:</span><br><span class="line">      exact: /login</span><br><span class="line">   - uri:</span><br><span class="line">      exact: /logout</span><br><span class="line">   - uri:</span><br><span class="line">      prefix: /api/v1/products</span><br><span class="line">  route:</span><br><span class="line">   - destination:</span><br><span class="line">      host: productpage</span><br><span class="line">      port:</span><br><span class="line">        number: <span class="number">9080</span></span><br></pre></td></tr></table></figure>

<p>确认网关创建完成：</p>
<figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">[<span class="type">root</span>@<span class="type">k8s</span>-<span class="type">master</span>-<span class="type">node1</span> <span class="type">ServiceMesh</span>]<span class="comment"># kubectl apply -f bookinfo-gateway.yaml</span></span><br><span class="line">gateway.networking.istio.io/bookinfo<span class="literal">-gateway</span> created</span><br><span class="line">virtualservice.networking.istio.io/bookinfo created</span><br><span class="line"></span><br><span class="line">[<span class="type">root</span>@<span class="type">k8s</span>-<span class="type">master</span>-<span class="type">node1</span> <span class="type">ServiceMesh</span>]<span class="comment"># kubectl get gateway</span></span><br><span class="line">NAME               AGE</span><br><span class="line">bookinfo<span class="literal">-gateway</span>       <span class="number">32</span>s</span><br></pre></td></tr></table></figure>

<h4 id="【题目-10】服务网格–创建基于用户身份的路由-0-5-分"><a href="#【题目-10】服务网格–创建基于用户身份的路由-0-5-分" class="headerlink" title="【题目 10】服务网格–创建基于用户身份的路由[0.5 分]"></a>【题目 10】服务网格–创建基于用户身份的路由[0.5 分]</h4><p>创建一个名为 reviews 路由，要求来自名为 Jason 的用户的所有流量将被路由到服务reviews:v2。<br>完成后提交 master 节点的用户名、密码和 IP 到答题框。<br>1.路由创建成功得 0.2 分；<br>2.用户限制正确得 0.3 分。</p>
<h4 id="【题目-11】服务网格–创建请求路由-0-5-分"><a href="#【题目-11】服务网格–创建请求路由-0-5-分" class="headerlink" title="【题目 11】服务网格–创建请求路由[0.5 分]"></a>【题目 11】服务网格–创建请求路由[0.5 分]</h4><p>在 default 命名空间下创建一个名为 reviews-route 的虚拟服务，默认情况下，所有的 HTTP流量都会被路由到标签为 version:v1 的 reviews 服务的 Pod 上。此外，路径以&#x2F;wpcatalog&#x2F;或&#x2F;consumercatalog&#x2F;开头的 HTTP 请求将被重写为&#x2F;newcatalog，并被发送到标签为 version:v2 的Pod 上。<br>完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.请求路由创建成功得 0.2 分；</p>
<p>2.路由策略配置正确得 0.3 分</p>
<h3 id="【任务-5】容器云服务运维：Kubernetes-基于虚拟机的运维-4-0-分"><a href="#【任务-5】容器云服务运维：Kubernetes-基于虚拟机的运维-4-0-分" class="headerlink" title="【任务 5】容器云服务运维：Kubernetes 基于虚拟机的运维[4.0 分]"></a>【任务 5】容器云服务运维：Kubernetes 基于虚拟机的运维[4.0 分]</h3><p>【适用平台】私有云</p>
<h4 id="【题目-1】VM-管理–创建-VM-0-5-分"><a href="#【题目-1】VM-管理–创建-VM-0-5-分" class="headerlink" title="【题目 1】VM 管理–创建 VM[0.5 分]"></a>【题目 1】VM 管理–创建 VM[0.5 分]</h4><p>使用镜像 fedora-virt:v1.0 在 default 命名空间下创建一台 vm，名称为 vm-fedora，内存为1G。<br>完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.VM 创建成功得 0.3 分；<br>2.VM 配置正确得 0.2 分。</p>
<p><em><strong>重要：启用仿真模拟，虽然会影响VM的性能 。</strong></em></p>
<blockquote>
<p>如果不启用，后边可能vm会启动不起来，但是开启后只能在本机访问，不能通过nodePort远程访问，此处建议执行，运行成功后可以尝试修改为false，然后再删除vm，重新创建vm。</p>
</blockquote>
<p><code>kubectl create configmap -n kubevirt kubevirt-config --from-literal debug.useEmulation=true</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat vm-fedora.yaml </span><br><span class="line">apiVersion: kubevirt.io/v1</span><br><span class="line">kind: VirtualMachine</span><br><span class="line">metadata:</span><br><span class="line">  name: vm-fedora</span><br><span class="line">spec:</span><br><span class="line">  running: false</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        kubevirt.io/vm: vm-fedora</span><br><span class="line">    spec:</span><br><span class="line">      domain:</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            memory: 1Gi</span><br><span class="line">        devices:</span><br><span class="line">          disks:</span><br><span class="line">          - name: containerdisk</span><br><span class="line">            disk: &#123;&#125;</span><br><span class="line">      volumes:</span><br><span class="line">      - name: containerdisk</span><br><span class="line">        containerDisk:</span><br><span class="line">          image: kubevirt/fedora-cloud-container-disk-demo:latest</span><br></pre></td></tr></table></figure>

<h4 id="【题目-2】存储与卷–创建-emptyDisk-卷-1-分"><a href="#【题目-2】存储与卷–创建-emptyDisk-卷-1-分" class="headerlink" title="【题目 2】存储与卷–创建 emptyDisk 卷[1 分]"></a>【题目 2】存储与卷–创建 emptyDisk 卷[1 分]</h4><p>使用镜像 fedora-virt:v1.0 在 default 命名空间下创建一台 vmi，名称为 vmi-fedora，并使用 emptyDisk 卷为 vmi 挂载一块 2G 的磁盘。<br>完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.VMI 创建成功得 0.4 分；<br>2.卷挂载成功得 0.6 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# cat vmi.yaml </span><br><span class="line">apiVersion: kubevirt.io/v1</span><br><span class="line">kind: VirtualMachineInstance</span><br><span class="line">metadata:</span><br><span class="line">  name: testvmi-nocloud</span><br><span class="line">spec:</span><br><span class="line">  domain:</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: 1024M</span><br><span class="line">    devices:</span><br><span class="line">      disks:</span><br><span class="line">      - name: containerdisk</span><br><span class="line">        disk:</span><br><span class="line">          bus: virtio</span><br><span class="line">      - name: emptydisk</span><br><span class="line">        disk:</span><br><span class="line">          bus: virtio</span><br><span class="line">  volumes:</span><br><span class="line">  - name: containerdisk</span><br><span class="line">    containerDisk:</span><br><span class="line">      image: kubevirt/fedora-cloud-container-disk-demo:latest</span><br><span class="line">  - name: emptydisk</span><br><span class="line">    emptyDisk:</span><br><span class="line">      capacity: &quot;2Gi&quot;</span><br></pre></td></tr></table></figure>

<h4 id="【题目-3】KubeVirt-运维–创建-VMI-1-5-分"><a href="#【题目-3】KubeVirt-运维–创建-VMI-1-5-分" class="headerlink" title="【题目 3】KubeVirt 运维–创建 VMI[1.5 分]"></a>【题目 3】KubeVirt 运维–创建 VMI[1.5 分]</h4><p>将提供的镜像 exam.qcow2 转换为 docker 镜像 exam:v1.0，然后使用镜像 exam:v1.0 镜像在 default 命名空间下创建一台 vmi，名称为 exam，将虚拟机的 80 端口以 NodePort 的方式对外暴露为 30082，并使用数据源在启动时将 VM 的主机名初始化为 exam。完成后提交 master 节点的 IP 地址、用户名和密码到答题框。<br>1.qcow2 镜像转化成功得 0.3 分；<br>2.VMI 创建成功得 0.3 分；</p>
<p>3.端口暴露成功得 0.3 分；<br>4.服务能正常访问得 0.4 分；<br>5.主机名初始化成功得 0.2 分。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 ~]# yum install -y  libguestfs-tools</span><br><span class="line">[root@k8s-master-node1 ~]# systemctl restart libvirtd</span><br><span class="line">[root@k8s-master-node1 ~]# guestmount -a CentOS-7-x86_64-2009.qcow2 -i /mnt/</span><br><span class="line">libguestfs: error: could not create appliance through libvirt.</span><br><span class="line">Try running qemu directly without libvirt using this environment variable:</span><br><span class="line">export LIBGUESTFS_BACKEND=direct</span><br><span class="line">Original error from libvirt: Cannot access storage file &#x27;/root/CentOS-7-x86_64-2009.qcow2&#x27; (as uid:107, gid:107): Permission denied [code=38 int1=13]</span><br><span class="line">[root@k8s-master-node1 ~]# export LIBGUESTFS_BACKEND=direct</span><br><span class="line">[root@k8s-master-node1 ~]# guestmount -a CentOS-7-x86_64-2009.qcow2 -i /mnt/</span><br><span class="line">[root@k8s-master-node1 ~]# ls /mnt/</span><br><span class="line">bin  boot  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var</span><br><span class="line">[root@k8s-master-node1 ~]# tar -cvzf exam.tar.gz  -C /mnt/ ./</span><br><span class="line">[root@k8s-master-node1 ~]# cat /root/exam.tar.gz | docker import - exam:v1.0</span><br><span class="line">[root@k8s-master-node1 ~]# docker images | grep exam</span><br><span class="line">exam                                                v1.0           b7271758eb57   2 minutes ago   807MB</span><br></pre></td></tr></table></figure>

<h4 id="【题目-4】KubeVirt-运维–启用快照-1-分"><a href="#【题目-4】KubeVirt-运维–启用快照-1-分" class="headerlink" title="【题目 4】KubeVirt 运维–启用快照[1 分]"></a>【题目 4】KubeVirt 运维–启用快照[1 分]</h4><p>KubeVirt 支持对 VM 进行快照，请启用 KubeVirt 快照管理功能：完成后提交 master 节点的 IP 地址、用户名和密码到答题框。</p>
<h3 id="【任务-6】容器云运维开发：Kubernetes-APIs-运维开发-10-分"><a href="#【任务-6】容器云运维开发：Kubernetes-APIs-运维开发-10-分" class="headerlink" title="【任务 6】容器云运维开发：Kubernetes APIs 运维开发[10 分]"></a>【任务 6】容器云运维开发：Kubernetes APIs 运维开发[10 分]</h3><p>【适用平台】私有云</p>
<h4 id="【题目-1】Python-运维开发：基于-Kubernetes-Restful-API-实现-Deployment-创建-2-分"><a href="#【题目-1】Python-运维开发：基于-Kubernetes-Restful-API-实现-Deployment-创建-2-分" class="headerlink" title="【题目 1】Python 运维开发：基于 Kubernetes Restful API 实现 Deployment 创建[2 分]"></a>【题目 1】Python 运维开发：基于 Kubernetes Restful API 实现 Deployment 创建[2 分]</h4><p>在提供的 OpenStack 私有云平台上，使用 k8s-python-dev 镜像创建 1 台云主机，云主机类型使用 4vCPU&#x2F;12G 内存&#x2F;100G 硬盘。该主机中已经默认安装了所需的开发环境，登录默认账号密码为“root&#x2F;1DaoYun@2022”。<br>使用 Kubernetes Restful API 库，在&#x2F;root 目录下，创建 api_deployment_manager.py 文件，要求编写 python 代码，代码实现以下任务：<br>（1）编写 Python 程序实现 Deployment 资源的创建。Deployment 配置信息如下。如果<br>同名 Deployment 存在，先删除再创建。<br>（2）创建完成后，查询该 Deployment 的详细信息，执行结果控制台输出，以 yaml 格<br>式展示。<br>创建 Deployment 的 yaml 的配置如下：<br>apiVersion: apps&#x2F;v1<br>kind: Deployment<br>metadata:<br>name: nginx-deployment<br>labels:</p>
<p>app: nginx<br>spec:<br>replicas: 3<br>selector:<br>matchLabels:<br>app: nginx<br>template:<br>metadata:<br>labels:<br>app: nginx<br>spec:<br>containers:<br>- name: nginx<br>image: nginx:1.15.4<br>ports:<br>- containerPort: 80<br>1.执行 api_deployment_manager.py 脚本，成功创建 deployment 资源，计 1 分；<br>2.检查创建的 deployment 资源，配置信息无误计 1 分。1.快照功能启用成功得 1 分。</p>
<h4 id="【题目-2】Python-运维开发：基于-Kubernetes-Python-SDK-实现-Job-创建-1-分"><a href="#【题目-2】Python-运维开发：基于-Kubernetes-Python-SDK-实现-Job-创建-1-分" class="headerlink" title="【题目 2】Python 运维开发：基于 Kubernetes Python SDK 实现 Job 创建[1 分]"></a>【题目 2】Python 运维开发：基于 Kubernetes Python SDK 实现 Job 创建[1 分]</h4><p>在前面已建好的 Kubernetes 开发环境云平台上。使用 Kubernetes python SDK 的“kubernetes”Python 库，在&#x2F;root 目录下，创建 sdk_job_manager.py 文件，要求编写 python 代码，代码实现以下任务：<br>（1）编写 Python 程序实现 Job 资源的创建。Job 配置信息如下。如果同名 Job 存在，先删除再创建。<br>（2）创建完成后，查询该 Job 的详细信息，执行结果控制台输出，以 json 格式展示。<br>Job 创建 yaml 的信息如下：<br>apiVersion: batch&#x2F;v1<br>kind: Job</p>
<p>metadata:<br>name: pi<br>spec:<br>template:<br>spec:<br>containers:<br>- name: pi<br>image: perl<br>command: [“perl”, “-Mbignum&#x3D;bpi”, “-wle”, “print bpi(2000)”]<br>restartPolicy: Never<br>backoffLimit: 4<br>1.执行 sdk_job_manager.py 脚本，成功创建 job 资源，计 0.5 分；<br>2.查询 job 资源，配置信息无误，计 0.5 分。</p>
<h4 id="【题目-3】Python-运维开发：Pod-资源的-Restful-APIs-HTTP-服务封装-3-分"><a href="#【题目-3】Python-运维开发：Pod-资源的-Restful-APIs-HTTP-服务封装-3-分" class="headerlink" title="【题目 3】Python 运维开发：Pod 资源的 Restful APIs HTTP 服务封装[3 分]"></a>【题目 3】Python 运维开发：Pod 资源的 Restful APIs HTTP 服务封装[3 分]</h4><p>编写 Python 程序实现 Pod 资源管理程序，将 Pod 资源管理的封装成 Web 服务。在&#x2F;root 目录下创建 pod_server.py 程序，实现 Pod 的增删查改等 Web 访问操作。http.server的 host 为 localhost，端口 8889；程序内部实现 Kubernetes 认证。提示说明：Python 标准库 http.server 模块，提供了 HTTP Server 请求封装。<br>需要实现的 Restful API 接口如下：GET &#x2F;pod&#x2F;{name} ，查询指定名称{name}的 Pod；Response 的 Body 以 json 格式输出。POST &#x2F;pod&#x2F;{yamlfilename} 创建 yaml 文件名称为{yamlfilename}的 Pod；Response 的<br>Body 以 json 格式。编码完成后，“手工下载”文件服务器主目录所有*.yaml 文件到 root 目录下，“手动执<br>行”所编写 pod_server.py 程序，提交答案进行检测。<br>1.HTTP 服务成功启动，计 1 分；<br>2.发起指定参数的 GET 查询 Pod 请求，成功查询指定名称的 pod 服务，计 1 分；</p>
<p>3.发起指定参数的 POST 创建 Pod 请求，成功创建 Pod 服务，计 1 分。</p>
<h4 id="【题目-4】Python-运维开发：Service-资源-Restful-APIs-HTTP-服务封装-4-分"><a href="#【题目-4】Python-运维开发：Service-资源-Restful-APIs-HTTP-服务封装-4-分" class="headerlink" title="【题目 4】Python 运维开发：Service 资源 Restful APIs HTTP 服务封装[4 分]"></a>【题目 4】Python 运维开发：Service 资源 Restful APIs HTTP 服务封装[4 分]</h4><p>编写 Python 程序实现 Service 资源管理程序，将 Service 资源管理的封装成 Web 服务。在&#x2F;root 目录下创建 service_server.py 程序，实现 Service 的增删查改等 Web 访问操作。http.server 的 host 为 localhost，端口 8888；程序内部实现 Kubernetes 认证。提示说明：Python 标准库 http.server 模块，提供了 HTTP Server 请求封装。需要实现的 Restful API 接口如下：GET &#x2F;services&#x2F;{name}，查询指定名称{name}的 Service；Response 的 Body 以 json 格式输出。<br>POST &#x2F;services&#x2F;{yamlfilename} 创建 yaml 文件名称为{yamlfilename}的 Service；<br>Response 的 Body 以 json 格式，（手工将文件服务器主目录所有*.yaml 文件下载到 root 目录<br>下）。<br>DELETE &#x2F;services&#x2F;{name}；删除指定名称的 Service；Response 的 Body 以 json 格式。<br>编码完成后，自己手动执行提供 Web HTTP 服务的 service_server.py 程序，提交答案进<br>行检测。<br>1.HTTP 服务成功启动，计 1 分；<br>2.发起指定参数的 POST 创建 service 请求，成功创建 service 资源，计 1 分；<br>3.发起指定参数的 GET 查询 service 请求，成功查询指定名称的 Service，计 1 分；<br>4.发起指定参数的 DELETE 删除 service 请求，成功删除指定名称的 Service，计 1 分。</p>
]]></content>
      <tags>
        <tag>容器云</tag>
      </tags>
  </entry>
  <entry>
    <title>2022浙江省云计算样题</title>
    <url>/2022/07/01/2022%E6%B5%99%E6%B1%9F%E7%9C%81%E4%BA%91%E8%AE%A1%E7%AE%97%E6%A0%B7%E9%A2%98/</url>
    <content><![CDATA[<h1 id="2022年浙江省职业院校技能大赛高职组云计算竞赛赛卷（样卷）"><a href="#2022年浙江省职业院校技能大赛高职组云计算竞赛赛卷（样卷）" class="headerlink" title="2022年浙江省职业院校技能大赛高职组云计算竞赛赛卷（样卷）"></a>2022年浙江省职业院校技能大赛高职组云计算竞赛赛卷（样卷）<span id="more"></span></h1><p>【任务1】基础运维任务[5分]</p>
<p>【题目1】基础环境配置</p>
<p>根据表1中的IP地址规划，设置各服务器节点的IP地址，确保网络正常通信，然后按以下要求配置服务器：</p>
<p>（1）设置控制节点主机名为controller，设置计算节点主机名为compute；</p>
<p><strong>controller:</strong></p>
<p><code>hostnamectl set-hostname controller</code></p>
<p><code>bash</code></p>
<p><strong>compute:</strong></p>
<p><code>hostnamectl set-hostname compute</code></p>
<p><code>bash</code></p>
<p>（2）修改hosts文件将IP地址映射为主机名；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.20.113 controller</span><br><span class="line">192.168.20.120 compute</span><br></pre></td></tr></table></figure>

<p>（3）关闭控制节点的防火墙，设置开机不启动；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# systemctl stop firewalld</span><br><span class="line"></span><br><span class="line">[root@controller ~]# systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<p>（4）设置SELinux为Permissive 模式。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/selinux/config </span><br><span class="line"></span><br><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"></span><br><span class="line"># SELINUX=permissive</span><br><span class="line"></span><br><span class="line">#     enforcing - SELinux security policy is enforced.</span><br><span class="line"></span><br><span class="line">#     permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line"></span><br><span class="line">#     disabled - No SELinux policy is loaded.</span><br><span class="line"></span><br><span class="line">SELINUX=permissive</span><br><span class="line"></span><br><span class="line"># SELINUXTYPE= can take one of three two values:</span><br><span class="line"></span><br><span class="line">#     targeted - Targeted processes are protected,</span><br><span class="line"></span><br><span class="line">#     minimum - Modification of targeted policy. Only selected processes are protected. </span><br><span class="line"></span><br><span class="line">#     mls - Multi Level Security protection.</span><br><span class="line"></span><br><span class="line">SELINUXTYPE=targeted</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# setenforce 0</span><br><span class="line">setenforce: SELinux is disabled</span><br></pre></td></tr></table></figure>

<p>【题目2】镜像挂载</p>
<p>将提供的CentOS-7-x86_64-DVD-1804.iso和chinaskill_cloud_iaas.iso光盘镜像上传到controller节点&#x2F;root目录下，然后在&#x2F;opt目录下分别创建centos目录和openstack目录，并将镜像文件CentOS-7-x86_64-DVD-1804.iso挂载到centos目录下，将镜像文件chinaskill_cloud_iaas.iso挂载到openstack目录下。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>略</strong></p>
<p>【题目3】Yum源配置</p>
<p>在controller节点上利用&#x2F;opt&#x2F;centos目录中的软件包安装vsftpd服务，设置开机自启动，并使用ftp提供yum仓库服务（ftp共享的目录为&#x2F;opt），分别设置controller节点和compute节点的yum源文件ftp.repo，其中ftp服务器地址使用主机名形式。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>略</strong></p>
<p>【题目4】时间同步配置</p>
<p>在controller节点上部署chrony服务器，允许其他节点同步时间，启动服务并设置为开机启动；在compute节点上指定controller节点为上游NTP服务器，重启服务并设为开机启动。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<blockquote>
<p>这里可以先下载iaas-xiandian</p>
</blockquote>
<p><code>yum install -y iaas-xiandian</code></p>
<p><strong>然后编辑变量</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/xiandian/openrc.sh </span><br><span class="line">#--------------------system Config--------------------##</span><br><span class="line">#Controller Server Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP=192.168.20.113</span><br><span class="line"></span><br><span class="line">#Controller HOST Password. example:000000 </span><br><span class="line">HOST_PASS=000000</span><br><span class="line"></span><br><span class="line">#Controller Server hostname. example:controller</span><br><span class="line">HOST_NAME=controller</span><br><span class="line"></span><br><span class="line">#Compute Node Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP_NODE=192.168.20.120</span><br><span class="line"></span><br><span class="line">#Compute HOST Password. example:000000 </span><br><span class="line">HOST_PASS_NODE=000000</span><br><span class="line"></span><br><span class="line">#Compute Node hostname. example:compute</span><br><span class="line">HOST_NAME_NODE=compute</span><br><span class="line"></span><br><span class="line">#--------------------Chrony Config-------------------##</span><br><span class="line">#Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)</span><br><span class="line">network_segment_IP=192.168.20.0/24</span><br><span class="line"></span><br><span class="line">#--------------------Rabbit Config ------------------##</span><br><span class="line">#user for rabbit. example:openstack</span><br><span class="line">RABBIT_USER=openstack</span><br><span class="line"></span><br><span class="line">#Password for rabbit user .example:000000</span><br><span class="line">RABBIT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------MySQL Config---------------------##</span><br><span class="line">#Password for MySQL root user . exmaple:000000</span><br><span class="line">DB_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Keystone Config------------------##</span><br><span class="line">#Password for Keystore admin user. exmaple:000000</span><br><span class="line">DOMAIN_NAME=demo</span><br><span class="line">ADMIN_PASS=000000</span><br><span class="line">DEMO_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql keystore user. exmaple:000000</span><br><span class="line">KEYSTONE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Glance Config--------------------##</span><br><span class="line">#Password for Mysql glance user. exmaple:000000</span><br><span class="line">GLANCE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore glance user. exmaple:000000</span><br><span class="line">GLANCE_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Nova Config----------------------##</span><br><span class="line">#Password for Mysql nova user. exmaple:000000</span><br><span class="line">NOVA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore nova user. exmaple:000000</span><br><span class="line">NOVA_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Neturon Config-------------------##</span><br><span class="line">#Password for Mysql neutron user. exmaple:000000</span><br><span class="line">NEUTRON_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore neutron user. exmaple:000000</span><br><span class="line">NEUTRON_PASS=000000</span><br><span class="line"></span><br><span class="line">#metadata secret for neutron. exmaple:000000</span><br><span class="line">METADATA_SECRET=000000</span><br><span class="line"></span><br><span class="line">#Tunnel Network Interface. example:x.x.x.x</span><br><span class="line">INTERFACE_IP=192.168.20.113 #注意这里是哪个节点就用哪个的ip</span><br><span class="line"></span><br><span class="line">#External Network Interface. example:eth1</span><br><span class="line">INTERFACE_NAME=eth1</span><br><span class="line"></span><br><span class="line">#External Network The Physical Adapter. example:provider</span><br><span class="line">Physical_NAME=provider</span><br><span class="line"></span><br><span class="line">#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101</span><br><span class="line">minvlan=101</span><br><span class="line"></span><br><span class="line">#Last Vlan ID in VLAN RANGE for VLAN Network. example:200</span><br><span class="line">maxvlan=200</span><br><span class="line"></span><br><span class="line">#--------------------Cinder Config--------------------##</span><br><span class="line">#Password for Mysql cinder user. exmaple:000000</span><br><span class="line">CINDER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore cinder user. exmaple:000000</span><br><span class="line">CINDER_PASS=000000</span><br><span class="line"></span><br><span class="line">#Cinder Block Disk. example:md126p3</span><br><span class="line">BLOCK_DISK=vdb1</span><br><span class="line"></span><br><span class="line">#--------------------Swift Config---------------------##</span><br><span class="line">#Password for Keystore swift user. exmaple:000000</span><br><span class="line">SWIFT_PASS=000000</span><br><span class="line"></span><br><span class="line">#The NODE Object Disk for Swift. example:md126p4.</span><br><span class="line">OBJECT_DISK=vdb2</span><br><span class="line"></span><br><span class="line">#The NODE IP for Swift Storage Network. example:x.x.x.x.</span><br><span class="line">STORAGE_LOCAL_NET_IP=192.168.20.120</span><br><span class="line"></span><br><span class="line">#--------------------Heat Config----------------------##</span><br><span class="line">#Password for Mysql heat user. exmaple:000000</span><br><span class="line">HEAT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore heat user. exmaple:000000</span><br><span class="line">HEAT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Zun Config-----------------------##</span><br><span class="line">#Password for Mysql Zun user. exmaple:000000</span><br><span class="line">ZUN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Zun user. exmaple:000000</span><br><span class="line">ZUN_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql Kuryr user. exmaple:000000</span><br><span class="line">KURYR_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Kuryr user. exmaple:000000</span><br><span class="line">KURYR_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Ceilometer Config----------------##</span><br><span class="line">#Password for Gnocchi ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------AODH Config----------------##</span><br><span class="line">#Password for Mysql AODH user. exmaple:000000</span><br><span class="line">AODH_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore AODH user. exmaple:000000</span><br><span class="line">AODH_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Barbican Config----------------##</span><br><span class="line">#Password for Mysql Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_PASS=000000</span><br></pre></td></tr></table></figure>

<p><strong>执行脚本</strong></p>
<p><code>iaas-pre-host.sh</code></p>
<blockquote>
<p>注意两个节点都要执行</p>
</blockquote>
<p>【题目5】计算节点分区</p>
<p>在compute节点上利用空白分区划分2个100G分区。完成后提交计算节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# umount /dev/vdb</span><br><span class="line"></span><br><span class="line">[root@compute ~]# fdisk /dev/vdb</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line"></span><br><span class="line">Changes will remain in memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write command.</span><br><span class="line"></span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p</span><br><span class="line">Using default response p</span><br><span class="line">Partition number (1-4, default 1): 1</span><br><span class="line">First sector (2048-125829119, default 2048): </span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-125829119, default 125829119): +20G</span><br><span class="line">Partition 1 of type Linux and of size 20 GiB is set</span><br><span class="line"></span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p</span><br><span class="line">Partition number (2-4, default 2): 2</span><br><span class="line">First sector (41945088-125829119, default 41945088): </span><br><span class="line">Using default value 41945088</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (41945088-125829119, default 125829119): +20G</span><br><span class="line">Partition 2 of type Linux and of size 20 GiB is set</span><br><span class="line"></span><br><span class="line">Command (m for help): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br></pre></td></tr></table></figure>

<p>【任务2】OpenStack搭建任务[15分]</p>
<p>【适用平台】私有云</p>
<p>【题目1】基础安装</p>
<p>在控制节点和计算节点上分别安装iaas-xiandian软件包，根据表2配置两个节点脚本文件中的基本变量（配置脚本文件为&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh）。</p>
<p>表2 云平台配置信息</p>
<table>
<thead>
<tr>
<th>服务名称</th>
<th>变量</th>
<th>参数&#x2F;密码</th>
</tr>
</thead>
<tbody><tr>
<td>Mysql</td>
<td>root</td>
<td>000000</td>
</tr>
<tr>
<td>Keystone</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Glance</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Nova</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Neutron</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Heat</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Zun</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Keystone</td>
<td>DOMAIN_NAME</td>
<td>demo</td>
</tr>
<tr>
<td>Admin</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Rabbit</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Glance</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Nova</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Neutron</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Heat</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Zun</td>
<td>000000</td>
<td></td>
</tr>
<tr>
<td>Neutron</td>
<td>Metadata</td>
<td>000000</td>
</tr>
<tr>
<td>External  Network</td>
<td>enp9s0（外网卡名）</td>
<td></td>
</tr>
</tbody></table>
<p>完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>这里我们在配置时间同步时已经完成了</strong></p>
<p>【题目2】数据库安装</p>
<p>在controller节点上使用iaas-install-mysql.sh 脚本安装Mariadb、Memcached、etcd服务。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-mysql.sh </span><br></pre></td></tr></table></figure>

<p>【题目3】Keystone服务安装</p>
<p>在controller节点上使用iaas-install-keystone.sh 脚本安装Keystone服务。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-keystone.sh </span><br></pre></td></tr></table></figure>

<p>【题目4】Glance安装</p>
<p>在controller节点上使用iaas-install-glance.sh脚本安装glance 服务。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-glance.sh </span><br></pre></td></tr></table></figure>

<p>【题目5】Nova安装</p>
<p>在controller节点和compute节点上分别使用iaas-install-nova -controller.sh脚本、iaas-install-nova-compute.sh脚本安装Nova 服务。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>controller</strong>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-nova-controller.sh </span><br></pre></td></tr></table></figure>

<p><strong>compute:</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-nova-compute.sh </span><br></pre></td></tr></table></figure>

<p>【题目6】Neutron安装</p>
<p>在controller节点和compute节点上分别修改iaas-install-neutron-controller.sh脚本、iaas-install-neutron-compute.sh脚本分别安装 Neutron 服务，执行完脚本后，网络默认是vlan模式。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# vi /usr/local/bin/iaas-install-neutron-controller.sh </span><br><span class="line"></span><br><span class="line">tenant_network_types  vxlan #将vxlan改为vlan</span><br></pre></td></tr></table></figure>

<p><strong>controller：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-neutron-controller.sh </span><br></pre></td></tr></table></figure>

<p><strong>compute：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-neutron-compute.sh </span><br></pre></td></tr></table></figure>

<p>【题目7】Doshboard安装</p>
<p>在controller节点上使用iaas-install-dashboad.sh脚本安装dashboad服务。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-dashboard.sh </span><br></pre></td></tr></table></figure>

<p>【题目8】Cinder安装</p>
<p>在控制节点和计算节点上分别使用iaas-install-cinder-controller.sh.sh脚本和iaas-install-cinder-compute.sh安装cinder服务。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-cinder-controller.sh</code></p>
<p><strong>compute</strong> </p>
<p><code>iaas-install-cinder-compute.sh</code></p>
<p>【任务3】OpenStack运维任务[15分]</p>
<p>【适用平台】私有云</p>
<p>【题目1】镜像管理</p>
<p>在openstack私有云平台上，基于cirros-0.3.4-x86_64-disk.img镜像，使用命令创建一个名为cirros的镜像。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image create cirros --disk qcow2 --container bare &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| Field            | Value                                                |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     |</span><br><span class="line">| container_format | bare                                                 |</span><br><span class="line">| created_at       | 2022-07-01T06:04:17Z                                 |</span><br><span class="line">| disk_format      | qcow2                                                |</span><br><span class="line">| file             | /v2/images/ace27239-b0e1-4ed5-a303-aee306ff5477/file |</span><br><span class="line">| id               | ace27239-b0e1-4ed5-a303-aee306ff5477                 |</span><br><span class="line">| min_disk         | 0                                                    |</span><br><span class="line">| min_ram          | 0                                                    |</span><br><span class="line">| name             | cirros                                               |</span><br><span class="line">| owner            | 94e98329acc846e38579a511d0bc82a8                     |</span><br><span class="line">| protected        | False                                                |</span><br><span class="line">| schema           | /v2/schemas/image                                    |</span><br><span class="line">| size             | 13287936                                             |</span><br><span class="line">| status           | active                                               |</span><br><span class="line">| tags             |                                                      |</span><br><span class="line">| updated_at       | 2022-07-01T06:04:18Z                                 |</span><br><span class="line">| virtual_size     | None                                                 |</span><br><span class="line">| visibility       | shared                                               |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>【题目2】实例类型管理</p>
<p>在openstack私有云平台上，使用命令创建一个名为Fmin，ID为1，内存为1024 MB，磁盘为10 GB，vcpu数量为1的云主机类型。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack flavor create Fmid --id 1 --disk 10 --ram 1024 --vcpus 1</span><br><span class="line">+----------------------------+-------+</span><br><span class="line">| Field                      | Value |</span><br><span class="line">+----------------------------+-------+</span><br><span class="line">| OS-FLV-DISABLED:disabled   | False |</span><br><span class="line">| OS-FLV-EXT-DATA:ephemeral  | 0     |</span><br><span class="line">| disk                       | 10    |</span><br><span class="line">| id                         | 1     |</span><br><span class="line">| name                       | Fmid  |</span><br><span class="line">| os-flavor-access:is_public | True  |</span><br><span class="line">| properties                 |       |</span><br><span class="line">| ram                        | 1024  |</span><br><span class="line">| rxtx_factor                | 1.0   |</span><br><span class="line">| swap                       |       |</span><br><span class="line">| vcpus                      | 1     |</span><br><span class="line">+----------------------------+-------+</span><br></pre></td></tr></table></figure>

<p>【题目3】网络管理</p>
<p>在openstack私有云平台上，创建云主机网络extnet，子网extsubnet，虚拟机网段为192.168.y.0&#x2F;24（其中y是vlan号），网关为192.168.y.1。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack network create extnet --share --external --provider-physical-network provider --provider-network-type vlan</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| Field                     | Value                                |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| admin_state_up            | UP                                   |</span><br><span class="line">| availability_zone_hints   |                                      |</span><br><span class="line">| availability_zones        |                                      |</span><br><span class="line">| created_at                | 2022-07-01T06:06:11Z                 |</span><br><span class="line">| description               |                                      |</span><br><span class="line">| dns_domain                | None                                 |</span><br><span class="line">| id                        | 1e7b526c-fe6f-491e-bebd-5125b34c8f0d |</span><br><span class="line">| ipv4_address_scope        | None                                 |</span><br><span class="line">| ipv6_address_scope        | None                                 |</span><br><span class="line">| is_default                | False                                |</span><br><span class="line">| is_vlan_transparent       | None                                 |</span><br><span class="line">| mtu                       | 1500                                 |</span><br><span class="line">| name                      | extnet                               |</span><br><span class="line">| port_security_enabled     | True                                 |</span><br><span class="line">| project_id                | 94e98329acc846e38579a511d0bc82a8     |</span><br><span class="line">| provider:network_type     | vlan                                 |</span><br><span class="line">| provider:physical_network | provider                             |</span><br><span class="line">| provider:segmentation_id  | 126                                  |</span><br><span class="line">| qos_policy_id             | None                                 |</span><br><span class="line">| revision_number           | 5                                    |</span><br><span class="line">| router:external           | External                             |</span><br><span class="line">| segments                  | None                                 |</span><br><span class="line">| shared                    | True                                 |</span><br><span class="line">| status                    | ACTIVE                               |</span><br><span class="line">| subnets                   |                                      |</span><br><span class="line">| tags                      |                                      |</span><br><span class="line">| updated_at                | 2022-07-01T06:06:12Z                 |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack subnet create extsubnet --network extnet --subnet-range 192.168.23.0/24 --gateway 192.168.23.1</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| Field             | Value                                |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| allocation_pools  | 192.168.23.2-192.168.23.254          |</span><br><span class="line">| cidr              | 192.168.23.0/24                      |</span><br><span class="line">| created_at        | 2022-07-01T06:07:25Z                 |</span><br><span class="line">| description       |                                      |</span><br><span class="line">| dns_nameservers   |                                      |</span><br><span class="line">| enable_dhcp       | True                                 |</span><br><span class="line">| gateway_ip        | 192.168.23.1                         |</span><br><span class="line">| host_routes       |                                      |</span><br><span class="line">| id                | 730bd899-2daf-4395-a6a4-03fdbe7054f6 |</span><br><span class="line">| ip_version        | 4                                    |</span><br><span class="line">| ipv6_address_mode | None                                 |</span><br><span class="line">| ipv6_ra_mode      | None                                 |</span><br><span class="line">| name              | extsubnet                            |</span><br><span class="line">| network_id        | 1e7b526c-fe6f-491e-bebd-5125b34c8f0d |</span><br><span class="line">| project_id        | 94e98329acc846e38579a511d0bc82a8     |</span><br><span class="line">| revision_number   | 0                                    |</span><br><span class="line">| segment_id        | None                                 |</span><br><span class="line">| service_types     |                                      |</span><br><span class="line">| subnetpool_id     | None                                 |</span><br><span class="line">| tags              |                                      |</span><br><span class="line">| updated_at        | 2022-07-01T06:07:25Z                 |</span><br><span class="line">+-------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p>【题目4】云主机管理</p>
<p>在openstack私有云平台上，基于“cirros”镜像、flavor使用“Fmin”、extnet的网络，创建一台虚拟机VM1，启动VM1，并使用PC机能远程登录到VM1。提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server create VM1 --flavor Fmid --image cirros --network extnet</span><br><span class="line">+-------------------------------------+-----------------------------------------------+</span><br><span class="line">| Field                               | Value                                         |</span><br><span class="line">+-------------------------------------+-----------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                        |</span><br><span class="line">| OS-EXT-AZ:availability_zone         |                                               |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | None                                          |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                          |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       |                                               |</span><br><span class="line">| OS-EXT-STS:power_state              | NOSTATE                                       |</span><br><span class="line">| OS-EXT-STS:task_state               | scheduling                                    |</span><br><span class="line">| OS-EXT-STS:vm_state                 | building                                      |</span><br><span class="line">| OS-SRV-USG:launched_at              | None                                          |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                          |</span><br><span class="line">| accessIPv4                          |                                               |</span><br><span class="line">| accessIPv6                          |                                               |</span><br><span class="line">| addresses                           |                                               |</span><br><span class="line">| adminPass                           | e6ARiS6VGc2x                                  |</span><br><span class="line">| config_drive                        |                                               |</span><br><span class="line">| created                             | 2022-07-01T21:51:24Z                          |</span><br><span class="line">| flavor                              | Fmid (1)                                      |</span><br><span class="line">| hostId                              |                                               |</span><br><span class="line">| id                                  | f36b7a73-8d87-413c-a9ac-440903c82aa0          |</span><br><span class="line">| image                               | cirros (ace27239-b0e1-4ed5-a303-aee306ff5477) |</span><br><span class="line">| key_name                            | None                                          |</span><br><span class="line">| name                                | VM1                                           |</span><br><span class="line">| progress                            | 0                                             |</span><br><span class="line">| project_id                          | 94e98329acc846e38579a511d0bc82a8              |</span><br><span class="line">| properties                          |                                               |</span><br><span class="line">| security_groups                     | name=&#x27;default&#x27;                                |</span><br><span class="line">| status                              | BUILD                                         |</span><br><span class="line">| updated                             | 2022-07-01T21:51:24Z                          |</span><br><span class="line">| user_id                             | e754b15e87104c81ad3554dc3bc64e25              |</span><br><span class="line">| volumes_attached                    |                                               |</span><br><span class="line">+-------------------------------------+-----------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>【题目5】Cinder管理</p>
<p>在openstack私有云平台上，创建一个名为“lvm”的卷类型，创建一块卷设备，名字为block、类型为lvm的40G云硬盘，并附加到虚拟机VM1上。完成后提交控制节点IP地址、用户名和密码到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cinder type-create  lvm</span><br><span class="line">+--------------------------------------+------+-------------+-----------+</span><br><span class="line">| ID                                   | Name | Description | Is_Public |</span><br><span class="line">+--------------------------------------+------+-------------+-----------+</span><br><span class="line">| 7ef26956-bb5e-425d-9630-65d91ea2db4b | lvm  | -           | True      |</span><br><span class="line">+--------------------------------------+------+-------------+-----------+</span><br><span class="line"></span><br><span class="line">[root@controller ~]# cinder create --name block 10 --volume-type lvm</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| Property                       | Value                                |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| attachments                    | []                                   |</span><br><span class="line">| availability_zone              | nova                                 |</span><br><span class="line">| bootable                       | false                                |</span><br><span class="line">| consistencygroup_id            | None                                 |</span><br><span class="line">| created_at                     | 2022-05-09T03:28:56.000000           |</span><br><span class="line">| description                    | None                                 |</span><br><span class="line">| encrypted                      | False                                |</span><br><span class="line">| id                             | 1dd10e41-f0c6-4feb-a7e8-ddb1d99b067f |</span><br><span class="line">| metadata                       | &#123;&#125;                                   |</span><br><span class="line">| migration_status               | None                                 |</span><br><span class="line">| multiattach                    | False                                |</span><br><span class="line">| name                           | block                                |</span><br><span class="line">| os-vol-host-attr:host          | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:migstat | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:name_id | None                                 |</span><br><span class="line">| os-vol-tenant-attr:tenant_id   | 0047a899f1b34aaba102c89bf5dbeab4     |</span><br><span class="line">| replication_status             | None                                 |</span><br><span class="line">| size                           | 1                                    |</span><br><span class="line">| snapshot_id                    | None                                 |</span><br><span class="line">| source_volid                   | None                                 |</span><br><span class="line">| status                         | creating                             |</span><br><span class="line">| updated_at                     | None                                 |</span><br><span class="line">| user_id                        | 73ba03dfca0f4de6953a79e478cd035f     |</span><br><span class="line">| volume_type                    | lvm                                 |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack server add volume vm1 block</span><br></pre></td></tr></table></figure>

<p>【题目6】Raid管理</p>
<p>在OpenStack私有云平台，创建一台云主机，并创建一个40G大小的cinder块存储，将块存储连接到云主机，然后在云主机上对云硬盘进行操作。要求分出4个大小为5G的分区，使用这4个分区，创建名为&#x2F;dev&#x2F;md5、raid级别为5的磁盘阵列加一个热备盘（使用最后一个分区作为热备盘）。完成后提交云主机的用户名、密码和IP地址到答题框。</p>
<p><code>yum install -y mdadm</code></p>
<p><code>umount /mnt</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@raid ~]# fdisk /dev/vdb</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line">Changes will remain in memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write command.</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">Partition number (1-4, default 1): </span><br><span class="line">First sector (2048-104857599, default 2048): </span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-104857599, default 104857599): +5G</span><br><span class="line">Partition 1 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">Partition number (2-4, default 2): </span><br><span class="line">First sector (10487808-104857599, default 10487808): </span><br><span class="line">Using default value 10487808</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (10487808-104857599, default 104857599): +5G</span><br><span class="line">Partition 2 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (2 primary, 0 extended, 2 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">Partition number (3,4, default 3): </span><br><span class="line">First sector (20973568-104857599, default 20973568): </span><br><span class="line">Using default value 20973568</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (20973568-104857599, default 104857599): +5G  </span><br><span class="line">Partition 3 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (3 primary, 0 extended, 1 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default e): p</span><br><span class="line">Selected partition 4</span><br><span class="line">First sector (31459328-104857599, default 31459328): </span><br><span class="line">Using default value 31459328</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (31459328-104857599, default 104857599): +5G</span><br><span class="line">Partition 4 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 1</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 2</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 3</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 4</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br></pre></td></tr></table></figure>

<p><code>mdadm -C /dev/md5 -l 5 -n 3 -x 1 /dev/vdb1 /dev/vdb2 /dev/vdb3 /dev/vdb4</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@raid ~]# mdadm -D /dev/md5 </span><br><span class="line">/dev/md5:</span><br><span class="line">           Version : 1.2</span><br><span class="line">     Creation Time : Fri Apr 22 07:34:32 2022</span><br><span class="line">        Raid Level : raid5</span><br><span class="line">        Array Size : 10475520 (9.99 GiB 10.73 GB)</span><br><span class="line">     Used Dev Size : 5237760 (5.00 GiB 5.36 GB)</span><br><span class="line">      Raid Devices : 3</span><br><span class="line">     Total Devices : 4</span><br><span class="line">       Persistence : Superblock is persistent</span><br><span class="line">   Update Time : Fri Apr 22 07:40:00 2022</span><br><span class="line">         State : clean </span><br><span class="line">Active Devices : 3</span><br><span class="line">   Working Devices : 4</span><br><span class="line">    Failed Devices : 0</span><br><span class="line">     Spare Devices : 1</span><br><span class="line">        Layout : left-symmetric</span><br><span class="line">    Chunk Size : 512K</span><br><span class="line">Consistency Policy : resync</span><br><span class="line">          Name : raid.novalocal:5  (local to host raid.novalocal)</span><br><span class="line">          UUID : a7ee7f6c:33942c54:654cf6c9:880cc731</span><br><span class="line">        Events : 20</span><br><span class="line">Number   Major   Minor   RaidDevice State</span><br><span class="line">   0     253       17        0      active sync   /dev/vdb1</span><br><span class="line">   1     253       18        1      active sync   /dev/vdb2</span><br><span class="line">   4     253       19        2      active sync   /dev/vdb3</span><br><span class="line">   3     253       20        -      spare   /dev/vdb4</span><br></pre></td></tr></table></figure>

<p>【题目7】数据库主从管理</p>
<p>使用OpenStack私有云平台，创建两台云主机vm1和vm2，在这两台云主机上分别安装数据库服务，并配置成主从数据库，vm1节点为主库，vm2节点为从库（数据库密码设置为000000）。完成后提交数据库从节点vm2节点的用户名、密码和IP地址到答题框。</p>
<p><strong>（1）修改主机名</strong></p>
<p><strong>mysql1</strong></p>
<p><code>hostnamectl set-hostname mysql1</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>mysql2</strong></p>
<p><code>hostnamectl set-hostname mysql2</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>（2）配置hosts文件</strong></p>
<p>两个节点配置&#x2F;etc&#x2F;hosts文件，修改为如下</p>
<p><code>vi /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">192.168.20.107  mysql1</span><br><span class="line">192.168.20.109  mysql2</span><br><span class="line">192.168.20.137  mycat</span><br></pre></td></tr></table></figure>

<p><strong>（3）配置YUM源</strong></p>
<p>两个节点均使用提供的mariadb–10.3.23-repo.tar.gz的压缩包，解压并放在&#x2F;opt目录下，进入&#x2F;etc&#x2F;yum.repos.d目录下，将原来的repo文件移除，新建local.repo文件并编辑内容，具体操作命令如下：</p>
<p><code>curl -O http://172.19.25.11/mariadb-10.3.23-repo.tar.gz</code></p>
<p> <code>tar -zxvf mariadb-10.3.23-repo.tar.gz -C /opt</code></p>
<p><code>rm -rf /etc/yum.repos.d/*</code></p>
<p><code>vi /etc/yum.repos.d/local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line">name=mariadb</span><br><span class="line">baseurl=file:///opt/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><strong>（4）安装数据库服务并启动</strong></p>
<p>配置完毕后，两个节点安装数据库服务，命令如下：</p>
<p><code>yum install -y mariadb mariadb-server</code></p>
<p><code>systemctl start mariadb</code></p>
<p><code>systemctl enable mariadb</code></p>
<p><strong>(5）初始化数据库</strong></p>
<p>两个节点初始化数据库，配置数据库root密码为000000，命令如下：</p>
<p><code>mysql_secure_installation</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/bin/mysql_secure_installation: line 379: find_mysql_client: command not found</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB</span><br><span class="line"></span><br><span class="line">   SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!</span><br><span class="line"></span><br><span class="line">In order to log into MariaDB to secure it, we&#x27;ll need the current</span><br><span class="line">password for the root user. If you&#x27;ve just installed MariaDB, and</span><br><span class="line">you haven&#x27;t set the root password yet, the password will be blank,</span><br><span class="line">so you should just press enter here.</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Enter current password for root (enter for none):     #默认按Enter键</span><br><span class="line">OK, successfully used password, moving on...</span><br><span class="line"></span><br><span class="line">Setting the root password ensures that nobody can log into the MariaDB</span><br><span class="line">root user without the proper authorisation.</span><br><span class="line"></span><br><span class="line">Set root password? [Y/n] y</span><br><span class="line"></span><br><span class="line">New password:                 #输入数据库root密码000000</span><br><span class="line">Re-enter new password:             #再次输入密码000000</span><br><span class="line">Password updated successfully!</span><br><span class="line">Reloading privilege tables..</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">By default, a MariaDB installation has an anonymous user, allowing anyone</span><br><span class="line">to log into MariaDB without having to have a user account created for</span><br><span class="line">them. This is intended only for testing, and to make the installation</span><br><span class="line">go a bit smoother. You should remove them before moving into a</span><br><span class="line">production environment.</span><br><span class="line"></span><br><span class="line">Remove anonymous users? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"> </span><br><span class="line">Normally, root should only be allowed to connect from &#x27;localhost&#x27;. This</span><br><span class="line">ensures that someone cannot guess at the root password from the network.</span><br><span class="line"> </span><br><span class="line">Disallow root login remotely? [Y/n] n</span><br><span class="line"> ... skipping.</span><br><span class="line"></span><br><span class="line">By default, MariaDB comes with a database named &#x27;test&#x27; that anyone can</span><br><span class="line">access. This is also intended only for testing, and should be removed</span><br><span class="line">before moving into a production environment.</span><br><span class="line"></span><br><span class="line">Remove test database and access to it? [Y/n] y</span><br><span class="line"> \- Dropping test database...</span><br><span class="line"> ... Success!</span><br><span class="line"> \- Removing privileges on test database...</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">Reloading the privilege tables will ensure that all changes made so far</span><br><span class="line">will take effect immediately.</span><br><span class="line"></span><br><span class="line">Reload privilege tables now? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"> </span><br><span class="line">Cleaning up...</span><br><span class="line"></span><br><span class="line">All done! If you&#x27;ve completed all of the above steps, your MariaDB</span><br><span class="line">installation should now be secure.</span><br><span class="line"></span><br><span class="line">Thanks for using MariaDB!</span><br></pre></td></tr></table></figure>

<p><strong>（6）配置mysql1主节点</strong></p>
<p>修改mysql1节点的数据库配置文件，在配置文件&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf中的[mysqld]增添如下内容。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# cat /etc/my.cnf.d/server.cnf</span><br><span class="line">... ...</span><br><span class="line">[mysqld]</span><br><span class="line">log_bin = mysql-bin            #记录操作日志</span><br><span class="line">binlog_ignore_db = mysql         #不同步MySQL系统数据库</span><br><span class="line">server_id = 12              #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例如172.30.11.12，server_id就写12</span><br></pre></td></tr></table></figure>

<p>重启数据库服务，并进入数据库，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# systemctl restart mariadb</span><br><span class="line">[root@mysql1 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 9</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"> </span><br><span class="line">MariaDB [(none)]&gt; </span><br></pre></td></tr></table></figure>

<p>在mysql1节点，授权在任何客户端机器上可以以root用户登录到数据库，然后在主节点上创建一个user用户连接节点mysql2，并赋予从节点同步主节点数据库的权限。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; grant all privileges on *.* to root@&#x27;%&#x27; identified by &quot;000000&quot;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; grant replication slave on *.* to &#x27;user&#x27;@&#x27;mysql2&#x27; identified by &#x27;000000&#x27;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><strong>（7）配置mysql2从节点</strong></p>
<p>修改mysql2节点的数据库配置文件，在配置文件&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf中的[mysqld]增添如下内容。</p>
<p><code>[root@mysql2 ~]# cat /etc/my.cnf.d/server.cnf</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">... ...</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">log_bin = mysql-bin            #记录操作日志</span><br><span class="line">binlog_ignore_db = mysql         #不同步MySQL系统数据库</span><br><span class="line">server_id = 13              #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例如172.30.11.13，server_id就写13</span><br><span class="line"></span><br><span class="line">... ...</span><br></pre></td></tr></table></figure>

<p>修改完配置文件后，重启数据库服务，并在从节点mysql2上登录MariaDB数据库，配置从节点连接主节点的连接信息。master_host为主节点主机名mysql1，master_user为上一步中创建的用户user，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql2 ~]# systemctl restart mariadb</span><br><span class="line">[root@mysql2 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 9</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; change master to master_host=&#x27;mysql1&#x27;,master_user=&#x27;user&#x27;,master_password=&#x27;000000&#x27;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>

<p>配置完毕主从数据库之间的连接信息之后，开启从节点服务。使用show slave status\G命令，并查看从节点服务状态，如果Slave_IO_Running和Slave_SQL_Running的状态都为YES，则从节点服务开启成功。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; start slave;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show slave status\G</span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">        Slave_IO_State: Waiting for master to send event</span><br><span class="line">          Master_Host: mysql1</span><br><span class="line">          Master_User: user</span><br><span class="line">         Master_Port: 3306</span><br><span class="line">         Connect_Retry: 60</span><br><span class="line">........</span><br><span class="line">      Slave_IO_Running: Yes</span><br><span class="line">       Slave_SQL_Running: Yes</span><br><span class="line">.........</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到Slave_IO_Running和Slave_SQL_Running的状态都是Yes，配置数据库主从集群成功。</p>
<p>【题目8】云平台安全策略提升</p>
<p>使用OpenStack私有云平台，通过提供的相关软件包，安装必要组件，将私有云平台的访问策略从http提升至https。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><code>yum -y install mod_ssl</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/openstack-dashboard/local_settings</span><br><span class="line">##在DEBUG = False下增加4行</span><br><span class="line">USE_SSL = True</span><br><span class="line">CSRF_COOKIE_SECURE = True                              ##原文中有，去掉注释即可</span><br><span class="line">SESSION_COOKIE_SECURE = True                       ##原文中有，去掉注释即可</span><br><span class="line">SESSION_COOKIE_HTTPONLY = True</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/httpd/conf.d/ssl.conf</span><br><span class="line">##将SSLProtocol all -SSLv2 -SSLv3改成：</span><br><span class="line">SSLProtocol all -SSLv2</span><br></pre></td></tr></table></figure>

<p> <code>systemctl restart httpd</code></p>
<p><code>systemctl restart memcached</code></p>
<p>【任务4】OpenStack运维开发任务[15分]</p>
<p>【适用平台】私有云</p>
<p>本任务只公布考试范围，不公布赛题</p>
<p>此任务包含2-4个子任务，内容涉及编写Python脚本调用OpenStack API对Openstack云平台进行管理和运维。使用自动化运维工具ansible对云主机进行批量系统配置、批量程序部署、批量运行命令等运维操作。</p>
<p>【任务5】Docker CE及私有仓库安装任务（5分）</p>
<p>【适用平台】私有云</p>
<p>【题目1】安装Docker CE和Docker Compose</p>
<p>在master、node各节点中分别安装DockerCE和docker-compose。完成后提交master节点的用户名、密码和IP到答题框。</p>
<p>脚本安装</p>
<p>【题目2】安装私有仓库</p>
<p>脚本安装</p>
<p>【题目3】容器编排</p>
<p>在master节点上编写&#x2F;root&#x2F;wordpress&#x2F;docker-compose.yaml文件，具体要求如下：</p>
<p>（1）容器名称：wordpress；镜像：wordpress:latest；端口映射：82:80；</p>
<p>（2）容器名称：mysql；镜像：mysql:5.6； </p>
<p>（3）MySQL root用户密码：123456；</p>
<p>（4）创建数据库wordpress。</p>
<p>完成后编排部署WordPress，并提交master节点的用户名、密码和IP到答题框。</p>
<p> <code>cd /root</code></p>
<p><code>vi docker-compose.yaml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3.3&#x27;</span><br><span class="line">services:</span><br><span class="line">   mysql:</span><br><span class="line">     image: mysql:5.6</span><br><span class="line">     restart: always</span><br><span class="line">     environment:</span><br><span class="line">       MYSQL_ROOT_PASSWORD: 123456</span><br><span class="line">       MYSQL_DATABASE: wordpress</span><br><span class="line">       MYSQL_USER: wordpress</span><br><span class="line">       MYSQL_PASSWORD: wordpress</span><br><span class="line">   wordpress:</span><br><span class="line">     depends_on:</span><br><span class="line">       - mysql</span><br><span class="line">     image: wordpress:latest</span><br><span class="line">     ports:</span><br><span class="line">       - &quot;82:80&quot;</span><br><span class="line">     restart: always</span><br><span class="line">     environment:</span><br><span class="line">       WORDPRESS_DB_HOST: mysql:3306</span><br><span class="line">       WORDPRESS_DB_USER: wordpress</span><br><span class="line">       WORDPRESS_DB_PASSWORD: wordpress</span><br><span class="line">       WORDPRESS_DB_NAME: wordpress</span><br></pre></td></tr></table></figure>

<p>开始部署</p>
<p><code>docker-compose up -d</code></p>
<p>查看是否部署成功</p>
<p><code>docker ps</code></p>
<p>打开浏览器访问192.168.20.132:82</p>
<p>【题目4】容器基础操作</p>
<p>在master节点上停止并删除上述部署的wordpress容器。完成后提交master节点的用户名、密码和IP到答题框。</p>
<p>docker-compose down</p>
<p>【任务6】基于Docker容器的web应用系统部署[15分]</p>
<p>将该公司开发的基于微服务架构的web商城应用系统实现全容器化部署。商城应用系统架构图如下：</p>
<img src="/2022/07/01/2022%E6%B5%99%E6%B1%9F%E7%9C%81%E4%BA%91%E8%AE%A1%E7%AE%97%E6%A0%B7%E9%A2%98/clip_image002.gif" class="" title="img">

<table>
<thead>
<tr>
<th>模块</th>
<th>使用技术</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>gpmall.sql</td>
<td>Mysql</td>
<td>网站的数据库</td>
</tr>
<tr>
<td>dist目录</td>
<td>Nginx</td>
<td>网站的前端项目</td>
</tr>
<tr>
<td>gpmall-shopping</td>
<td>web项目</td>
<td>8081端口，商品&#x2F;购物车&#x2F;首页渲染等交互</td>
</tr>
<tr>
<td>gpmall-user</td>
<td>8082端口，提供用户相关的交互，如登录、注册、个人中心等</td>
<td></td>
</tr>
<tr>
<td>user-provider</td>
<td>后端服务</td>
<td>提供用户相关服务</td>
</tr>
<tr>
<td>shopping-provider</td>
<td>提供购物车、推荐商品、商品等服务</td>
<td></td>
</tr>
</tbody></table>
<p>【适用平台】私有云</p>
<p>【题目1】容器化部署Redis</p>
<p>在master节点上编写&#x2F;root&#x2F;redis&#x2F;Dockerfile文件，基于提供的软件包gpmall-single.tar构建chinaskill-redis:v1.1镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；</p>
<p>（2）作者：Chinaskill；</p>
<p>（3）修改配置文件中的bind 127.0.0.1为bind 0.0.0.0；</p>
<p>（4）设置Redis免密，并关闭保护模式；</p>
<p>（5）开放端口：6379；</p>
<p>（6）设置服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的用户名、密码和IP到答题框。</p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">RUN yum -y install redis</span><br><span class="line">RUN sed -i &#x27;s/bind 127.0.0.1/bind 0.0.0.0/g&#x27; /etc/redis.conf &amp;&amp; sed -i &#x27;s/protected-mode yes/protected-mode no/g&#x27; /etc/redis.conf</span><br><span class="line">EXPOSE 6379</span><br><span class="line">CMD [&quot;redis-server&quot;,&quot;/etc/redis.conf&quot;]</span><br></pre></td></tr></table></figure>

<p><code>vi local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[docker]</span><br><span class="line">baseurl=http://172.19.25.11/paas/kubernetes-repo/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[centos]</span><br><span class="line">baseurl=ftp://192.168.20.132/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><code>docker build -t chinaskill-redis:v1.1 .</code></p>
<p>【题目2】容器化部署MariaDB</p>
<p>在master节点上编写&#x2F;root&#x2F;mariadb&#x2F;Dockerfile文件，基于提供的软件包gpmall-single.tar构建chinaskill-mariadb:v1.1镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；</p>
<p>（2）作者：Chinaskill；</p>
<p>（3）设置数据库密码：123456；</p>
<p>（4）创建数据库gpmall并导入数据库文件gpmall.sql；</p>
<p>（5）设置字符编码：UTF-8；</p>
<p>（6）开放端口：3306；</p>
<p>（7）设置服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的用户名、密码和IP到答题框。</p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">ADD gpmall.sql /opt/</span><br><span class="line">ADD start.sh /opt/</span><br><span class="line">RUN yum -y install mariadb-server \</span><br><span class="line">&amp;&amp; chmod +x /opt/start.sh \</span><br><span class="line">&amp;&amp; /opt/start.sh</span><br><span class="line">EXPOSE 3306</span><br><span class="line">ENV LC_ALL en_US.UTF-8</span><br><span class="line">CMD mysqld_safe</span><br></pre></td></tr></table></figure>

<p><code>cp /root/redis/local.repo /root/mariadb/</code></p>
<p><code>cp /opt/ChinaskillMall/gpmall.sql /root/mariadb/</code></p>
<p><code>vi start.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">mysql_install_db --user=mysql</span><br><span class="line">mysqld_safe &amp;</span><br><span class="line">sleep 3</span><br><span class="line">mysqladmin -u root password &#x27;123456&#x27;</span><br><span class="line">mysql -uroot -p123456 -e &quot;grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;&quot;</span><br><span class="line">mysql -uroot -p123456 -e &quot;create database gpmall;use gpmall;source /opt/gpmall.sql;&quot;</span><br></pre></td></tr></table></figure>

<p><code>docker build -t chinaskill-mariadb:v1.1 .</code></p>
<p>【题目3】容器化部署Zookeeper</p>
<p>在master节点上编写&#x2F;root&#x2F;zookeeper&#x2F;Dockerfile文件，基于提供的软件包gpmall-single.tar构建chinaskill-zookeeper:v1.1镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；</p>
<p>（2）作者：Chinaskill；</p>
<p>（3）开放端口：2181；</p>
<p>（4）设置服务开机自启。</p>
<p>完成后构建镜像，使用构建的镜像运行容器myzookeeper，并提交master节点的用户名、密码和IP到答题框。</p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">EXPOSE 2181</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/ftp.repo</span><br><span class="line">ADD zookeeper-3.4.14.tar.gz /opt</span><br><span class="line">RUN yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel \</span><br><span class="line">&amp;&amp; mv /opt/zookeeper-3.4.14/conf/zoo_sample.cfg /opt/zookeeper-3.4.14/conf/zoo.cfg</span><br><span class="line">CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;/opt/zookeeper-3.4.14/bin/zkServer.sh start &amp;&amp; tail -f /etc/shadow&quot;]</span><br></pre></td></tr></table></figure>

<p><code>cp /opt/ChinaskillMall/zookeeper-3.4.14.tar.gz /root/zookeeper/</code></p>
<p><code>docker build -t chinaskill-zookeeper:v1.1 .</code></p>
<p>【题目4】容器化部署Kafka</p>
<p>在master节点上编写&#x2F;root&#x2F;kafka&#x2F;Dockerfile文件，基于提供的软件包gpmall-single.tar构建chinaskill-kafka:v1.1镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；</p>
<p>（2）作者：Chinaskill；</p>
<p>（3）开放端口：9092；</p>
<p>（4）设置服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的用户名、密码和IP到答题框。</p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">ADD zookeeper-3.4.14.tar.gz /opt</span><br><span class="line">ADD kafka_2.11-1.1.1.tgz /opt</span><br><span class="line">RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel \</span><br><span class="line"> &amp;&amp; mv /opt/zookeeper-3.4.14/conf/zoo_sample.cfg /opt/zookeeper-3.4.14/conf/zoo.cfg</span><br><span class="line">EXPOSE 9092</span><br><span class="line">CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;/opt/zookeeper-3.4.14/bin/zkServer.sh start &amp;&amp; /opt/kafka_2.11-1.1.1/bin/kafka-server-start.sh /opt/kafka_2.11-1.1.1/config/server.properties&quot;]</span><br></pre></td></tr></table></figure>

<p><code>cp /opt/ChinaskillMall/kafka_2.11-1.1.1.tgz /root/kafka/</code></p>
<p><code>cp /opt/ChinaskillMall/zookeeper-3.4.14.tar.gz /root/kafka/</code></p>
<p><code>cp /root/redis/local.repo /root/kafka</code></p>
<p><code>docker build -t chinaskill-kafka:v1.1 .</code></p>
<p>【题目5】容器化部署Nginx</p>
<p>在master节点上编写&#x2F;root&#x2F;nginx&#x2F;Dockerfile文件，基于提供的软件包gpmall-single.tar构建chinaskill-nginx:v1.1镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；</p>
<p>（2）作者：Chinaskill；</p>
<p>（3）编写&#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;default.conf文件，配置反向代理，将80端口请求转发到8081、8082和8083；</p>
<p>（4）将dist.tar解压并复制到&#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;目录下；</p>
<p>（5）开放端口：80、443、8081、8082、8083；</p>
<p>（6）设置服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的用户名、密码和IP到答题框。</p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">ADD *.jar /root/</span><br><span class="line">ADD setup.sh /root/</span><br><span class="line">RUN yum install -y nginx java-1.8.0-openjdk java-1.8.0-openjdk-devel </span><br><span class="line">RUN sed -i &#x27;1a location /shopping &#123; proxy_pass http://127.0.0.1:8081; &#125;&#x27; /etc/nginx/conf.d/default.conf </span><br><span class="line">RUN sed -i &#x27;2a location /user &#123; proxy_pass http://127.0.0.1:8082; &#125;&#x27; /etc/nginx/conf.d/default.conf </span><br><span class="line">RUN sed -i &#x27;3a location /casher &#123; proxy_pass http://127.0.0.1:8083; &#125;&#x27; /etc/nginx/conf.d/default.conf </span><br><span class="line">RUN chmod +x /root/setup.sh </span><br><span class="line">RUN rm -rf /usr/share/nginx/html/</span><br><span class="line">EXPOSE 80 443 8081 8082 8083 </span><br><span class="line">ADD dist/ /usr/share/nginx/html/</span><br><span class="line">CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]</span><br></pre></td></tr></table></figure>

<p><code>vi local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[centos]</span><br><span class="line">baseurl=ftp://192.168.20.132/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[docker]</span><br><span class="line">baseurl=ftp://192.168.20.132/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[gpmal]</span><br><span class="line">baseurl=http://172.19.25.11/paas/ChinaskillMall/gpmall-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><code>cp /opt/ChinaskillMall/*.jar /root/nginx/</code></p>
<p><code>cp /opt/ChinaskillMall/dist/ /root/nginx/</code></p>
<p><code>vi setup.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">nohup java -jar /root/shopping-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /root/user-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /root/gpmall-shopping-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /root/gpmall-user-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br></pre></td></tr></table></figure>

<p><code>docker build -t chinaskill-nginx:v1.1 .</code></p>
<p>【题目6】编排部署GPMall商城</p>
<p>在master节点上编写&#x2F;root&#x2F;chinaskillmall&#x2F;docker-compose.yaml文件，具体要求如下：</p>
<p>（1）容器1名称：mysql；镜像：chinaskill-mariadb:v1.1；端口映射：3306:3306；</p>
<p>（2）容器2名称：redis；镜像：chinaskill-redis:v1.1；端口映射：6379:6379；</p>
<p>（3）容器3名称：kafka；镜像：chinaskill-kafka:v1.1；端口映射：9092:9092；</p>
<p>（4）容器4名称：zookeeper；镜像：chinaskill-zookeeper:v1.1；端口映射：2181:2181；</p>
<p>（5）容器5名称：nginx；镜像：chinaskill-nginx:v1.1；端口映射：80:80，443:443。</p>
<p>完成后编排部署GPMall，并提交master节点的用户名、密码和IP到答题框。</p>
<p><code>vi docker-compose.yaml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3.3&#x27;</span><br><span class="line">services:</span><br><span class="line">  mall-mysql:</span><br><span class="line">    image: chinaskill-mariadb:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 3306:3306</span><br><span class="line"></span><br><span class="line">  mall-redis:</span><br><span class="line">    image: chinaskill-redis:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 6379:6379</span><br><span class="line"></span><br><span class="line">  mall-kafka:</span><br><span class="line">    image: chinaskill-kafka:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 9092:9092</span><br><span class="line"></span><br><span class="line">  mall-zookeeper:</span><br><span class="line">    image: chinaskill-zookeeper:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 2181:2181</span><br><span class="line"></span><br><span class="line">  mall-nginx:</span><br><span class="line">    image: chinaskill-nginx:v1.1</span><br><span class="line">    depends_on:</span><br><span class="line">      - mall-mysql</span><br><span class="line">      - mall-redis</span><br><span class="line">      - mall-zookeeper</span><br><span class="line">      - mall-kafka</span><br><span class="line">    links:</span><br><span class="line">      - mall-mysql:mysql.mall</span><br><span class="line">      - mall-redis:redis.mall</span><br><span class="line">      - mall-zookeeper:zookeeper.mall</span><br><span class="line">      - mall-kafka:kafka.mall</span><br><span class="line">    ports:</span><br><span class="line">      - 83:80</span><br><span class="line">      - 443:443</span><br><span class="line">    command: [&quot;sh&quot;,&quot;-c&quot;,&quot;/root/setup.sh &amp;&amp; nginx &amp;&amp; tail -f /etc/shadow&quot;]</span><br></pre></td></tr></table></figure>

<p><code>docker-compose up -d</code></p>
<p>查看是否开启成功</p>
<p><code>docker ps</code></p>
]]></content>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>ansible</title>
    <url>/2022/06/24/ansible/</url>
    <content><![CDATA[<h1 id="Ansible-部署服务"><a href="#Ansible-部署服务" class="headerlink" title="Ansible 部署服务"></a>Ansible 部署服务<span id="more"></span></h1><p>使用赛项提供的 OpenStack 私有云平台，创建 2 台系统为 centos7.5 的云主机，其中一台作为 ansible 的母机并命名为 ansible，另外一台云主机命名为 node1，通过 http 服务中的ansible.tar.gz 软件包在 ansible 节点安装 ansible 服务；并用这台母机，编写 ansible 脚本（在&#x2F;root 目 录 下 创 建 ansible_ftp 目 录 作 为 ansible 工 作 目 录 ， 部 署 的 入 口 文 件 命 名 为install_ftp.yaml）。install_ftp.yaml 文件中需要完成的内容为<br>（1）yaml 中被执行节点为 node1，执行者为 root；<br>（2）使用 copy 模块将 ansible 节点的 local.repo 传到 node 节点；（local.repo 用于配置2021 年职业院校技能大赛“云计算”赛项 赛卷node 节点的 yum 源，可自行创建）<br>（3）使用 yum 模块安装 ftp 服务；<br>（4）使用 service 模块启动 ftp 服务。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ansible openstack]# cat ftpd.yaml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">- name: install ftpd</span><br><span class="line">  hosts: compute</span><br><span class="line">  tasks:</span><br><span class="line">    - name: copy local.repo to compute</span><br><span class="line">      copy:</span><br><span class="line">        src: /etc/yum.repos.d/http.repo</span><br><span class="line">        dest: /etc/yum.repos.d/local.repo</span><br><span class="line"></span><br><span class="line">    - name: install ftpd </span><br><span class="line">      yum:</span><br><span class="line">        name:</span><br><span class="line">          - vsftpd</span><br><span class="line">        state: latest</span><br><span class="line"></span><br><span class="line">    - name: enable ftpd service</span><br><span class="line">      service:</span><br><span class="line">        name: vsftpd</span><br><span class="line">        state: started</span><br><span class="line">        enabled: true</span><br></pre></td></tr></table></figure>

<h1 id="ansible搭建openstack"><a href="#ansible搭建openstack" class="headerlink" title="ansible搭建openstack"></a><strong>ansible搭建openstack</strong></h1><p><code>yum install -y ansible</code></p>
<p><code>mkdir openstack</code></p>
<p><code>cd openstack</code></p>
<p><code>cp /etc/ansible/ansible.cfg .</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ansible openstack]# cat /etc/ansible/hosts </span><br><span class="line"></span><br><span class="line">[controller]</span><br><span class="line">192.168.20.113</span><br><span class="line"></span><br><span class="line">[compute]</span><br><span class="line">192.168.20.120</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat ansible.cfg</span><br><span class="line">[defaults]</span><br><span class="line">inventory      = /etc/ansible/hosts</span><br><span class="line">remote_user = root</span><br><span class="line">[privilege_escalation]</span><br><span class="line">become=True</span><br><span class="line">become_method=sudo</span><br><span class="line">become_user=root</span><br><span class="line">become_ask_pass=False</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ansible openstack]# cat openstack_start.yaml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">- name: init</span><br><span class="line">  hosts: all</span><br><span class="line">  roles:</span><br><span class="line">    - init</span><br><span class="line"></span><br><span class="line">- name: openrc</span><br><span class="line">  hosts: all</span><br><span class="line">  roles:</span><br><span class="line">    - jinjia2</span><br><span class="line"></span><br><span class="line">- name: install controller</span><br><span class="line">  hosts: controller</span><br><span class="line">  roles:</span><br><span class="line">    - mariadb</span><br><span class="line">    - keystone</span><br><span class="line">    - glance</span><br><span class="line">    - nova-controller</span><br><span class="line">    - neutron-controller</span><br><span class="line">    - dashboard</span><br><span class="line">    - cinder-controller</span><br><span class="line">    - swift-controller</span><br><span class="line">    - heat</span><br><span class="line">- name: install compute</span><br><span class="line">  hosts: compute</span><br><span class="line">  roles:</span><br><span class="line">    - nova-compute</span><br><span class="line">    - neutron-compute</span><br><span class="line">    - cinder-compute</span><br><span class="line">    - swift-compute</span><br></pre></td></tr></table></figure>

<p><code>for i in &#123;init,jinjia2,mariadb,keystone,glance,nova-controller,neutron-controller,dashboard,cinder-controller,swift-controller,heat,nova-compute,neutron-compute,cinder-compute,swift-compute&#125;;do ansible-galaxy init roles/$i ;done</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ansible openstack]# cat roles/init/tasks/main.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">- name: init controller</span><br><span class="line">  block:</span><br><span class="line"></span><br><span class="line">    - name: hostname</span><br><span class="line">      shell: hostnamectl set-hostname controller</span><br><span class="line"></span><br><span class="line">    - name: bash</span><br><span class="line">      shell: bash</span><br><span class="line"></span><br><span class="line">    - name: delete yum</span><br><span class="line">      shell: rm -rf /etc/yum.repos.d/*</span><br><span class="line"></span><br><span class="line">    - name: copy yum</span><br><span class="line">      copy:</span><br><span class="line">        src: /etc/yum.repos.d/http.repo</span><br><span class="line">        dest: /etc/yum.repos.d/local.repo</span><br><span class="line">      when: ansible_hostname == &#x27;controller&#x27;</span><br><span class="line"></span><br><span class="line">- name: init compute</span><br><span class="line">  block:</span><br><span class="line"></span><br><span class="line">    - name: hostname</span><br><span class="line">      shell: hostnamectl set-hostname compute</span><br><span class="line"></span><br><span class="line">    - name: bash</span><br><span class="line">      shell: bash</span><br><span class="line"></span><br><span class="line">    - name: delete yum</span><br><span class="line">      shell: rm -rf /etc/yum.repos.d/*</span><br><span class="line"></span><br><span class="line">    - name: copy yum</span><br><span class="line">      copy:</span><br><span class="line">        src: /etc/yum.repos.d/http.repo</span><br><span class="line">        dest: /etc/yum.repos.d/local.repo</span><br><span class="line"></span><br><span class="line">    - name: umount</span><br><span class="line">      mount: </span><br><span class="line">        path: /mnt</span><br><span class="line">        state: unmounted</span><br><span class="line"></span><br><span class="line">    - name: part1</span><br><span class="line">      parted:</span><br><span class="line">        device: /dev/vdb</span><br><span class="line">        number: 1</span><br><span class="line">        state: present</span><br><span class="line">        part_end: 20GiB</span><br><span class="line"></span><br><span class="line">    - name: part2</span><br><span class="line">      parted:</span><br><span class="line">        device: /dev/vdb</span><br><span class="line">        number: 2</span><br><span class="line">        state: present</span><br><span class="line">        part_start: 20GiB</span><br><span class="line">        part_end: 40GiB</span><br><span class="line"></span><br><span class="line">  when: ansible_hostname == &#x27;compute&#x27;</span><br><span class="line"></span><br><span class="line">- name: install</span><br><span class="line">  yum: </span><br><span class="line">    name: iaas-xiandian</span><br><span class="line">    state: latest</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ansible openstack]# cat roles/jinjia2/tasks/main.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># tasks file for roles/jinjia2</span><br><span class="line"></span><br><span class="line">- name: jinjia2</span><br><span class="line">  block:</span><br><span class="line">    - name: template</span><br><span class="line">      template: </span><br><span class="line">        src: ../templates/openrc.sh.j2</span><br><span class="line">        dest: /etc/xiandian/openrc.sh</span><br><span class="line">    - name: pre-host</span><br><span class="line">      shell: iaas-pre-host.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ansible openstack]# cat roles/mariadb/tasks/main.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># tasks file for roles/mariadb</span><br><span class="line"></span><br><span class="line">- name: install mariadb</span><br><span class="line">  shell: iaas-install-mysql.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@ansible openstack]# cat roles/keystone/tasks/main.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># tasks file for roles/keystone</span><br><span class="line"></span><br><span class="line">- name: install keystone</span><br><span class="line">  shell: iaas-install-keystone.sh</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/04/20/docker%E9%83%A8%E7%BD%B2prometheus/</url>
    <content><![CDATA[<h1 id="dockerfile部署prometheus"><a href="#dockerfile部署prometheus" class="headerlink" title="dockerfile部署prometheus"></a>dockerfile部署prometheus</h1><h4 id="1、【实操题】容器化部署Node-Exporter-（1分）"><a href="#1、【实操题】容器化部署Node-Exporter-（1分）" class="headerlink" title="1、【实操题】容器化部署Node-Exporter （1分）"></a>1、【实操题】容器化部署Node-Exporter （1分）<span id="more"></span></h4><p>在master节点上编写&#x2F;root&#x2F;Monitor&#x2F;Dockerfile- exporter文件构建monitor-exporter:v1.0镜像，具体要求如下：（需要用到的软件包：Monitor.tar.gz）</p>
<p>（1）基础镜像：centos:centos7.9.2009；</p>
<p>（2）使用二进制包node_exporter-0.18.1.linux-amd64.tar.gz安装node-exporter服务；</p>
<p>（3）声明端口：9100；</p>
<p>（4）设置服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat Dockerfile-exporter </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD node_exporter-0.18.1.linux-amd64.tar.gz /usr/local</span><br><span class="line">RUN chmod +x /usr/local/node_exporter-0.18.1.linux-amd64/node_exporter</span><br><span class="line">EXPOSE 9100</span><br><span class="line">CMD [&quot;/usr/local/node_exporter-0.18.1.linux-amd64/node_exporter&quot;]</span><br></pre></td></tr></table></figure>



<h4 id="2、【实操题】容器化部署Alertmanager-（1-5分）"><a href="#2、【实操题】容器化部署Alertmanager-（1-5分）" class="headerlink" title="2、【实操题】容器化部署Alertmanager （1.5分）"></a>2、【实操题】容器化部署Alertmanager （1.5分）</h4><p>在master节点上编写&#x2F;root&#x2F;Monitor&#x2F;Dockerfile-alert文件构建monitor-alert:v1.0镜像，具体要求如下：（需要用到的软件包：Monitor.tar.gz）</p>
<p>（1）基础镜像：centos:centos7.9.2009；</p>
<p>（2）使用提供的二进制包alertmanager-0.19.0.linux-amd64.tar.gz安装Alertmanager服务；</p>
<p>（3）声明端口：9093、9094；</p>
<p>（4）设置服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat alertmanager/alertmanager.yml </span><br><span class="line">global:</span><br><span class="line">resolve_timeout: 5m</span><br><span class="line">smtp_smarthost: &#x27;114463512@.163.com:465&#x27;</span><br><span class="line">smtp_from: &#x27;alert@163.com&#x27;</span><br><span class="line">smtp_auth_username: &#x27;114463512@163.com&#x27;</span><br><span class="line">smtp_auth_password: &#x27;flwlf[a;&#x27;     </span><br><span class="line">smtp_require_tls: false</span><br><span class="line"></span><br><span class="line">route:</span><br><span class="line">  receiver: &quot;default&quot;</span><br><span class="line">  group_wait: 10s</span><br><span class="line">  group_interval: 1m</span><br><span class="line">  repeat_interval: 1h</span><br><span class="line">  group_by: [&#x27;alertname&#x27;]</span><br><span class="line"></span><br><span class="line">inhibit_rules:</span><br><span class="line">- source_match:</span><br><span class="line">  severity: &#x27;critical&#x27;</span><br><span class="line">  target_match:</span><br><span class="line">    severity: &#x27;warning&#x27;</span><br><span class="line">  equal: [&#x27;alertname&#x27;, &#x27;instance&#x27;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat alert-rules.yml </span><br><span class="line">groups:</span><br><span class="line"></span><br><span class="line">  - name: node-alert</span><br><span class="line">    rules:</span><br><span class="line">    - alert: NodeDown</span><br><span class="line">      expr: up&#123;job=&quot;node&quot;&#125; == 0</span><br><span class="line">      for: 5m</span><br><span class="line">      labels:</span><br><span class="line">        severity: critical</span><br><span class="line">        instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">      annotations:</span><br><span class="line">        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; down&quot;</span><br><span class="line">        description: &quot;Instance: &#123;&#123; $labels.instance &#125;&#125; 已经宕机 5分钟&quot;</span><br><span class="line">        value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">    - alert: NodeCpuHigh</span><br><span class="line">      expr: (1 - avg by (instance) (irate(node_cpu_seconds_total&#123;job=&quot;node&quot;,mode=&quot;idle&quot;&#125;[5m]))) * 100 &gt; 80</span><br><span class="line">      for: 5m</span><br><span class="line">      labels:</span><br><span class="line">        severity: warning</span><br><span class="line">        instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">      annotations:</span><br><span class="line">        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu使用率过高&quot;</span><br><span class="line">        description: &quot;CPU 使用率超过 80%&quot;</span><br><span class="line">        value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">    - alert: NodeCpuIowaitHigh</span><br><span class="line">      expr: avg by (instance) (irate(node_cpu_seconds_total&#123;job=&quot;node&quot;,mode=&quot;iowait&quot;&#125;[5m])) * 100 &gt; 50</span><br><span class="line">      for: 5m</span><br><span class="line">      labels:</span><br><span class="line">        severity: warning</span><br><span class="line">        instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">      annotations:</span><br><span class="line">        summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; cpu iowait 使用率过高&quot;</span><br><span class="line">        description: &quot;CPU iowait 使用率超过 50%&quot;</span><br><span class="line">        value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">    - alert: NodeLoad5High</span><br><span class="line">          expr: node_load5 &gt; (count by (instance) (node_cpu_seconds_total&#123;job=&quot;node&quot;,mode=&#x27;system&#x27;&#125;)) * 1.2</span><br><span class="line">          for: 5m</span><br><span class="line">          labels:</span><br><span class="line">            severity: warning</span><br><span class="line">            instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">          annotations:</span><br><span class="line">            summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; load(5m) 过高&quot;</span><br><span class="line">            description: &quot;Load(5m) 过高，超出cpu核数 1.2倍&quot;</span><br><span class="line">            value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line">        - alert: NodeMemoryHigh</span><br><span class="line">          expr: (1 - node_memory_MemAvailable_bytes&#123;job=&quot;node&quot;&#125; / node_memory_MemTotal_bytes&#123;job=&quot;node&quot;&#125;) * 100 &gt; 90</span><br><span class="line">          for: 5m</span><br><span class="line">          labels:</span><br><span class="line">            severity: warning</span><br><span class="line">            instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">          annotations:</span><br><span class="line">            summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; memory 使用率过高&quot;</span><br><span class="line">            description: &quot;Memory 使用率超过 90%&quot;</span><br><span class="line">            value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">        - alert: NodeDiskRootHigh</span><br><span class="line">          expr: (1 - node_filesystem_avail_bytes&#123;job=&quot;node&quot;,fstype=~&quot;ext.*|xfs&quot;,mountpoint =&quot;/&quot;&#125; / node_filesystem_size_bytes&#123;job=&quot;node&quot;,fstype=~&quot;ext.*|xfs&quot;,mountpoint =&quot;/&quot;&#125;) * 100 &gt;</span><br><span class="line">           90</span><br><span class="line">          for: 10m</span><br><span class="line">          labels:</span><br><span class="line">            severity: warning</span><br><span class="line">            instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">          annotations:</span><br><span class="line">            summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; disk(/ 分区) 使用率过高&quot;</span><br><span class="line">            description: &quot;Disk(/ 分区) 使用率超过 90%&quot;</span><br><span class="line">            value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line"> - alert: NodeDiskBootHigh</span><br><span class="line">   expr: (1 - node_filesystem_avail_bytes&#123;job=&quot;node&quot;,fstype=~&quot;ext.*|xfs&quot;,mountpoint =&quot;/boot&quot;&#125; / node_filesystem_size_bytes&#123;job=&quot;node&quot;,fstype=~&quot;ext.*|xfs&quot;,mountpoint =&quot;/boot&quot;&#125;)</span><br><span class="line"></span><br><span class="line"> * 100 &gt; 80</span><br><span class="line">   for: 10m</span><br><span class="line">   labels:</span><br><span class="line">     severity: warning</span><br><span class="line">     instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">   annotations:</span><br><span class="line">     summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; disk(/boot 分区) 使用率过高&quot;</span><br><span class="line">     description: &quot;Disk(/boot 分区) 使用率超过 80%&quot;</span><br><span class="line">     value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">   - alert: NodeDiskReadHigh</span><br><span class="line">     expr: irate(node_disk_read_bytes_total&#123;job=&quot;node&quot;&#125;[5m]) &gt; 20 * (1024 ^ 2)</span><br><span class="line">     for: 5m</span><br><span class="line">     labels:</span><br><span class="line">       severity: warning</span><br><span class="line">       instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">     annotations:</span><br><span class="line">       summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; disk 读取字节数 速率过高&quot;</span><br><span class="line">       description: &quot;Disk 读取字节数 速率超过 20 MB/s&quot;</span><br><span class="line">       value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">   - alert: NodeDiskWriteHigh</span><br><span class="line">     expr: irate(node_disk_written_bytes_total&#123;job=&quot;node&quot;&#125;[5m]) &gt; 20 * (1024 ^ 2)</span><br><span class="line">     for: 5m</span><br><span class="line">     labels:</span><br><span class="line">       severity: warning</span><br><span class="line">       instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">     annotations:</span><br><span class="line">       summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; disk 写入字节数 速率过高&quot;</span><br><span class="line">       description: &quot;Disk 写入字节数 速率超过 20 MB/s&quot;</span><br><span class="line">       value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">   - alert: NodeDiskReadRateCountHigh</span><br><span class="line">     expr: irate(node_disk_reads_completed_total&#123;job=&quot;node&quot;&#125;[5m]) &gt; 3000</span><br><span class="line">     for: 5m</span><br><span class="line"></span><br><span class="line">   - alert: NodeDiskReadRateCountHigh</span><br><span class="line">         expr: irate(node_disk_reads_completed_total&#123;job=&quot;node&quot;&#125;[5m]) &gt; 3000</span><br><span class="line">         for: 5m</span><br><span class="line">         labels:</span><br><span class="line">           severity: warning</span><br><span class="line">           instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">         annotations:</span><br><span class="line">           summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; disk iops 每秒读取速率过高&quot;</span><br><span class="line">           description: &quot;Disk iops 每秒读取速率超过 3000 iops&quot;</span><br><span class="line">           value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line">       - alert: NodeDiskWriteRateCountHigh</span><br><span class="line">         expr: irate(node_disk_writes_completed_total&#123;job=&quot;node&quot;&#125;[5m]) &gt; 3000</span><br><span class="line">         for: 5m</span><br><span class="line">         labels:</span><br><span class="line">           severity: warning</span><br><span class="line">           instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">         annotations:</span><br><span class="line">           summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; disk iops 每秒写入速率过高&quot;</span><br><span class="line">           description: &quot;Disk iops 每秒写入速率超过 3000 iops&quot;</span><br><span class="line">           value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line">       - alert: NodeInodeRootUsedPercentHigh</span><br><span class="line">         expr: (1 - node_filesystem_files_free&#123;job=&quot;node&quot;,fstype=~&quot;ext4|xfs&quot;,mountpoint=&quot;/&quot;&#125; / node_filesystem_files&#123;job=&quot;node&quot;,fstype=~&quot;ext4|xfs&quot;,mountpoint=&quot;/&quot;&#125;) * 100 &gt; 80</span><br><span class="line">         for: 10m</span><br><span class="line">         labels:</span><br><span class="line">           severity: warning</span><br><span class="line">           instance: &quot;&#123;&#123; $labels.instance &#125;&#125;&quot;</span><br><span class="line">         annotations:</span><br><span class="line">           summary: &quot;instance: &#123;&#123; $labels.instance &#125;&#125; disk(/ 分区) inode 使用率过高&quot;</span><br><span class="line">           description: &quot;Disk (/ 分区) inode 使用率超过 80%&quot;</span><br><span class="line">           value: &quot;&#123;&#123; $value &#125;&#125;&quot;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat Dockerfile-alert </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD alertmanager-0.19.0.linux-amd64.tar.gz /usr/local/</span><br><span class="line">RUN chmod +x /usr/local/alertmanager-0.19.0.linux-amd64/alertmanager</span><br><span class="line">EXPOSE 9093</span><br><span class="line">EXPOSE 9094</span><br><span class="line">CMD [&quot;/usr/local/alertmanager-0.19.0.linux-amd64/alertmanager&quot;,&quot;--config.file=/usr/local/alertmanager-0.19.0.linux-amd64/alertmanager.yml&quot;]</span><br></pre></td></tr></table></figure>

<h4 id="3、【实操题】容器化部署Grafana（1-5分）"><a href="#3、【实操题】容器化部署Grafana（1-5分）" class="headerlink" title="3、【实操题】容器化部署Grafana（1.5分）"></a>3、【实操题】容器化部署Grafana（1.5分）</h4><p>在master节点上写&#x2F;root&#x2F;Monitor&#x2F;Dockerfile-grafana文件构建monitor-grafana:v1.0镜像，具体要求如下：（需要用到的软件包：Monitor.tar.gz）</p>
<p>（1）基础镜像：centos:centos7.9.2009；</p>
<p>（2）使用提供的二进制包grafana-6.4.1.linux-amd64.tar.gz安装grafana服务；</p>
<p>（3）声明端口：3000；</p>
<p>（4）设置nacos服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat Dockerfile-grafana </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD grafana-6.4.1.linux-amd64.tar.gz /usr/local/</span><br><span class="line">RUN chmod +x /usr/local/grafana-6.4.1/bin/grafana-server</span><br><span class="line">EXPOSE 3000</span><br><span class="line">CMD [&quot;/usr/local/grafana-6.4.1/bin/grafana-server&quot;,&quot;--config=/usr/local/grafana-6.4.1/conf/defaults.ini&quot;,&quot;--homepath=/usr/local/grafana-6.4.1&quot;] </span><br></pre></td></tr></table></figure>

<h4 id="4、【实操题】容器化部署Prometheus（1-5分）"><a href="#4、【实操题】容器化部署Prometheus（1-5分）" class="headerlink" title="4、【实操题】容器化部署Prometheus（1.5分）"></a>4、【实操题】容器化部署Prometheus（1.5分）</h4><p>在master节点上编写&#x2F;root&#x2F;Monitor&#x2F;Dockerfile-prometheus文件构建monitor-prometheus:v1.0镜像，具体要求如下：（需要用到的软件包：Monitor.tar.gz）</p>
<p>（1）基础镜像：centos:centos7.9.2009；</p>
<p>（2）使用提供的二进制包prometheus-2.13.0.linux-amd64.tar.gz安装promethues服务；</p>
<p>（3）编写prometheus.yml文件，创建3个任务模板：prometheus、node和alertmanager，并将该文件拷贝到&#x2F;data&#x2F;prometheus&#x2F;目录下；</p>
<p>（4）声明端口：9090；</p>
<p>（5）设置服务开机自启。</p>
<p>完成后构建镜像，并提交master节点的IP地址、用户名和密码到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 monint]# cat prometheus.yml </span><br><span class="line"></span><br><span class="line"># my global config</span><br><span class="line"></span><br><span class="line">global:</span><br><span class="line">  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"></span><br><span class="line">  # scrape_timeout is set to the global default (10s).</span><br><span class="line"></span><br><span class="line"># Alertmanager configuration</span><br><span class="line"></span><br><span class="line">alerting:</span><br><span class="line">  alertmanagers:</span><br><span class="line">    - static_configs:</span><br><span class="line">        - targets: </span><br><span class="line">          - 192.168.20.133:9093</span><br><span class="line"></span><br><span class="line"># Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.</span><br><span class="line"></span><br><span class="line">rule_files:</span><br><span class="line"></span><br><span class="line">  - &quot;rules/*.yml&quot;</span><br><span class="line"></span><br><span class="line">  # - &quot;second_rules.yml&quot;</span><br><span class="line"></span><br><span class="line"># A scrape configuration containing exactly one endpoint to scrape:</span><br><span class="line"></span><br><span class="line"># Here it&#x27;s Prometheus itself.</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line"></span><br><span class="line">  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span><br><span class="line"></span><br><span class="line">  - job_name: &quot;prometheus&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.20.133:9090&quot;]</span><br><span class="line"></span><br><span class="line">  - job_name: &quot;node&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.20.133:9100&quot;]</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD prometheus-2.13.0.linux-amd64.tar.gz /usr/local/</span><br><span class="line">RUN chmod +x /usr/local/prometheus-2.13.0.linux-amd64/prometheus</span><br><span class="line">ADD prometheus.yml /usr/local/prometheus-2.13.0.linux-amd64/</span><br><span class="line">RUN mkdir /usr/local/prometheus-2.13.0.linux-amd64/rules</span><br><span class="line">ADD alert-rules.yml /usr/local/prometheus-2.13.0.linux-amd64/rules </span><br><span class="line">EXPOSE 9090</span><br><span class="line">CMD [&quot;/usr/local/prometheus-2.13.0.linux-amd64/prometheus&quot;,&quot;--config.file=/usr/local/prometheus-2.13.0.linux-amd64/prometheus.yml&quot;]</span><br></pre></td></tr></table></figure>

<h4 id="5、【实操题】编排部署监控系统（4-5分）"><a href="#5、【实操题】编排部署监控系统（4-5分）" class="headerlink" title="5、【实操题】编排部署监控系统（4.5分）"></a>5、【实操题】编排部署监控系统（4.5分）</h4><p>在master节点上编写&#x2F;root&#x2F;Monitor&#x2F;docker-compose.yaml文件，具体要求如下：</p>
<p>（1）容器1名称：monitor-node；镜像：monitor-exporter:v1.0；端口映射：9100:9100；</p>
<p>（2）容器2名称：monitor- alertmanager；镜像：monitor-alert:v1.0；端口映射：9093:9093、9094:9094；</p>
<p>（3）容器3名称：monitor-grafana；镜像：monitor-grafana:v1.0；端口映射：3000:3000；</p>
<p>（4）容器4名称：monitor-prometheus；镜像：monitor-prometheus:v1.0；端口映射：9090:9090。</p>
<p>完成后编排部署监控系统，将Prometheus设置为Grafana的数据源，并命名为Prometheus。然后提交master节点的IP地址、用户名和密码到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat docker-compose.yaml </span><br><span class="line">version: &#x27;3&#x27;</span><br><span class="line">services:</span><br><span class="line">  node-exporter:</span><br><span class="line">    image: monitor-exporter:v1.0</span><br><span class="line">    container_name: &quot;monitor-node&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9100:9100&quot;</span><br><span class="line">    restart: always</span><br><span class="line">  alertmanager:</span><br><span class="line">    image: monitor-alert:v1.0</span><br><span class="line">    container_name: &quot;monitor-alertmanager&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9093:9093&quot;</span><br><span class="line">      - &quot;9094:9094&quot;</span><br><span class="line">    restart: always</span><br><span class="line">  grafana:</span><br><span class="line">    image: monitor-grafana:v1.0</span><br><span class="line">    container_name: &quot;monitor-grafana&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;3000:3000&quot;</span><br><span class="line">    restart: always</span><br><span class="line"></span><br><span class="line">  prometheus:</span><br><span class="line">    image: monitor-prometheus:v1.0</span><br><span class="line">    container_name: &quot;monitor-prometheus&quot;</span><br><span class="line">    depends_on:</span><br><span class="line">      - node-exporter</span><br><span class="line">      - alertmanager</span><br><span class="line">      - grafana</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9090:9090&quot;</span><br><span class="line">    restart: always </span><br></pre></td></tr></table></figure>

<h2 id="三、容器云应用搭建：基于Kubernetes构建CICD（当前任务共4道题目）"><a href="#三、容器云应用搭建：基于Kubernetes构建CICD（当前任务共4道题目）" class="headerlink" title="三、容器云应用搭建：基于Kubernetes构建CICD（当前任务共4道题目）"></a>三、容器云应用搭建：基于Kubernetes构建CICD（当前任务共4道题目）</h2><h4 id="1、【实操题】安装Jenkins环境（2分）"><a href="#1、【实操题】安装Jenkins环境（2分）" class="headerlink" title="1、【实操题】安装Jenkins环境（2分）"></a>1、【实操题】安装Jenkins环境（2分）</h4><p>使用镜像jenkins&#x2F;Jenkins:latest在Kubernetes集群devops命名空间下完成Jenkins的部署，Deployment和Service名称均为jenkins，要求以NodePort方式将Jenkins的8080端口对外暴露为30880，并使用提供的软件包完成Blue Ocean等离线插件的安装。部署完成后设置Jenkins用户名为jenkins；密码为000000，并在授权策略中配置“任何用户可以做任何事(没有任何限制)”。</p>
<p>完成后交master节点的用户名、密码和IP地址到答题框。（需要用到的软件包路径http:&#x2F;&#x2F;<IP>&#x2F;BlueOcean.tar.gz）</p>
<h4 id="2、【实操题】安装GitLab环境（2分）"><a href="#2、【实操题】安装GitLab环境（2分）" class="headerlink" title="2、【实操题】安装GitLab环境（2分）"></a>2、【实操题】安装GitLab环境（2分）</h4><p>使用镜像gitlab&#x2F;gitlab-ce:latest在Kubernetes集群devops命名空间下完成GitLab的部署，Deployment和Service名称均为gitlab，设置GitLab的root用户密码为admin@123，并以NodePort方式将GitLab的80端口对外暴露为30888。部署完成后新建公开项目springcloud，并将springcloud文件夹中的代码上传到该项目。</p>
<p>完成后提交master节点的用户名、密码和IP地址到答题框。（需要用到的软件包路径http:&#x2F;&#x2F;<IP>&#x2F;BlueOcean.tar.gz）</p>
<h3 id="3、【实操题】配置Jenkins连接GitLab（2分）"><a href="#3、【实操题】配置Jenkins连接GitLab（2分）" class="headerlink" title="3、【实操题】配置Jenkins连接GitLab（2分）"></a>3、【实操题】配置Jenkins连接GitLab（2分）</h3><p>在GitLab中生成名为jenkins的“Access Tokens”，在Jenkins中配置GitLab凭据并测试其连通性。</p>
<p>完成后提交master节点的用户名、密码和IP地址到答题框。（需要用到的软件包路径http:&#x2F;&#x2F;<IP>&#x2F;BlueOcean.tar.gz）</p>
<h3 id="4、【实操题】构建CI-x2F-CD（4分）"><a href="#4、【实操题】构建CI-x2F-CD（4分）" class="headerlink" title="4、【实操题】构建CI&#x2F;CD（4分）"></a>4、【实操题】构建CI&#x2F;CD（4分）</h3><p>在Jenkins中新建流水线任务springcloud，流水线选择“Pipeline script from SCM”。在springcloud项目中新建Jenkinsfile脚本文件，编写声明式Pipeline，要求完成构建maven项目，然后构建Docker镜像并推送到Harbor仓库的springcloud项目，并基于新构建的镜像完成config和gateway服务自动发布到Kubernetes集群springcloud命名空间下。最后配置Webhook触发构建。</p>
<p>完成后提交master节点的用户名、密码和IP地址到答题框。（需要用到的软件包路径http:&#x2F;&#x2F;<IP>&#x2F;BlueOcean.tar.gz）</p>
<h3 id="1、【实操题】WordPress上云–使用多容器Pod部署服务（2分）"><a href="#1、【实操题】WordPress上云–使用多容器Pod部署服务（2分）" class="headerlink" title="1、【实操题】WordPress上云–使用多容器Pod部署服务（2分）"></a>1、【实操题】WordPress上云–使用多容器Pod部署服务（2分）</h3><p>使用Deployment方式将WordPress博客系统部署到Kubernetes集群default命名空间下，名称为wordpress，要求将Wordpress应用和MySQL数据库部署到同一Pod中，使用环境变量设置数据库用户及密码均为wordpress，并新建数据库wordpress。</p>
<p>完成后提交master节点的用户名、密码和IP地址到答题框。(需要用到的软件包：WordPress.tar.gz)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: wordpress</span><br><span class="line">spec:</span><br><span class="line"> selector:</span><br><span class="line">  matchLabels:</span><br><span class="line">   app: wordpress</span><br><span class="line"> replicas: 1</span><br><span class="line"> template:</span><br><span class="line">  metadata:</span><br><span class="line">   labels:</span><br><span class="line">    app: wordpress</span><br><span class="line">  spec:</span><br><span class="line">   containers:</span><br><span class="line">    \- name: wordpress</span><br><span class="line">     image: wordpress:5.8.2-apache</span><br><span class="line">     env:</span><br><span class="line">      \- name: WORDPRESS_DB_HOST</span><br><span class="line">       value: mysql:3306</span><br><span class="line">      \- name: WORDPRESS_DB_USER</span><br><span class="line">       value: root</span><br><span class="line">      \- name: WORDPRESS_DB_PASSWORD</span><br><span class="line">       value: wordpress</span><br><span class="line">      \- name: WORDPRESS_DB_NAME</span><br><span class="line">       value: wordpress</span><br><span class="line"></span><br><span class="line">​    \- name: mysql</span><br><span class="line">​     image: mysql:5.7</span><br><span class="line">​     env:</span><br><span class="line">​      \- name: MYSQL_ROOT_PASSWORD</span><br><span class="line">​       value: wordpress</span><br><span class="line">​      \- name: MYSQL_DATABASE</span><br><span class="line">​       value: wordpress</span><br><span class="line">​      \- name: MYSQL_USER</span><br><span class="line">​       value: wordpress</span><br><span class="line">​      \- name: MYSQL_PASSWORD</span><br><span class="line">​       value: wordpress </span><br><span class="line">​     volumeMounts:</span><br><span class="line">​      \- name: mysql</span><br><span class="line">​       mountPath: /var/lib/mysql </span><br><span class="line"></span><br><span class="line">   volumes:</span><br><span class="line">    \- name: mysql</span><br><span class="line">     hostPath: </span><br><span class="line">      path: /root/db_data</span><br></pre></td></tr></table></figure>



<h3 id="2、【实操题】WordPress上云–使用Service暴露服务（2分）"><a href="#2、【实操题】WordPress上云–使用Service暴露服务（2分）" class="headerlink" title="2、【实操题】WordPress上云–使用Service暴露服务（2分）"></a>2、【实操题】WordPress上云–使用Service暴露服务（2分）</h3><p>在Kubernetes集群default命名空间下为WordPress服务创建一个NodePort类型的Service，名称为wordpress，将80端口对外暴露为30083端口；为MySQL服务创建一个ClusterIP类型的Service，名称为wordpress-mysql。</p>
<p>完成后提交master节点的用户名、密码和IP地址到答题框。(需要用到的软件包：WordPress.tar.gz)</p>
<h3 id="3、【实操题】WordPress上云–使用StorageClass实现持久化存储（2分）"><a href="#3、【实操题】WordPress上云–使用StorageClass实现持久化存储（2分）" class="headerlink" title="3、【实操题】WordPress上云–使用StorageClass实现持久化存储（2分）"></a>3、【实操题】WordPress上云–使用StorageClass实现持久化存储（2分）</h3><p>在master节点上部署NFS服务器，创建共享目录&#x2F;data&#x2F;kubernetes，基于该NFS服务器在Kubernetes集群中创建名为nfs-storage的默认动态存储类。基于nfs-storage动态存储类，在Kubernetes集群default命名空间下创建两个PVC，PVC1名称mysql-pvc，PVC2名称wordpress-pvc，大小均为20G，均使用ReadWriteMany模式。</p>
<p>完成后提交master节点的用户名、密码和IP地址到答题框。(需要用到的软件包：WordPress.tar.gz)</p>
<h3 id="4、【实操题】HPA管理–快速扩容（3分）"><a href="#4、【实操题】HPA管理–快速扩容（3分）" class="headerlink" title="4、【实操题】HPA管理–快速扩容（3分）"></a>4、【实操题】HPA管理–快速扩容（3分）</h3><p>默认情况下HPA是无法调整伸缩灵敏度的，但不同的业务场景对伸缩灵敏度的要求不一样。要求在default命名空间下使用nginx镜像创建一个名为web的deployment，自定义HPA的伸缩灵敏度，为该deployment创建一个名为web的HPA，扩容时立即新增当前9倍数量的副本数，时间窗口为5s，伸缩范围为1–1000。假如一开始只有1个Pod，当CPU使用率超过80%时，Pod数量变化趋势为：1 → 10 → 100 → 1000。</p>
<p>完成后提交master节点的IP地址、用户名和密码到答题框。</p>
<h3 id="5、【实操题】流量管理–创建基于用户身份的路由（3分）"><a href="#5、【实操题】流量管理–创建基于用户身份的路由（3分）" class="headerlink" title="5、【实操题】流量管理–创建基于用户身份的路由（3分）"></a>5、【实操题】流量管理–创建基于用户身份的路由（3分）</h3><p>为Bookinfo应用创建一个名为reviews的VirtualService，要求来自名为Jason的用户的所有流量将被路由到reviews服务的v2版本。</p>
<p>完成后提交master节点的IP地址、用户名和密码到答题框。</p>
<h2 id="五、容器云平台运维开发：Kubernetes-APIs运维开发（当前任务共2道题目）"><a href="#五、容器云平台运维开发：Kubernetes-APIs运维开发（当前任务共2道题目）" class="headerlink" title="五、容器云平台运维开发：Kubernetes APIs运维开发（当前任务共2道题目）"></a>五、容器云平台运维开发：Kubernetes APIs运维开发（当前任务共2道题目）</h2><h4 id="1、【实操题】Kubernetes-Python运维开发：使用SDK方式管理deployment服务（3分）"><a href="#1、【实操题】Kubernetes-Python运维开发：使用SDK方式管理deployment服务（3分）" class="headerlink" title="1、【实操题】Kubernetes Python运维开发：使用SDK方式管理deployment服务（3分）"></a>1、【实操题】Kubernetes Python运维开发：使用SDK方式管理deployment服务（3分）</h4><p>在提供的OpenStack私有云平台上，使用k8s-python-dev镜像创建1台云主机，云主机类型使用4vCPU&#x2F;12G内存&#x2F;100G硬盘。该主机中已经默认安装了所需的开发环境，登录默认账号密码为“root&#x2F;1DaoYun@2022”。</p>
<p>使用Kubernetes python SDK的“kubernetes”Python库，在&#x2F;root目录下，创建sdk_manager_deployment.py文件，要求编写python代码，代码实现以下任务：</p>
<p>（1）首先使用nginx-deployment.yaml文件创建deployment资源。</p>
<p>（2）创建完成后，查询该服务的信息，查询的body部分通过控制台输出，并以json格式的文件输出到当前目录下的deployment_sdk_dev.json文件中。</p>
<p>编写完成后，提交该云主机的用户名、密码和IP地址到答题框。</p>
]]></content>
  </entry>
  <entry>
    <title>2022云计算国赛私有云</title>
    <url>/2023/03/07/2022%E4%BA%91%E8%AE%A1%E7%AE%97%E5%9B%BD%E8%B5%9B%E7%A7%81%E6%9C%89%E4%BA%91/</url>
    <content><![CDATA[<h1 id="2022-年全国职业院校技能大赛高职组云计算赛项试卷"><a href="#2022-年全国职业院校技能大赛高职组云计算赛项试卷" class="headerlink" title="2022 年全国职业院校技能大赛高职组云计算赛项试卷"></a>2022 年全国职业院校技能大赛高职组云计算赛项试卷<span id="more"></span></h1><h2 id="云计算赛项第一场-私有云"><a href="#云计算赛项第一场-私有云" class="headerlink" title="云计算赛项第一场-私有云"></a>云计算赛项第一场-私有云</h2><h3 id="【任务-1】私有云服务搭建-10-分"><a href="#【任务-1】私有云服务搭建-10-分" class="headerlink" title="【任务 1】私有云服务搭建[10 分]"></a>【任务 1】私有云服务搭建[10 分]</h3><h4 id="【题目-1】基础环境配置-0-5-分"><a href="#【题目-1】基础环境配置-0-5-分" class="headerlink" title="【题目 1】基础环境配置[0.5 分]"></a>【题目 1】基础环境配置[0.5 分]</h4><p>使用提供的用户名密码，登录提供的 OpenStack 私有云平台，在当前租户下，使用CentOS7.9 镜像，创建两台云主机，云主机类型使用 4vCPU&#x2F;12G&#x2F;100G_50G 类型。当前租户下默认存在一张网卡，自行创建第二张网卡并连接至 controller 和 compute 节点（第二张网卡的网段为 10.10.X.0&#x2F;24，X 为工位号，不需要创建路由）。自行检查安全组策略，以确保网络正常通信与 ssh 连接，然后按以下要求配置服务器：<br>（1）设置控制节点主机名为 controller，设置计算节点主机名为 compute；<br>（2）修改 hosts 文件将 IP 地址映射为主机名；<br>完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.查看控制节点名字为 controller 正确计 0.2 分<br>2.查看 hosts 文件中有正确的主机名和 IP 映射计 0.2 分<br>3.控制节点正确使用两块网卡计 0.1 分</p>
<p><strong>controller</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# hostnamectl set-hostname controller</span><br><span class="line">[root@controller ~]# su</span><br><span class="line">[root@controller ~]# hostnamectl </span><br><span class="line">   Static hostname: controller</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: cc2c86fe566741e6a2ff6d399c5d5daa</span><br><span class="line">           Boot ID: 214933a71db6473cb11d2c126d890cdf</span><br><span class="line">    Virtualization: kvm</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-1160.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br></pre></td></tr></table></figure>

<p><strong>compute</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# hostnamectl set-hostname compute</span><br><span class="line">[root@compute ~]# su</span><br><span class="line">[root@compute ~]# hostnamectl </span><br><span class="line">   Static hostname: compute</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: cc2c86fe566741e6a2ff6d399c5d5daa</span><br><span class="line">           Boot ID: 3f03732ccb29461b9a4f3772d76ea5c3</span><br><span class="line">    Virtualization: kvm</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-1160.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.20.161	controller</span><br><span class="line">192.168.20.195	compute</span><br></pre></td></tr></table></figure>

<h4 id="【题目-2】Yum-源配置-0-5-分"><a href="#【题目-2】Yum-源配置-0-5-分" class="headerlink" title="【题目 2】Yum 源配置[0.5 分]"></a>【题目 2】Yum 源配置[0.5 分]</h4><p>使用提供的 http 服务地址，在 http 服务下，存在 centos7.9 和 iaas 的网络 yum 源，使用该 http 源作为安装 iaas 平台的网络源。分别设置 controller 节点和 compute 节点的 yum 源文件 http.repo。完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.查看&#x2F;etc&#x2F;yum.repos.d&#x2F;http.repo 文件，有正确的 baseurl 路径，计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# rm -rf /etc/yum.repos.d/*</span><br><span class="line">[root@controller ~]# cat /etc/yum.repos.d/http.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=http://172.19.25.11/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[iaas]</span><br><span class="line">name=iaas</span><br><span class="line">baseurl=http://172.19.25.11/iaas/iaas-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[root@controller ~]# yum repolist</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Determining fastest mirrors</span><br><span class="line">centos                                                                                                                           | 3.6 kB  00:00:00     </span><br><span class="line">iaas                                                                                                                             | 2.9 kB  00:00:00     </span><br><span class="line">(1/3): centos/group_gz                                                                                                           | 153 kB  00:00:00     </span><br><span class="line">(2/3): iaas/primary_db                                                                                                           | 597 kB  00:00:00     </span><br><span class="line">(3/3): centos/primary_db                                                                                                         | 3.3 MB  00:00:00     </span><br><span class="line">repo id                                                                  repo name                                                                status</span><br><span class="line">centos                                                                   centos                                                                   4,070</span><br><span class="line">iaas                                                                     iaas                                                                       954</span><br><span class="line">repolist: 5,024</span><br></pre></td></tr></table></figure>

<h4 id="【题目-3】配置无秘钥-ssh-0-5-分"><a href="#【题目-3】配置无秘钥-ssh-0-5-分" class="headerlink" title="【题目 3】配置无秘钥 ssh[0.5 分]"></a>【题目 3】配置无秘钥 ssh[0.5 分]</h4><p>配置 controller 节点可以无秘钥访问 compute 节点，配置完成后，尝试 ssh 连接 compute节点的 hostname 进行测试。完成后提交 controller 节点的用户名、密码和 IP 地址到答题框。</p>
<p>1.查看控制节点允许计算节点无秘钥登录计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# ssh-keygen </span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:eMrYvgTXlKEoQ4SVWEeZlTiVZMtbt2H5Z/LAjwq9K18 root@controller</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">| *=ooB=o.        |</span><br><span class="line">|o...=+oo o .     |</span><br><span class="line">|  o ..+ + =      |</span><br><span class="line">|   o   * o =     |</span><br><span class="line">|    . + S . = o  |</span><br><span class="line">|     * o .   O   |</span><br><span class="line">|    . = . . E o  |</span><br><span class="line">|     o  .. +     |</span><br><span class="line">|      o. o=.     |</span><br><span class="line">+----[SHA256]-----+</span><br><span class="line">[root@controller ~]# ssh-copy-id root@compute </span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@compute&#x27;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@compute&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"></span><br><span class="line">[root@controller ~]# ssh root@compute </span><br><span class="line">Last login: Tue Mar  7 02:18:21 2023</span><br><span class="line">[root@compute ~]#                     #无密钥连接成功</span><br></pre></td></tr></table></figure>

<h4 id="【题目-4】基础安装-0-5-分"><a href="#【题目-4】基础安装-0-5-分" class="headerlink" title="【题目 4】基础安装[0.5 分]"></a>【题目 4】基础安装[0.5 分]</h4><p>在控制节点和计算节点上分别安装 openstack-iaas 软件包，根据表 2 配置两个节点脚本文件中的基本变量（配置脚本文件为&#x2F;etc&#x2F;openstack&#x2F;openrc.sh）。<br>完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查环境变量文件配置正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# yum install -y openstack-iaas</span><br><span class="line">[root@controller ~]# cat /etc/openstack/openrc.sh </span><br><span class="line">#--------------------system Config--------------------##</span><br><span class="line">#Controller Server Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP=192.168.20.161</span><br><span class="line"></span><br><span class="line">#Controller HOST Password. example:000000 </span><br><span class="line">HOST_PASS=000000</span><br><span class="line"></span><br><span class="line">#Controller Server hostname. example:controller</span><br><span class="line">HOST_NAME=controller</span><br><span class="line"></span><br><span class="line">#Compute Node Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP_NODE=192.168.20.195</span><br><span class="line"></span><br><span class="line">#Compute HOST Password. example:000000 </span><br><span class="line">HOST_PASS_NODE=000000</span><br><span class="line"></span><br><span class="line">#Compute Node hostname. example:compute</span><br><span class="line">HOST_NAME_NODE=compute</span><br><span class="line"></span><br><span class="line">#--------------------Chrony Config-------------------##</span><br><span class="line">#Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)</span><br><span class="line">network_segment_IP=192.168.20.0/24</span><br><span class="line"></span><br><span class="line">#--------------------Rabbit Config ------------------##</span><br><span class="line">#user for rabbit. example:openstack</span><br><span class="line">RABBIT_USER=openstack</span><br><span class="line"></span><br><span class="line">#Password for rabbit user .example:000000</span><br><span class="line">RABBIT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------MySQL Config---------------------##</span><br><span class="line">#Password for MySQL root user . exmaple:000000</span><br><span class="line">DB_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Keystone Config------------------##</span><br><span class="line">#Password for Keystore admin user. exmaple:000000</span><br><span class="line">DOMAIN_NAME=demo</span><br><span class="line">ADMIN_PASS=000000</span><br><span class="line">DEMO_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql keystore user. exmaple:000000</span><br><span class="line">KEYSTONE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Glance Config--------------------##</span><br><span class="line">#Password for Mysql glance user. exmaple:000000</span><br><span class="line">GLANCE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore glance user. exmaple:000000</span><br><span class="line">GLANCE_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Placement Config----------------------##</span><br><span class="line">#Password for Mysql placement user. exmaple:000000</span><br><span class="line">PLACEMENT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore placement user. exmaple:000000</span><br><span class="line">PLACEMENT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Nova Config----------------------##</span><br><span class="line">#Password for Mysql nova user. exmaple:000000</span><br><span class="line">NOVA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore nova user. exmaple:000000</span><br><span class="line">NOVA_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Neutron Config-------------------##</span><br><span class="line">#Password for Mysql neutron user. exmaple:000000</span><br><span class="line">NEUTRON_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore neutron user. exmaple:000000</span><br><span class="line">NEUTRON_PASS=000000</span><br><span class="line"></span><br><span class="line">#metadata secret for neutron. exmaple:000000</span><br><span class="line">METADATA_SECRET=000000</span><br><span class="line"></span><br><span class="line">#External Network Interface. example:eth1</span><br><span class="line">INTERFACE_NAME=eth0</span><br><span class="line"></span><br><span class="line">#External Network The Physical Adapter. example:provider</span><br><span class="line">Physical_NAME=provider</span><br><span class="line"></span><br><span class="line">#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101</span><br><span class="line">minvlan=101</span><br><span class="line"></span><br><span class="line">#Last Vlan ID in VLAN RANGE for VLAN Network. example:200</span><br><span class="line">maxvlan=200</span><br><span class="line"></span><br><span class="line">#--------------------Cinder Config--------------------##</span><br><span class="line">#Password for Mysql cinder user. exmaple:000000</span><br><span class="line">CINDER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore cinder user. exmaple:000000</span><br><span class="line">CINDER_PASS=000000</span><br><span class="line"></span><br><span class="line">#Cinder Block Disk. example:md126p3</span><br><span class="line">BLOCK_DISK=vdb1</span><br><span class="line"></span><br><span class="line">#--------------------Swift Config---------------------##</span><br><span class="line">#Password for Keystore swift user. exmaple:000000</span><br><span class="line">SWIFT_PASS=000000</span><br><span class="line"></span><br><span class="line">#The NODE Object Disk for Swift. example:md126p4.</span><br><span class="line">OBJECT_DISK=vdb2</span><br><span class="line"></span><br><span class="line">#The NODE IP for Swift Storage Network. example:x.x.x.x.</span><br><span class="line">STORAGE_LOCAL_NET_IP=192.168.20.195</span><br><span class="line"></span><br><span class="line">#--------------------Trove Config----------------------##</span><br><span class="line">#Password for Mysql trove user. exmaple:000000</span><br><span class="line">TROVE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore trove user. exmaple:000000</span><br><span class="line">TROVE_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Heat Config----------------------##</span><br><span class="line">#Password for Mysql heat user. exmaple:000000</span><br><span class="line">HEAT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore heat user. exmaple:000000</span><br><span class="line">HEAT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Ceilometer Config----------------##</span><br><span class="line">#Password for Gnocchi ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------AODH Config----------------##</span><br><span class="line">#Password for Mysql AODH user. exmaple:000000</span><br><span class="line">AODH_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore AODH user. exmaple:000000</span><br><span class="line">AODH_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------ZUN Config----------------##</span><br><span class="line">#Password for Mysql ZUN user. exmaple:000000</span><br><span class="line">ZUN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore ZUN user. exmaple:000000</span><br><span class="line">ZUN_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore KURYR user. exmaple:000000</span><br><span class="line">KURYR_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------OCTAVIA Config----------------##</span><br><span class="line">#Password for Mysql OCTAVIA user. exmaple:000000</span><br><span class="line">OCTAVIA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore OCTAVIA user. exmaple:000000</span><br><span class="line">OCTAVIA_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Manila Config----------------##</span><br><span class="line">#Password for Mysql Manila user. exmaple:000000</span><br><span class="line">MANILA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Manila user. exmaple:000000</span><br><span class="line">MANILA_PASS=000000</span><br><span class="line"></span><br><span class="line">#The NODE Object Disk for Manila. example:md126p5.</span><br><span class="line">SHARE_DISK=vdb3</span><br><span class="line"></span><br><span class="line">#--------------------Cloudkitty Config----------------##</span><br><span class="line">#Password for Mysql Cloudkitty user. exmaple:000000</span><br><span class="line">CLOUDKITTY_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Cloudkitty user. exmaple:000000</span><br><span class="line">CLOUDKITTY_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Barbican Config----------------##</span><br><span class="line">#Password for Mysql Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_PASS=000000</span><br><span class="line">###############################################################</span><br><span class="line">#####在vi编辑器中执行:%s/^.\&#123;1\&#125;//  删除每行前1个字符(#号)#####</span><br><span class="line">###############################################################</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-pre-host.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-pre-host.sh</span><br></pre></td></tr></table></figure>

<h4 id="【题目-5】数据库安装与调优-0-5-分"><a href="#【题目-5】数据库安装与调优-0-5-分" class="headerlink" title="【题目 5】数据库安装与调优[0.5 分]"></a>【题目 5】数据库安装与调优[0.5 分]</h4><p>在 controller 节点上使用 iaas-install-mysql.sh 脚本安装 Mariadb、Memcached、RabbitMQ等服务。安装服务完毕后，修改&#x2F;etc&#x2F;my.cnf 文件，完成下列要求：<br>1.设置数据库支持大小写；<br>2.设置数据库缓存 innodb 表的索引，数据，插入数据时的缓冲为 4G；<br>3.设置数据库的 log buffer 为 64MB；<br>4.设置数据库的 redo log 大小为 256MB；</p>
<p>5.设置数据库的 redo log 文件组为 2。<br>完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查数据库配置正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-mysql.sh</span><br><span class="line">[root@controller ~]# cat /etc/my.cnf</span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># This group is read both both by the client and the server</span><br><span class="line"></span><br><span class="line"># use it for options that affect everything</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">[client-server]</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># This group is read by the server</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line"># Disabling symbolic-links is recommended to prevent assorted security risks</span><br><span class="line"></span><br><span class="line">symbolic-links=0</span><br><span class="line">default-storage-engine = innodb</span><br><span class="line">innodb_file_per_table</span><br><span class="line">collation-server = utf8_general_ci</span><br><span class="line">init-connect = &#x27;SET NAMES utf8&#x27;</span><br><span class="line">character-set-server = utf8</span><br><span class="line">max_connections=10000</span><br><span class="line">lower_case_table_names = 1 #数据库支持大小写</span><br><span class="line">innodb_buffer_pool_size = 4G #数据库缓存</span><br><span class="line">innodb_log_buffer_size = 64MB #设置数据库的log buffer为64mb</span><br><span class="line">innodb_log_file_size = 256MB #设置数据库的redo log大小为256mb</span><br><span class="line">innodb_log_files_in_group = 2 #设置数据库的redo log文件组为2</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line"></span><br><span class="line"># include all files from the config directory</span><br><span class="line"></span><br><span class="line">#</span><br><span class="line">!includedir /etc/my.cnf.d</span><br><span class="line"></span><br><span class="line">[root@controller ~]# systemctl restart mariadb</span><br></pre></td></tr></table></figure>

<h4 id="【题目-6】Keystone-服务安装与使用-0-5-分"><a href="#【题目-6】Keystone-服务安装与使用-0-5-分" class="headerlink" title="【题目 6】Keystone 服务安装与使用[0.5 分]"></a>【题目 6】Keystone 服务安装与使用[0.5 分]</h4><p>在 controller 节点上使用 iaas-install-keystone.sh 脚本安装 Keystone 服务。安装完成后，使用相关命令，创建用户 chinaskill，密码为 000000。完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查 keystone 服务安装正确计 0.2 分<br>2.检查 chinaskill 用户创建正确计 0.3 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-keystone.sh </span><br><span class="line">[root@controller ~]# source /etc/keystone/admin-openrc.sh </span><br><span class="line">[root@controller ~]# openstack user create --domain demo chinaskill --password 000000</span><br><span class="line">+---------------------+----------------------------------+</span><br><span class="line">| Field               | Value                            |</span><br><span class="line">+---------------------+----------------------------------+</span><br><span class="line">| domain_id           | 916f69a6dc5d462c8743799c33d6cd8b |</span><br><span class="line">| enabled             | True                             |</span><br><span class="line">| id                  | ce30ed9e337b4441acbb5947ce95b9ee |</span><br><span class="line">| name                | chinaskill                       |</span><br><span class="line">| options             | &#123;&#125;                               |</span><br><span class="line">| password_expires_at | None                             |</span><br><span class="line">+---------------------+----------------------------------+</span><br><span class="line">[root@controller ~]# openstack user list</span><br><span class="line">+----------------------------------+------------+</span><br><span class="line">| ID                               | Name       |</span><br><span class="line">+----------------------------------+------------+</span><br><span class="line">| e5bcdce7fc39415693bd84a44f6d6373 | admin      |</span><br><span class="line">| c6c1bf4fad804eca9eed9515bcf16d78 | demo       |</span><br><span class="line">| ce30ed9e337b4441acbb5947ce95b9ee | chinaskill |</span><br><span class="line">+----------------------------------+------------+</span><br></pre></td></tr></table></figure>

<h4 id="【题目-7】Glance-安装与使用-0-5-分"><a href="#【题目-7】Glance-安装与使用-0-5-分" class="headerlink" title="【题目 7】Glance 安装与使用[0.5 分]"></a>【题目 7】Glance 安装与使用[0.5 分]</h4><p>在 controller 节点上使用 iaas-install-glance.sh 脚本安装 glance 服务。使用命令将提供的 cirros-0.3.4-x86_64-disk.img 镜像（该镜像在 HTTP 服务中，可自行下载）上传至平台，命名为 cirros，并设置最小启动需要的硬盘为 10G，最小启动需要的内存为 1G。完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查 glance 服务安装正确计 0.1 分<br>2.检查 cirros 镜像最小启动硬盘与内存配置正确计 0.4 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-glance.sh</span><br><span class="line">[root@controller ~]# openstack image create cirros --disk qcow2 --container bare --min-disk 10 --min-ram 1024 &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Field            | Value                                                                                                                                                                                      |</span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                                                                                                                                                           |</span><br><span class="line">| container_format | bare                                                                                                                                                                                       |</span><br><span class="line">| created_at       | 2023-03-07T03:08:17Z                                                                                                                                                                       |</span><br><span class="line">| disk_format      | qcow2                                                                                                                                                                                      |</span><br><span class="line">| file             | /v2/images/da9fee49-f9a7-45e6-a830-f749e87b404b/file                                                                                                                                       |</span><br><span class="line">| id               | da9fee49-f9a7-45e6-a830-f749e87b404b                                                                                                                                                       |</span><br><span class="line">| min_disk         | 10                                                                                                                                                                                         |</span><br><span class="line">| min_ram          | 1024                                                                                                                                                                                       |</span><br><span class="line">| name             | cirros                                                                                                                                                                                     |</span><br><span class="line">| owner            | 5a824a04c09c4ba4b75767019900dc98                                                                                                                                                           |</span><br><span class="line">| properties       | os_hash_algo=&#x27;sha512&#x27;, os_hash_value=&#x27;1b03ca1bc3fafe448b90583c12f367949f8b0e665685979d95b004e48574b953316799e23240f4f739d1b5eb4c4ca24d38fdc6f4f9d8247a2bc64db25d6bbdb2&#x27;, os_hidden=&#x27;False&#x27; |</span><br><span class="line">| protected        | False                                                                                                                                                                                      |</span><br><span class="line">| schema           | /v2/schemas/image                                                                                                                                                                          |</span><br><span class="line">| size             | 13287936                                                                                                                                                                                   |</span><br><span class="line">| status           | active                                                                                                                                                                                     |</span><br><span class="line">| tags             |                                                                                                                                                                                            |</span><br><span class="line">| updated_at       | 2023-03-07T03:08:18Z                                                                                                                                                                       |</span><br><span class="line">| virtual_size     | None                                                                                                                                                                                       |</span><br><span class="line">| visibility       | shared                                                                                                                                                                                     |</span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<h4 id="【题目-8】Nova-安装与优化-0-5-分"><a href="#【题目-8】Nova-安装与优化-0-5-分" class="headerlink" title="【题目 8】Nova 安装与优化[0.5 分]"></a>【题目 8】Nova 安装与优化[0.5 分]</h4><p>在 controller 节 点 和 compute 节 点 上 分 别 使 用 iaas-install-placement.sh 脚 本 、iaas-install-nova -controller.sh 脚本、iaas-install-nova-compute.sh 脚本安装 Nova 服务。安装完成后，请修改 nova 相关配置文件，解决因等待时间过长而导致虚拟机启动超时从而获取不到 IP 地址而报错失败的问题。配置完成后提交 controller 点的用户名、密码和 IP 地址到答题框。<br>1.检查 nova 服务解决超时问题配置正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-placement.sh </span><br><span class="line">[root@controller ~]# iaas-install-nova-controller.sh </span><br><span class="line">[root@controller ~]# cat /etc/nova/nova.conf  | grep vif_plugging_is_fatal=false</span><br><span class="line">vif_plugging_is_fatal=false</span><br><span class="line">[root@controller ~]# systemctl restart *nova*</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-nova-compute.sh </span><br></pre></td></tr></table></figure>

<h4 id="【题目-9】Neutron-安装-0-5-分"><a href="#【题目-9】Neutron-安装-0-5-分" class="headerlink" title="【题目 9】Neutron 安装[0.5 分]"></a>【题目 9】Neutron 安装[0.5 分]</h4><p>使用提供的脚本 iaas-install-neutron-controller.sh 和 iaas-install-neutron-compute.sh，在controller 和 compute 节点上安装 neutron 服务。完成后提交控制节点的用户名、密码和 IP地址到答题框。<br>1.检查 neutron 服务安装正确计 0.2 分<br>2.检查 neutron 服务的 linuxbridge 网桥服务启动正确计 0.3 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-neutron-controller.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-neutron-compute.sh </span><br></pre></td></tr></table></figure>

<h4 id="【题目-10】Doshboard-安装-0-5-分"><a href="#【题目-10】Doshboard-安装-0-5-分" class="headerlink" title="【题目 10】Doshboard 安装[0.5 分]"></a>【题目 10】Doshboard 安装[0.5 分]</h4><p>在 controller 节点上使用 iaas-install-dashboad.sh 脚本安装 dashboad 服务。安装完成后，将 Dashboard 中的 Djingo 数据修改为存储在文件中（此种修改解决了 ALL-in-one 快照在其他云平台 Dashboard 不能访问的问题）。完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查 Dashboard 服务安装正确计 0.2 分<br>2.检查 Dashboard 服务中 Djingo 数据修改为存储在文件中配置正确计 0.3 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-dashboard.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/openstack-dashboard/local_settings | grep SESSION_ENGINE</span><br><span class="line"># SESSION_ENGINE to django.contrib.sessions.backends.signed_cookies</span><br><span class="line">SESSION_ENGINE = &#x27;django.contrib.sessions.backends.file&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="【题目-11】Swift-安装-0-5-分"><a href="#【题目-11】Swift-安装-0-5-分" class="headerlink" title="【题目 11】Swift 安装[0.5 分]"></a>【题目 11】Swift 安装[0.5 分]</h4><p>在 控 制 节 点 和 计 算 节 点 上 分 别 使 用 iaas-install-swift-controller.sh 和iaas-install-swift-compute.sh 脚本安装 Swift 服务。安装完成后，使用命令创建一个名叫examcontainer 的容器，将 cirros-0.3.4-x86_64-disk.img 镜像上传到 examcontainer 容器中，并设置分段存放，每一段大小为 10M。完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查 swift 服务安装正确计 0.3 分<br>2.分段上传 cirros 镜像正确计 0.2 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-swift-controller.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-swift-compute.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack container create examcontaienr</span><br><span class="line">+---------------------------------------+---------------+------------------------------------+</span><br><span class="line">| account                               | container     | x-trans-id                         |</span><br><span class="line">+---------------------------------------+---------------+------------------------------------+</span><br><span class="line">| AUTH_5a824a04c09c4ba4b75767019900dc98 | examcontaienr | tx453b000251d94847a87e7-006406d19d |</span><br><span class="line">+---------------------------------------+---------------+------------------------------------+</span><br><span class="line">[root@controller ~]# swift upload examcontainer -S 10000000 cirros-0.3.4-x86_64-disk.img </span><br><span class="line">cirros-0.3.4-x86_64-disk.img segment 1</span><br><span class="line">cirros-0.3.4-x86_64-disk.img segment 0</span><br><span class="line">cirros-0.3.4-x86_64-disk.img</span><br></pre></td></tr></table></figure>

<h4 id="【题目-12】Cinder-创建硬盘-0-5-分"><a href="#【题目-12】Cinder-创建硬盘-0-5-分" class="headerlink" title="【题目 12】Cinder 创建硬盘[0.5 分]"></a>【题目 12】Cinder 创建硬盘[0.5 分]</h4><p>在 控 制 节 点 和 计 算 节 点 分 别 使 用 iaas-install-cinder-controller.sh 、iaas-install-cinder-compute.sh 脚本安装 Cinder 服务，请在计算节点，对块存储进行扩容操作，即在计算节点再分出一个 5G 的分区，加入到 cinder 块存储的后端存储中去。完成后提交计算节点的用户名、密码和 IP 地址到答题框。</p>
<p>1.检查 cinder 后端存储扩容成功计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-cinder-controller.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-cinder-compute.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# vgextend cinder-volumes /dev/vdb4</span><br><span class="line">[root@compute ~]# vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               cinder-volumes</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        2</span><br><span class="line">  Metadata Sequence No  5</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                1</span><br><span class="line">  Open LV               0</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                2</span><br><span class="line">  Act PV                2</span><br><span class="line">  VG Size               24.99 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              6398</span><br><span class="line">  Alloc PE / Size       4874 / &lt;19.04 GiB</span><br><span class="line">  Free  PE / Size       1524 / 5.95 GiB</span><br><span class="line">  VG UUID               nM28QF-pGdI-Q1Nh-LvoZ-F6aV-GuDh-2rey4k</span><br></pre></td></tr></table></figure>

<h4 id="【题目-13】Manila-服务安装与使用-0-5-分"><a href="#【题目-13】Manila-服务安装与使用-0-5-分" class="headerlink" title="【题目 13】Manila 服务安装与使用[0.5 分]"></a>【题目 13】Manila 服务安装与使用[0.5 分]</h4><p>在 控 制 和 计 算 节 点 上 分 别 使 用 iaas-install-manila-controller.sh 和iaas-install-manila-compute.sh 脚本安装 manila 服务。安装服务后创建 default_share_type 共享类型（不使用驱动程序支持），接着创建一个大小为 2G 的共享存储名为 share01 并开放share01 目录对 OpenStack 管理网段使用权限。最后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查 share01 共享存储正确创建并赋予权限计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-manila-controller.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-manila-compute.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# manila type-create default_share_type false    #创建 default_share_type 共享类型（不使用驱动程序支持）</span><br><span class="line">+----------------------+--------------------------------------+</span><br><span class="line">| Property             | Value                                |</span><br><span class="line">+----------------------+--------------------------------------+</span><br><span class="line">| required_extra_specs | driver_handles_share_servers : False |</span><br><span class="line">| Name                 | default_share_type                   |</span><br><span class="line">| Visibility           | public                               |</span><br><span class="line">| is_default           | YES                                  |</span><br><span class="line">| ID                   | 6cfdc0f1-bfca-4f77-8012-57f07b3a53e1 |</span><br><span class="line">| optional_extra_specs |                                      |</span><br><span class="line">| Description          | None                                 |</span><br><span class="line">+----------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# manila create NFS 2 --name share01    #创建一个大小为 2G 的共享存储名为 share01</span><br><span class="line">+---------------------------------------+--------------------------------------+</span><br><span class="line">| Property                              | Value                                |</span><br><span class="line">+---------------------------------------+--------------------------------------+</span><br><span class="line">| status                                | creating                             |</span><br><span class="line">| share_type_name                       | default_share_type                   |</span><br><span class="line">| description                           | None                                 |</span><br><span class="line">| availability_zone                     | None                                 |</span><br><span class="line">| share_network_id                      | None                                 |</span><br><span class="line">| share_server_id                       | None                                 |</span><br><span class="line">| share_group_id                        | None                                 |</span><br><span class="line">| host                                  |                                      |</span><br><span class="line">| revert_to_snapshot_support            | False                                |</span><br><span class="line">| access_rules_status                   | active                               |</span><br><span class="line">| snapshot_id                           | None                                 |</span><br><span class="line">| create_share_from_snapshot_support    | False                                |</span><br><span class="line">| is_public                             | False                                |</span><br><span class="line">| task_state                            | None                                 |</span><br><span class="line">| snapshot_support                      | False                                |</span><br><span class="line">| id                                    | 39a379b3-2874-486d-9a04-b4e5daadb9ed |</span><br><span class="line">| size                                  | 2                                    |</span><br><span class="line">| source_share_group_snapshot_member_id | None                                 |</span><br><span class="line">| user_id                               | e5bcdce7fc39415693bd84a44f6d6373     |</span><br><span class="line">| name                                  | share01                              |</span><br><span class="line">| share_type                            | 6cfdc0f1-bfca-4f77-8012-57f07b3a53e1 |</span><br><span class="line">| has_replicas                          | False                                |</span><br><span class="line">| replication_type                      | None                                 |</span><br><span class="line">| created_at                            | 2023-03-07T06:21:59.000000           |</span><br><span class="line">| share_proto                           | NFS                                  |</span><br><span class="line">| mount_snapshot_support                | False                                |</span><br><span class="line">| project_id                            | 5a824a04c09c4ba4b75767019900dc98     |</span><br><span class="line">| metadata                              | &#123;&#125;                                   |</span><br><span class="line">+---------------------------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# manila access-allow share01 ip 192.168.20.0/24 --access-level rw    #开放share01 目录对 OpenStack 管理网段使用权限</span><br><span class="line">+--------------+--------------------------------------+</span><br><span class="line">| Property     | Value                                |</span><br><span class="line">+--------------+--------------------------------------+</span><br><span class="line">| access_key   | None                                 |</span><br><span class="line">| share_id     | 39a379b3-2874-486d-9a04-b4e5daadb9ed |</span><br><span class="line">| created_at   | 2023-03-07T06:22:54.000000           |</span><br><span class="line">| updated_at   | None                                 |</span><br><span class="line">| access_type  | ip                                   |</span><br><span class="line">| access_to    | 192.168.20.0/24                      |</span><br><span class="line">| access_level | rw                                   |</span><br><span class="line">| state        | queued_to_apply                      |</span><br><span class="line">| id           | 0a57e84a-7b61-4adf-b422-d76f0af111be |</span><br><span class="line">| metadata     | &#123;&#125;                                   |</span><br><span class="line">+--------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# manila type-list</span><br><span class="line">+--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+</span><br><span class="line">| ID                                   | Name               | visibility | is_default | required_extra_specs                 | optional_extra_specs | Description |</span><br><span class="line">+--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+</span><br><span class="line">| 6cfdc0f1-bfca-4f77-8012-57f07b3a53e1 | default_share_type | public     | YES        | driver_handles_share_servers : False |                      | None        |</span><br><span class="line">+--------------------------------------+--------------------+------------+------------+--------------------------------------+----------------------+-------------+</span><br><span class="line">[root@controller ~]# manila list</span><br><span class="line">+--------------------------------------+---------+------+-------------+-----------+-----------+--------------------+-----------------------------+-------------------+</span><br><span class="line">| ID                                   | Name    | Size | Share Proto | Status    | Is Public | Share Type Name    | Host                        | Availability Zone |</span><br><span class="line">+--------------------------------------+---------+------+-------------+-----------+-----------+--------------------+-----------------------------+-------------------+</span><br><span class="line">| 39a379b3-2874-486d-9a04-b4e5daadb9ed | share01 | 2    | NFS         | available | False     | default_share_type | compute@lvm#lvm-single-pool | nova              |</span><br><span class="line">+--------------------------------------+---------+------+-------------+-----------+-----------+--------------------+-----------------------------+-------------------+</span><br></pre></td></tr></table></figure>

<h4 id="【题目-14】Barbican-服务安装与使用-0-5-分"><a href="#【题目-14】Barbican-服务安装与使用-0-5-分" class="headerlink" title="【题目 14】Barbican 服务安装与使用[0.5 分]"></a>【题目 14】Barbican 服务安装与使用[0.5 分]</h4><p>使用 iaas-install-barbican.sh 脚本安装 barbican 服务，安装服务完毕后，使用 openstack命令创建一个名为 secret01 的密钥，创建完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查 secret01 密钥创建正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-barbican.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack secret store --name secret01 --payload secretkey</span><br><span class="line">+---------------+------------------------------------------------------------------------+</span><br><span class="line">| Field         | Value                                                                  |</span><br><span class="line">+---------------+------------------------------------------------------------------------+</span><br><span class="line">| Secret href   | http://controller:9311/v1/secrets/40500d87-2c4d-4a14-8c75-5eeb8c9c9bc1 |</span><br><span class="line">| Name          | secret01                                                               |</span><br><span class="line">| Created       | None                                                                   |</span><br><span class="line">| Status        | None                                                                   |</span><br><span class="line">| Content types | None                                                                   |</span><br><span class="line">| Algorithm     | aes                                                                    |</span><br><span class="line">| Bit length    | 256                                                                    |</span><br><span class="line">| Secret type   | opaque                                                                 |</span><br><span class="line">| Mode          | cbc                                                                    |</span><br><span class="line">| Expiration    | None                                                                   |</span><br><span class="line">+---------------+------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<h4 id="【题目-15】Cloudkitty-服务安装与使用-1-分"><a href="#【题目-15】Cloudkitty-服务安装与使用-1-分" class="headerlink" title="【题目 15】Cloudkitty 服务安装与使用[1 分]"></a>【题目 15】Cloudkitty 服务安装与使用[1 分]</h4><p>使用 iaas-install-cloudkitty.sh 脚本安装 cloudkitty 服务，安装完毕后，启用 hashmap 评级模块，接着创建 volume_thresholds 组，创建服务匹配规则 volume.size，并设置每 GB 的价格为 0.01。接下来对应大量数据设置应用折扣，在组 volume_thresholds 中创建阈值，设置若超过 50GB 的阈值，应用 2%的折扣（0.98）。设置完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查 hashmap 评级模块启用成功计 0.2 分<br>2.检查服务匹配规则 volume.size 创建成功 0.8 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-cloudkitty.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack rating module enable hashmap  #启用hashmap评级模块</span><br><span class="line">+---------+---------+----------+</span><br><span class="line">| Module  | Enabled | Priority |</span><br><span class="line">+---------+---------+----------+</span><br><span class="line">| hashmap | True    |        1 |</span><br><span class="line">+---------+---------+----------+</span><br><span class="line">[root@controller ~]# openstack rating hashmap group create volume_thresholds   #创建volume_thresholds组</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| Name              | Group ID                             |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| volume_thresholds | ea44607d-a9a2-4cfd-ac56-aabd88ba007f |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack rating hashmap service create volume.size     #创建服务匹配规则volume.size</span><br><span class="line">+-------------+--------------------------------------+</span><br><span class="line">| Name        | Service ID                           |</span><br><span class="line">+-------------+--------------------------------------+</span><br><span class="line">| volume.size | 54ef69c0-6000-4713-9518-2e02a67adbbe |</span><br><span class="line">+-------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack rating hashmap mapping create -s 54ef69c0-6000-4713-9518-2e02a67adbbe -g ea44607d-a9a2-4cfd-ac56-aabd88ba007f -t flat 0.01     #设置每GB的价格为0.01</span><br><span class="line">+--------------------------------------+-------+------------+------+----------+--------------------------------------+--------------------------------------+------------+</span><br><span class="line">| Mapping ID                           | Value | Cost       | Type | Field ID | Service ID                           | Group ID                             | Project ID |</span><br><span class="line">+--------------------------------------+-------+------------+------+----------+--------------------------------------+--------------------------------------+------------+</span><br><span class="line">| e2f9a6f9-bc27-4c49-953a-77dfa04df597 | None  | 0.01000000 | flat | None     | 54ef69c0-6000-4713-9518-2e02a67adbbe | ea44607d-a9a2-4cfd-ac56-aabd88ba007f | None       |</span><br><span class="line">+--------------------------------------+-------+------------+------+----------+--------------------------------------+--------------------------------------+------------+</span><br><span class="line">[root@controller ~]# openstack rating hashmap threshold  create -s 54ef69c0-6000-4713-9518-2e02a67adbbe -g ea44607d-a9a2-4cfd-ac56-aabd88ba007f -t rate 50 0.98    #在组 volume_thresholds 中创建阈值，设置若超过 50GB 的阈值，应用 2%的折扣（0.98）</span><br><span class="line">+--------------------------------------+-------------+------------+------+----------+--------------------------------------+--------------------------------------+------------+</span><br><span class="line">| Threshold ID                         | Level       | Cost       | Type | Field ID | Service ID                           | Group ID                             | Project ID |</span><br><span class="line">+--------------------------------------+-------------+------------+------+----------+--------------------------------------+--------------------------------------+------------+</span><br><span class="line">| f0def644-3b5b-45a8-a5d5-2ebd76aa7266 | 50.00000000 | 0.98000000 | rate | None     | 54ef69c0-6000-4713-9518-2e02a67adbbe | ea44607d-a9a2-4cfd-ac56-aabd88ba007f | None       |</span><br><span class="line">+--------------------------------------+-------------+------------+------+----------+--------------------------------------+--------------------------------------+------------+</span><br></pre></td></tr></table></figure>

<h4 id="【题目-16】OpenStack-平台内存优化-0-5-分"><a href="#【题目-16】OpenStack-平台内存优化-0-5-分" class="headerlink" title="【题目 16】OpenStack 平台内存优化[0.5 分]"></a>【题目 16】OpenStack 平台内存优化[0.5 分]</h4><p>搭建完 OpenStack 平台后，关闭系统的内存共享，打开透明大页。完成后提交控制节点的用户名、密码和 IP 地址到答题框。<br>1.检查系统内存优化成功计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">[always] madvise never</span><br><span class="line">[root@controller ~]# echo never &gt;&gt; /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">[root@controller ~]# cat /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">always madvise [never]</span><br></pre></td></tr></table></figure>

<h4 id="【题目-17】修改文件句柄数-0-5-分"><a href="#【题目-17】修改文件句柄数-0-5-分" class="headerlink" title="【题目 17】修改文件句柄数[0.5 分]"></a>【题目 17】修改文件句柄数[0.5 分]</h4><p>Linux 服务器大并发时，往往需要预先调优 Linux 参数。默认情况下，Linux 最大文件句柄数为 1024 个。当你的服务器在大并发达到极限时，就会报出“too many open files”。创建一台云主机，修改相关配置，将控制节点的最大文件句柄数永久修改为 65535。配置完成后提交 controller 点的用户名、密码和 IP 地址到答题框。<br>1.检查配置 linux 系统句柄数为 65535 成功计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/security/limits.conf </span><br><span class="line"></span><br><span class="line">*		 hard    nofile		 65535</span><br><span class="line">*		 soft    nofile		 65535</span><br></pre></td></tr></table></figure>

<h4 id="【题目-18】Linux-系统调优-防止-SYN-攻击-1-分"><a href="#【题目-18】Linux-系统调优-防止-SYN-攻击-1-分" class="headerlink" title="【题目 18】Linux 系统调优-防止 SYN 攻击[1 分]"></a>【题目 18】Linux 系统调优-防止 SYN 攻击[1 分]</h4><p>修改 controller 节点的相关配置文件，开启 SYN cookie，防止 SYN 洪水攻击。完成后提交 controller 节点的用户名、密码和 IP 地址到答题框。<br>1.检查开启 SYN cookie 配置计 1 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/sysctl.conf </span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">[root@controller ~]# sysctl -p</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br></pre></td></tr></table></figure>

<h3 id="【任务-2】私有云服务运维-10-分"><a href="#【任务-2】私有云服务运维-10-分" class="headerlink" title="【任务 2】私有云服务运维[10 分]"></a>【任务 2】私有云服务运维[10 分]</h3><h4 id="【题目-1】OpenStack-开放镜像权限-0-5-分"><a href="#【题目-1】OpenStack-开放镜像权限-0-5-分" class="headerlink" title="【题目 1】OpenStack 开放镜像权限[0.5 分]"></a>【题目 1】OpenStack 开放镜像权限[0.5 分]</h4><p>使 用 OpenStack 私 有 云 平 台 ， 在 OpenStack 平 台 的 admin 项 目 中 使 用cirros-0.3.4-x86_64-disk.img 镜像文件创建名为 glance-cirros 的镜像，通过 OpenStack 命令将glance-cirros 镜像指定 demo 项目进行共享使用。配置完成后提交 controller 点的用户名、密码和 IP 地址到答题框。<br>1.检查 glance-cirros 镜像权限开放正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image create glance-cirros &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Field            | Value                                                                                                                                                                                      |</span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                                                                                                                                                           |</span><br><span class="line">| container_format | bare                                                                                                                                                                                       |</span><br><span class="line">| created_at       | 2023-03-07T07:05:58Z                                                                                                                                                                       |</span><br><span class="line">| disk_format      | raw                                                                                                                                                                                        |</span><br><span class="line">| file             | /v2/images/54b116cf-e941-403c-9fa3-082e767712f6/file                                                                                                                                       |</span><br><span class="line">| id               | 54b116cf-e941-403c-9fa3-082e767712f6                                                                                                                                                       |</span><br><span class="line">| min_disk         | 0                                                                                                                                                                                          |</span><br><span class="line">| min_ram          | 0                                                                                                                                                                                          |</span><br><span class="line">| name             | glance-cirros                                                                                                                                                                              |</span><br><span class="line">| owner            | 5a824a04c09c4ba4b75767019900dc98                                                                                                                                                           |</span><br><span class="line">| properties       | os_hash_algo=&#x27;sha512&#x27;, os_hash_value=&#x27;1b03ca1bc3fafe448b90583c12f367949f8b0e665685979d95b004e48574b953316799e23240f4f739d1b5eb4c4ca24d38fdc6f4f9d8247a2bc64db25d6bbdb2&#x27;, os_hidden=&#x27;False&#x27; |</span><br><span class="line">| protected        | False                                                                                                                                                                                      |</span><br><span class="line">| schema           | /v2/schemas/image                                                                                                                                                                          |</span><br><span class="line">| size             | 13287936                                                                                                                                                                                   |</span><br><span class="line">| status           | active                                                                                                                                                                                     |</span><br><span class="line">| tags             |                                                                                                                                                                                            |</span><br><span class="line">| updated_at       | 2023-03-07T07:05:59Z                                                                                                                                                                       |</span><br><span class="line">| virtual_size     | None                                                                                                                                                                                       |</span><br><span class="line">| visibility       | shared                                                                                                                                                                                     |</span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">[root@controller ~]# openstack project list</span><br><span class="line">+----------------------------------+---------+</span><br><span class="line">| ID                               | Name    |</span><br><span class="line">+----------------------------------+---------+</span><br><span class="line">| 5a824a04c09c4ba4b75767019900dc98 | admin   |</span><br><span class="line">| a580b951935c454595e36921e15b4781 | service |</span><br><span class="line">| b8d9637523374f939026d268e033a77b | demo    |</span><br><span class="line">+----------------------------------+---------+</span><br><span class="line">[root@controller ~]# glance member-create 54b116cf-e941-403c-9fa3-082e767712f6 b8d9637523374f939026d268e033a77b</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">| Image ID                             | Member ID                        | Status  |</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">| 54b116cf-e941-403c-9fa3-082e767712f6 | b8d9637523374f939026d268e033a77b | pending |</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">[root@controller ~]# glance member-update 54b116cf-e941-403c-9fa3-082e767712f6 b8d9637523374f939026d268e033a77b accepted</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br><span class="line">| Image ID                             | Member ID                        | Status   |</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br><span class="line">| 54b116cf-e941-403c-9fa3-082e767712f6 | b8d9637523374f939026d268e033a77b | accepted |</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br></pre></td></tr></table></figure>

<h4 id="【题目-2】OpenStack-消息队列调优-0-5-分"><a href="#【题目-2】OpenStack-消息队列调优-0-5-分" class="headerlink" title="【题目 2】OpenStack 消息队列调优[0.5 分]"></a>【题目 2】OpenStack 消息队列调优[0.5 分]</h4><p>OpenStack 各服务内部通信都是通过 RPC 来交互，各 agent 都需要去连接 RabbitMQ；随着各服务 agent 增多，MQ 的连接数会随之增多，最终可能会到达上限，成为瓶颈。使用自行搭建的 OpenStack 私有云平台，分别通过用户级别、系统级别、配置文件来设置 RabbitMQ服务的最大连接数为 10240，配置完成后提交修改节点的用户名、密码和 IP 地址到答题框。<br>1.检查 rabbitmq 服务最大连接数正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/security/limits.conf   #用户级别</span><br><span class="line"></span><br><span class="line">*		 hard    nofile		 10240</span><br><span class="line">*		 soft    nofile		 10240</span><br><span class="line">[root@controller ~]# cat /etc/sysctl.conf </span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">fs.file-max=10240   #系统级别</span><br><span class="line">[root@controller ~]# sysctl -p</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line">fs.file-max = 10240</span><br><span class="line">[root@controller ~]# cat /usr/lib/systemd/system/rabbitmq-server.service </span><br><span class="line"># systemd unit example</span><br><span class="line">[Unit]</span><br><span class="line">Description=RabbitMQ broker</span><br><span class="line">After=network.target epmd@0.0.0.0.socket</span><br><span class="line">Wants=network.target epmd@0.0.0.0.socket</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">User=rabbitmq</span><br><span class="line">Group=rabbitmq</span><br><span class="line">NotifyAccess=all</span><br><span class="line">TimeoutStartSec=3600</span><br><span class="line"># Note:</span><br><span class="line"># You *may* wish to add the following to automatically restart RabbitMQ</span><br><span class="line"># in the event of a failure. systemd service restarts are not a</span><br><span class="line"># replacement for service monitoring. Please see</span><br><span class="line"># http://www.rabbitmq.com/monitoring.html</span><br><span class="line">#</span><br><span class="line"># Restart=on-failure</span><br><span class="line"># RestartSec=10</span><br><span class="line">WorkingDirectory=/var/lib/rabbitmq</span><br><span class="line">ExecStart=/usr/lib/rabbitmq/bin/rabbitmq-server</span><br><span class="line">ExecStop=/usr/lib/rabbitmq/bin/rabbitmqctl stop</span><br><span class="line">ExecStop=/bin/sh -c &quot;while ps -p $MAINPID &gt;/dev/null 2&gt;&amp;1; do sleep 1; done&quot;</span><br><span class="line">LimitNOFILE=10240    #配置文件</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# systemctl daemon-reload</span><br><span class="line">[root@controller ~]# systemctl restart rabbitmq-server</span><br><span class="line">[root@controller ~]# </span><br><span class="line">[root@controller ~]# </span><br><span class="line">[root@controller ~]# rabbitmqctl status</span><br><span class="line">Status of node rabbit@controller</span><br><span class="line">[&#123;pid,5068&#125;,</span><br><span class="line"> &#123;running_applications,</span><br><span class="line">     [&#123;rabbit,&quot;RabbitMQ&quot;,&quot;3.6.16&quot;&#125;,</span><br><span class="line">      &#123;rabbit_common,</span><br><span class="line">          &quot;Modules shared by rabbitmq-server and rabbitmq-erlang-client&quot;,</span><br><span class="line">          &quot;3.6.16&quot;&#125;,</span><br><span class="line">      &#123;compiler,&quot;ERTS  CXC 138 10&quot;,&quot;7.0.4.1&quot;&#125;,</span><br><span class="line">      &#123;xmerl,&quot;XML parser&quot;,&quot;1.3.14&quot;&#125;,</span><br><span class="line">      &#123;ranch,&quot;Socket acceptor pool for TCP protocols.&quot;,&quot;1.3.2&quot;&#125;,</span><br><span class="line">      &#123;mnesia,&quot;MNESIA  CXC 138 12&quot;,&quot;4.14.3&quot;&#125;,</span><br><span class="line">      &#123;syntax_tools,&quot;Syntax tools&quot;,&quot;2.1.1&quot;&#125;,</span><br><span class="line">      &#123;ssl,&quot;Erlang/OTP SSL application&quot;,&quot;8.1.3.1&quot;&#125;,</span><br><span class="line">      &#123;os_mon,&quot;CPO  CXC 138 46&quot;,&quot;2.4.2&quot;&#125;,</span><br><span class="line">      &#123;public_key,&quot;Public key infrastructure&quot;,&quot;1.4&quot;&#125;,</span><br><span class="line">      &#123;crypto,&quot;CRYPTO&quot;,&quot;3.7.4&quot;&#125;,</span><br><span class="line">      &#123;asn1,&quot;The Erlang ASN1 compiler version 4.0.4&quot;,&quot;4.0.4&quot;&#125;,</span><br><span class="line">      &#123;recon,&quot;Diagnostic tools for production use&quot;,&quot;2.3.2&quot;&#125;,</span><br><span class="line">      &#123;sasl,&quot;SASL  CXC 138 11&quot;,&quot;3.0.3&quot;&#125;,</span><br><span class="line">      &#123;stdlib,&quot;ERTS  CXC 138 10&quot;,&quot;3.3&quot;&#125;,</span><br><span class="line">      &#123;kernel,&quot;ERTS  CXC 138 10&quot;,&quot;5.2&quot;&#125;]&#125;,</span><br><span class="line"> &#123;os,&#123;unix,linux&#125;&#125;,</span><br><span class="line"> &#123;erlang_version,</span><br><span class="line">     &quot;Erlang/OTP 19 [erts-8.3.5.3] [source] [64-bit] [smp:4:4] [async-threads:64] [hipe] [kernel-poll:true]\n&quot;&#125;,</span><br><span class="line"> &#123;memory,</span><br><span class="line">     [&#123;connection_readers,2026008&#125;,</span><br><span class="line">      &#123;connection_writers,360960&#125;,</span><br><span class="line">      &#123;connection_channels,515976&#125;,</span><br><span class="line">      &#123;connection_other,3295536&#125;,</span><br><span class="line">      &#123;queue_procs,1851512&#125;,</span><br><span class="line">      &#123;queue_slave_procs,0&#125;,</span><br><span class="line">      &#123;plugins,0&#125;,</span><br><span class="line">      &#123;other_proc,19619360&#125;,</span><br><span class="line">      &#123;metrics,421520&#125;,</span><br><span class="line">      &#123;mgmt_db,0&#125;,</span><br><span class="line">      &#123;mnesia,335752&#125;,</span><br><span class="line">      &#123;other_ets,2143792&#125;,</span><br><span class="line">      &#123;binary,141972984&#125;,</span><br><span class="line">      &#123;msg_index,84952&#125;,</span><br><span class="line">      &#123;code,21467691&#125;,</span><br><span class="line">      &#123;atom,891849&#125;,</span><br><span class="line">      &#123;other_system,10030300&#125;,</span><br><span class="line">      &#123;allocated_unused,35666864&#125;,</span><br><span class="line">      &#123;reserved_unallocated,0&#125;,</span><br><span class="line">      &#123;total,91086848&#125;]&#125;,</span><br><span class="line"> &#123;alarms,[]&#125;,</span><br><span class="line"> &#123;listeners,[&#123;clustering,25672,&quot;::&quot;&#125;,&#123;amqp,5672,&quot;::&quot;&#125;]&#125;,</span><br><span class="line"> &#123;vm_memory_calculation_strategy,rss&#125;,</span><br><span class="line"> &#123;vm_memory_high_watermark,0.4&#125;,</span><br><span class="line"> &#123;vm_memory_limit,4971485593&#125;,</span><br><span class="line"> &#123;disk_free_limit,50000000&#125;,</span><br><span class="line"> &#123;disk_free,103964745728&#125;,</span><br><span class="line"> &#123;file_descriptors,</span><br><span class="line">     [&#123;total_limit,10140&#125;,</span><br><span class="line">      &#123;total_used,74&#125;,</span><br><span class="line">      &#123;sockets_limit,9124&#125;,</span><br><span class="line">      &#123;sockets_used,72&#125;]&#125;,</span><br><span class="line"> &#123;processes,[&#123;limit,1048576&#125;,&#123;used,1181&#125;]&#125;,</span><br><span class="line"> &#123;run_queue,0&#125;,</span><br><span class="line"> &#123;uptime,9&#125;,</span><br><span class="line"> &#123;kernel,&#123;net_ticktime,60&#125;&#125;]</span><br></pre></td></tr></table></figure>

<h4 id="【题目-3】OpenStack-Glance-镜像压缩-0-5-分"><a href="#【题目-3】OpenStack-Glance-镜像压缩-0-5-分" class="headerlink" title="【题目 3】OpenStack Glance 镜像压缩[0.5 分]"></a>【题目 3】OpenStack Glance 镜像压缩[0.5 分]</h4><p>使 用 自 行 搭 建 的 OpenStack 平 台 。 在 HTTP 服 务 中 存 在 一 个 镜 像 为CentOS7.5-compress.qcow2 的镜像，请使用 qemu 相关命令，对该镜像进行压缩，压缩后的镜像命名为 chinaskill-js-compress.qcow2 并存放在&#x2F;root 目录下。完成后提交 controller 点的用户名、密码和 IP 地址到答题框。<br>1.检查镜像压缩正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# qemu-img convert -c -O qcow2 cirros-0.3.4-x86_64-disk.img chinaskill-cirros.img</span><br></pre></td></tr></table></figure>

<h4 id="【题目-4】glance-对接-cinder-后端存储-0-5-分"><a href="#【题目-4】glance-对接-cinder-后端存储-0-5-分" class="headerlink" title="【题目 4】glance 对接 cinder 后端存储[0.5 分]"></a>【题目 4】glance 对接 cinder 后端存储[0.5 分]</h4><p>在自行搭建的 OpenStack 平台中修改相关参数，使 glance 可以使用 cinder 作为后端存储，将镜像存储于 cinder 卷中。使用 cirros-0.3.4-x86_64-disk.img 文件创建 cirros-image 镜像存储于 cirros-cinder 卷中，通过 cirros-image 镜像使用 cinder 卷启动盘的方式进行创建虚拟机。完成后提交修改节点的用户名、密码和 IP 地址到答题框。</p>
<p>1.检查修改 glance 后端存储为 cinder 正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# vi /etc/glance/glance-api.conf</span><br><span class="line">stores = file,http,swift,cinder</span><br><span class="line">show_multiple_locations = True</span><br><span class="line">[root@controller ~]# vi /etc/cinder/cinder.conf</span><br><span class="line">glance_api_version = 2</span><br><span class="line">allowed_direct_url_schemes = cinder</span><br><span class="line">image_upload_use_internal_tenant = True</span><br><span class="line">[root@controller ~]# systemctl restart openstack-glance*</span><br><span class="line">[root@controller ~]# systemctl restart openstack-cinder*</span><br><span class="line">去图形界面通过镜像创建一个云硬盘</span><br><span class="line">[root@controller ~]# cinder list</span><br><span class="line">+--------------------------------------+-----------+---------------+------+-------------+----------+-------------</span><br><span class="line">+</span><br><span class="line">| ID | Status | Name | Size | Volume Type</span><br><span class="line">| Bootable | Attached to |</span><br><span class="line">+--------------------------------------+-----------+---------------+------+-------------+----------+-------------</span><br><span class="line">+</span><br><span class="line">| 13b79150-8571-48e6-b3f9-478912d2b678 | available | cirros-cinder | 15 | - | true</span><br><span class="line">| |</span><br><span class="line">+--------------------------------------+-----------+---------------+------+-------------+----------+-------------</span><br><span class="line">+</span><br><span class="line">[root@controller ~]# glance image-create --name cirros-image --disk-format qcow2 --container</span><br><span class="line">bare</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| checksum | None |</span><br><span class="line">| container_format | bare |</span><br><span class="line">| created_at | 2022-05-27T10:29:35Z |</span><br><span class="line">| disk_format | qcow2 |</span><br><span class="line">| id | 9d163f0d-58f2-49bd-b438-a4e2704e57ad |</span><br><span class="line">| locations | [] |</span><br><span class="line">| min_disk | 0 |</span><br><span class="line">| min_ram | 0 |</span><br><span class="line">| name | cirros-image |</span><br><span class="line">| os_hash_algo | None |</span><br><span class="line">| os_hash_value | None |</span><br><span class="line">| os_hidden | False |</span><br><span class="line">| owner | 50957190677d4ef186e7b0c04b99b44f |</span><br><span class="line">| protected | False |</span><br><span class="line">| size | None |</span><br><span class="line">| status | queued |</span><br><span class="line">| tags | [] |</span><br><span class="line">| updated_at | 2022-05-27T10:29:35Z |</span><br><span class="line">| virtual_size | Not available |</span><br><span class="line">| visibility | shared |</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# glance location-add 9d163f0d-58f2-49bd-b438-a4e2704e57ad --url</span><br><span class="line">cinder://13b79150-8571-48e6-b3f9-478912d2b678</span><br></pre></td></tr></table></figure>



<h4 id="【题目-5】OpenStack-Heat-运维：创建容器-0-5-分"><a href="#【题目-5】OpenStack-Heat-运维：创建容器-0-5-分" class="headerlink" title="【题目 5】OpenStack Heat 运维：创建容器[0.5 分]"></a>【题目 5】OpenStack Heat 运维：创建容器[0.5 分]</h4><p>在 自 行 搭 建 的 OpenStack 私 有 云 平 台 上 ， 在 &#x2F;root 目 录 下 编 写 Heat 模 板create_container.yaml，要求执行 yaml 文件可以创建名为 heat-swift 的容器。完成后提交控制节点的用户名、密码和 IP 地址到答题框。（在提交信息前请准备好 yaml 模板执行的环境）</p>
<p>1.执行 heat 模板文件成功创建容器计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_container.yaml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  server:</span><br><span class="line">    type: OS::Swift::Container</span><br><span class="line">    properties:</span><br><span class="line">      name: heat-swift</span><br><span class="line">[root@controller ~]# openstack stack create -t create_container.yaml heat-swift</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| Field               | Value                                |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| id                  | 81873448-b1ad-48d9-9bbc-31fb158e1881 |</span><br><span class="line">| stack_name          | heat-swift                           |</span><br><span class="line">| description         | No description                       |</span><br><span class="line">| creation_time       | 2023-03-07T07:50:52Z                 |</span><br><span class="line">| updated_time        | None                                 |</span><br><span class="line">| stack_status        | CREATE_IN_PROGRESS                   |</span><br><span class="line">| stack_status_reason | Stack CREATE started                 |</span><br><span class="line">+---------------------+--------------------------------------+   </span><br></pre></td></tr></table></figure>

<h4 id="【题目-6】OpenStack-Nova-清除缓存-0-5-分"><a href="#【题目-6】OpenStack-Nova-清除缓存-0-5-分" class="headerlink" title="【题目 6】OpenStack Nova 清除缓存[0.5 分]"></a>【题目 6】OpenStack Nova 清除缓存[0.5 分]</h4><p>在 OpenStack 平台的一台计算节点创建虚拟机，若是第一次在该节点创建次虚拟机，会先将镜像文件复制到该计算节点目录&#x2F;var&#x2F;lib&#x2F;nova&#x2F;instances&#x2F;_base。长期下来，该目录会占用比较大的磁盘空间而要清理。可以通过修改 nova 的配置文件来自动清理该缓存目录，即在该节点没有使用某镜像启动的云主机，那么这个镜像在过一定的时间后会被自动删除。配置完成后提交改动节点的用户名、密码和 IP 地址到答题框。<br>1.检查 nova 配置自动清理缓存文件正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/nova/nova.conf | grep remove_unused_base_images=true</span><br><span class="line">remove_unused_base_images=true</span><br><span class="line">[root@controller ~]# systemctl restart *nova*</span><br></pre></td></tr></table></figure>

<h4 id="【题目-7】Redis-一主二从三哨兵模式-1-分"><a href="#【题目-7】Redis-一主二从三哨兵模式-1-分" class="headerlink" title="【题目 7】Redis 一主二从三哨兵模式[1 分]"></a>【题目 7】Redis 一主二从三哨兵模式[1 分]</h4><p>使用提供的 OpenStack 私有云平台，申请三台 CentOS7.9 系统的云主机，使用提供的http 源，在三个节点自行安装 Redis 服务并启动，配置 Redis 的访问需要密码，密码设置为123456。然后将这三个 Redis 节点配置为 Redis 的一主二从三哨兵架构，即一个 Redis 主节点，两个从节点，三个节点均为哨兵节点。配置完成后提交 Redis 主节点的用户名、密码和IP 地址到答题框。<br>1.检查 redis 主从集群部署正确计 0.5 分<br>2.检查 redis 集群部署为哨兵节点正确计 0.5 分</p>
<p>主节点：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf </span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">daemonize yes</span><br><span class="line">masterauth 123456</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<p>从节点1：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf</span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">daemonize yes</span><br><span class="line">slaveof 192.168.20.104 6379</span><br><span class="line">masterauth 123456</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@redis-2 ~]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; info</span><br><span class="line">.................</span><br><span class="line"># Replication</span><br><span class="line">role:slave</span><br><span class="line">master_host:192.168.20.104</span><br><span class="line">master_port:6379</span><br><span class="line">master_link_status:up</span><br><span class="line">master_last_io_seconds_ago:7</span><br><span class="line">master_sync_in_progress:0</span><br><span class="line">slave_repl_offset:589</span><br><span class="line">slave_priority:100</span><br><span class="line">slave_read_only:1</span><br><span class="line">connected_slaves:0</span><br><span class="line">master_repl_offset:0</span><br><span class="line">repl_backlog_active:0</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:0</span><br><span class="line">repl_backlog_histlen:0</span><br><span class="line">....................</span><br></pre></td></tr></table></figure>

<p>从节点2：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf</span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">daemonize yes</span><br><span class="line">slaveof 192.168.20.104 6379</span><br><span class="line">masterauth 123456</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@redis-3 ~]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; info</span><br><span class="line">.................</span><br><span class="line"># Replication</span><br><span class="line">role:slave</span><br><span class="line">master_host:192.168.20.104</span><br><span class="line">master_port:6379</span><br><span class="line">master_link_status:up</span><br><span class="line">master_last_io_seconds_ago:7</span><br><span class="line">master_sync_in_progress:0</span><br><span class="line">slave_repl_offset:589</span><br><span class="line">slave_priority:100</span><br><span class="line">slave_read_only:1</span><br><span class="line">connected_slaves:0</span><br><span class="line">master_repl_offset:0</span><br><span class="line">repl_backlog_active:0</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:0</span><br><span class="line">repl_backlog_histlen:0</span><br><span class="line">....................</span><br></pre></td></tr></table></figure>

<p>1、哨兵模式搭建：<br>哨兵模式需要修改sentinel.conf文件，三台服务器均为此配置</p>
<p><code>vi /etc/redis-sentinel.conf</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">端口默认为26379。</span><br><span class="line">port 26379</span><br><span class="line">关闭保护模式，可以外部访问。</span><br><span class="line">protected-mode no</span><br><span class="line">设置为后台启动。</span><br><span class="line">daemonize yes</span><br><span class="line">指定主机IP地址和端口(三台配置均为指定主机ip)，并且指定当有2台哨兵认为主机宕机，则对主机进行容灾切换。mymaster：设置master名字，必须在其它有用到该名字的命令之前设置</span><br><span class="line">sentinel monitor mymaster 192.168.20.104 6379 2</span><br><span class="line">当在Redis实例中开启了requirepass，这里就需要提供密码。</span><br><span class="line">sentinel auth-pass mymaster 123456</span><br><span class="line">这里设置了主机多少秒无响应，则认为挂了。此处3秒</span><br><span class="line">sentinel down-after-milliseconds mymaster 3000</span><br><span class="line">主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的1。</span><br><span class="line">snetinel parallel-syncs mymaster 1</span><br><span class="line">故障转移的超时时间，这里设置为三分钟。</span><br><span class="line">sentinel failover-timeout mymaster 180000</span><br></pre></td></tr></table></figure>

<h4 id="【题目-8】Redis-服务调优-AOF-1-分"><a href="#【题目-8】Redis-服务调优-AOF-1-分" class="headerlink" title="【题目 8】Redis 服务调优-AOF[1 分]"></a>【题目 8】Redis 服务调优-AOF[1 分]</h4><p>使用上一题安装的 Redis 服务。在 Redis 中，AOF 配置为以三种不同的方式在磁盘上执行 write 或者 fsync。假设当前 Redis 压力过大，请配置 Redis 不执行 fsync。除此之外，避免AOF 文件过大，Redis 会进行 AOF 重写，生成缩小的 AOF 文件。请修改配置，让 AOF 重写时，不进行 fsync 操作。配置完成后提交 Redis 节点的用户名、密码和 IP 地址到答题框。<br>1.检查配置 redis 不执行 fsync 正确计 0.5 分、<br>2.检查配置 redis 进行 AOF 重写不执行 fsync 正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# vi /etc/redis.conf</span><br><span class="line">appendonly yes</span><br><span class="line">appendfsync no                 # 将原来的everysec修改为no,表示不执行fsync</span><br><span class="line">no-appendfsync-on-rewrite yes  # 将原来的no改为yes,表示进行aof重写不执行fsync</span><br></pre></td></tr></table></figure>

<h4 id="【题目-9】应用部署：堡垒机部署-0-5-分"><a href="#【题目-9】应用部署：堡垒机部署-0-5-分" class="headerlink" title="【题目 9】应用部署：堡垒机部署[0.5 分]"></a>【题目 9】应用部署：堡垒机部署[0.5 分]</h4><p>使用提供的 OpenStack 平台申请一台 CentOS7.9 的云主机，使用提供的软件包安装JumpServer 堡垒机服务，并配置使用该堡垒机对接自己安装的 controller 和 compute 节点。完成后提交 JumpServer 节点的用户名、密码和 IP 地址到答题框。<br>1.检查堡垒机部署正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver ~]# ls</span><br><span class="line">jumpserver.tar.gz</span><br><span class="line">[root@jumpserver ~]# tar -zxvf jumpserver.tar.gz -C /opt/</span><br><span class="line">[root@jumpserver ~]# cd /opt/</span><br><span class="line">[root@jumpserver opt]# ls</span><br><span class="line">compose  config  docker  docker.service  images  jumpserver  jumpserver-repo  static.env</span><br><span class="line">[root@jumpserver ~]# cd /etc/yum.repos.d/</span><br><span class="line">[root@jumpserver yum.repos.d]# mkdir bak</span><br><span class="line">[root@jumpserver yum.repos.d]# mv CentOS-* bak/</span><br><span class="line">[root@jumpserver yum.repos.d]# cat local.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=ftp://192.168.200.10/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">[jumpserver ]</span><br><span class="line">name=jumpserver</span><br><span class="line">baseurl=file:///opt/jumpserver-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">[root@jumpserver ~]# yum install python2 -y</span><br><span class="line">[root@jumpserver opt]# cp -rf /opt/docker/* /usr/bin/</span><br><span class="line">[root@jumpserver opt]# chmod 775 /usr/bin/docker*</span><br><span class="line">[root@jumpserver opt]# cp -rf /opt/docker.service /etc/systemd/system</span><br><span class="line">[root@jumpserver opt]# chmod 775 /etc/systemd/system/docker.service </span><br><span class="line">[root@jumpserver opt]# systemctl daemon-reload</span><br><span class="line">[root@jumpserver opt]# systemctl enable docker --now</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /etc/systemd/system/docker.service.</span><br><span class="line">[root@jumpserver opt]# docker -v</span><br><span class="line">Docker version 18.06.3-ce, build d7080c1</span><br><span class="line">[root@jumpserver opt]# docker-compose -v</span><br><span class="line">docker-compose version 1.27.4, build 40524192</span><br><span class="line">[root@jumpserver opt]# </span><br><span class="line">[root@jumpserver images]# ls</span><br><span class="line">jumpserver_core_v2.11.4.tar  jumpserver_lion_v2.11.4.tar  jumpserver_nginx_alpine2.tar</span><br><span class="line">jumpserver_koko_v2.11.4.tar  jumpserver_luna_v2.11.4.tar  jumpserver_redis_6-alpine.tar</span><br><span class="line">jumpserver_lina_v2.11.4.tar  jumpserver_mysql_5.tar       load.sh</span><br><span class="line">[root@jumpserver images]# cat load.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">docker load -i jumpserver_core_v2.11.4.tar</span><br><span class="line">docker load -i jumpserver_koko_v2.11.4.tar</span><br><span class="line">docker load -i jumpserver_lina_v2.11.4.tar</span><br><span class="line">docker load -i jumpserver_lion_v2.11.4.tar</span><br><span class="line">docker load -i jumpserver_luna_v2.11.4.tar</span><br><span class="line">docker load -i jumpserver_mysql_5.tar</span><br><span class="line">docker load -i jumpserver_nginx_alpine2.tar</span><br><span class="line">docker load -i jumpserver_redis_6-alpine.tar</span><br><span class="line">[root@jumpserver images]# sh load.sh </span><br><span class="line">[root@jumpserver images]# mkdir -p /opt/jumpserver/&#123;core,koko,lion,mysql,nginx,redis&#125;</span><br><span class="line">[root@jumpserver images]# cp -rf /opt/config /opt/jumpserver/</span><br><span class="line">[root@jumpserver compose]# ls</span><br><span class="line">config_static                docker-compose-lb.yml              docker-compose-network.yml         down.sh</span><br><span class="line">docker-compose-app.yml       docker-compose-mysql-internal.yml  docker-compose-redis-internal.yml  up.sh</span><br><span class="line">docker-compose-es.yml        docker-compose-mysql.yml           docker-compose-redis.yml</span><br><span class="line">docker-compose-external.yml  docker-compose-network_ipv6.yml    docker-compose-task.yml</span><br><span class="line">[root@jumpserver compose]# source /opt/static.env </span><br><span class="line">[root@jumpserver compose]# sh up.sh </span><br><span class="line">Creating network &quot;jms_net&quot; with driver &quot;bridge&quot;</span><br><span class="line">Creating jms_redis ... done</span><br><span class="line">Creating jms_mysql ... done</span><br><span class="line">Creating jms_core  ... done</span><br><span class="line">Creating jms_lina   ... done</span><br><span class="line">Creating jms_nginx  ... done</span><br><span class="line">Creating jms_celery ... done</span><br><span class="line">Creating jms_lion   ... done</span><br><span class="line">Creating jms_luna   ... done</span><br><span class="line">Creating jms_koko   ... done</span><br><span class="line">[root@jumpserver compose]#</span><br></pre></td></tr></table></figure>

<h4 id="【题目-10】skywalking-服务部署与应用-1-分"><a href="#【题目-10】skywalking-服务部署与应用-1-分" class="headerlink" title="【题目 10】skywalking 服务部署与应用[1 分]"></a>【题目 10】skywalking 服务部署与应用[1 分]</h4><p>使用提供的 OpenStack 私有云平台，申请一台 centos7.9 系统的云主机，使用提供的软件包安装 Elasticsearch 服务和 skywalking 服务，将 skywalking 的 UI 访问端口修改为 8888。接下来再申请一台 CentOS7.9 的云主机，用于搭建 gpmall 商城应用，并配置 SkyWalking Agent，将 gpmall 的 jar 包放置探针并启动。安装与配置完成后提交 skywalking 节点的用户名、密码和 IP 地址到答题框。<br>1.检查 skywalking 服务部署正确计 1 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1. 部署 Elasticsearch 服务</span><br><span class="line">（1）修改主机名，修改主机名后，单击实验环境左侧工具栏最后一个重连按钮以生效新主</span><br><span class="line">机名。</span><br><span class="line">[root@node-1 ~]# hostnamectl set-hostname node-1</span><br><span class="line">[root@node-1 ~]# hostnamectl</span><br><span class="line">Static hostname: node-1</span><br><span class="line">Icon name: computer-vm</span><br><span class="line">Chassis: vm</span><br><span class="line">Machine ID: cc2c86fe566741e6a2ff6d399c5d5daa</span><br><span class="line">Boot ID: 6c32a0c1d29e4f30929422c8032239ca</span><br><span class="line">Virtualization: kvm</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">Kernel: Linux 3.10.0-1160.el7.x86_64</span><br><span class="line">Architecture: x86-64</span><br><span class="line">（2）将提供的 elasticsearch-7.17.0-linux-x86_64.tar.gz 软件包下载到此节点并解压到/opt</span><br><span class="line">目录，进入解压后的目录并创建 data 目录：</span><br><span class="line">[root@node-1 ~]# curl -O http://mirrors.douxuedu.com/competition/elasticsearch-7.17.</span><br><span class="line">0-linux-x86_64.tar.gz</span><br><span class="line">[root@node-1 ~]# tar -zxvf elasticsearch-7.17.0-linux-x86_64.tar.gz -C /opt</span><br><span class="line">[root@node-1 ~]# cd /opt/elasticsearch-7.17.0/</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# mkdir data</span><br><span class="line">（3）修改 Elasticsearch 配置，在文件最后添加如下几行内容，按“i”建进入编辑模式进行配</span><br><span class="line">置，按 ESC 键输入:wq 保存退出。</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# vi config/elasticsearch.yml</span><br><span class="line">...</span><br><span class="line">cluster.name: my-application</span><br><span class="line">node.name: node-1</span><br><span class="line">path.data: /opt/elasticsearch-7.17.0/data</span><br><span class="line">path.logs: /opt/elasticsearch-7.17.0/logs</span><br><span class="line">network.host: 0.0.0.0</span><br><span class="line">cluster.initial_master_nodes: [&quot;node-1&quot;]</span><br><span class="line">http.cors.enabled: true</span><br><span class="line">http.cors.allow-origin: &quot;*&quot;</span><br><span class="line">http.cors.allow-headers: Authorization,X-Requested-With,Content-Length,Content-Type</span><br><span class="line">（4）创建 Elasticsearch 启动用户，并设置属组及权限：</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# groupadd elsearch</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# useradd elsearch -g elsearch -p elasticsearch</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# chown -R elsearch:elsearch /opt/elasticsearch-7.</span><br><span class="line">17.0</span><br><span class="line">（5）修改资源限制及内核配置，添加如下内容：</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# vi /etc/security/limits.conf</span><br><span class="line">...</span><br><span class="line">* hard nofile 65536</span><br><span class="line">* soft nofile 65536</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# vi /etc/sysctl.conf</span><br><span class="line">vm.max_map_count=262144</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# sysctl -p</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# reboot</span><br><span class="line">重启后等待一段时间后刷新页面点击重新连接。</span><br><span class="line">（6）启动 Elasticsearch 服务：</span><br><span class="line">[root@node-1 ~]# cd /opt/elasticsearch-7.17.0/</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# su elsearch</span><br><span class="line">[elsearch@node-1 elasticsearch-7.17.0]$ ./bin/elasticsearch -d</span><br><span class="line">（7）查询端口，存在 9200 则成功启动：</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]$ netstat -ntpl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program n</span><br><span class="line">ame tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1081/ss</span><br><span class="line">hd tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1041</span><br><span class="line">/master tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 6</span><br><span class="line">10/rpcbind</span><br><span class="line">tcp6 0 0 :::9300 :::* LISTEN 2261/java</span><br><span class="line">tcp6 0 0 :::22 :::* LISTEN 1081/sshd</span><br><span class="line">tcp6 0 0 ::1:25 :::* LISTEN 1041/master</span><br><span class="line">tcp6 0 0 :::111 :::* LISTEN 610/rpcbind</span><br><span class="line">tcp6 0 0 :::9200 :::* LISTEN 2261/java</span><br><span class="line">（8）切换至带有桌面的虚拟机环境中，使用浏览器访问 10.24.193.154:9200，如图 1 所示：</span><br><span class="line">图 1 Elasticsearch 服务访问</span><br><span class="line">2. 部署 SkyWalking OAP 服务</span><br><span class="line">（1）将提供的 jdk-8u144-linux-x64.tar.gz 软件包下载至 node-1 节点/root/目录中，并配置</span><br><span class="line">jdk 如下所示：</span><br><span class="line">[elsearch@node-1 elasticsearch-7.17.0]$ exit</span><br><span class="line">[root@node-1 elasticsearch-7.17.0]# cd</span><br><span class="line">[root@node-1 ~]# curl -O http://mirrors.douxuedu.com/competition/jdk-8u144-linux-x6</span><br><span class="line">4.tar.gz</span><br><span class="line">[root@node-1 ~]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /usr/local/</span><br><span class="line">修改 profile 环境变量文件，代码如下所示：</span><br><span class="line">[root@node-1 ~]# vi /etc/profile</span><br><span class="line">export JAVA_HOME=/usr/local/jdk1.8.0_144</span><br><span class="line">export CLASSPATH=.:$&#123;JAVA_HOME&#125;/jre/lib/rt.jar:$&#123;JAVA_HOME&#125;/lib/dt.jar:$&#123;JAVA_HOME&#125;/</span><br><span class="line">lib/tools.jar</span><br><span class="line">export PATH=$PATH:$&#123;JAVA_HOME&#125;/bin</span><br><span class="line">[root@node-1 ~]# source /etc/profile</span><br><span class="line">[root@node-1 ~]# java -version</span><br><span class="line">java version &quot;1.8.0_144&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_144-b01)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)</span><br><span class="line">（2）将提供的 apache-skywalking-apm-es7-8.0.0.tar.gz 软件包下载至 node-1 节点上并解</span><br><span class="line">压到/opt 目录下：</span><br><span class="line">[root@node-1 ~]# curl -O http://mirrors.douxuedu.com/competition/apache-skywalking-a</span><br><span class="line">pm-es7-8.0.0.tar.gz</span><br><span class="line">[root@node-1 ~]# tar -zxvf apache-skywalking-apm-es7-8.0.0.tar.gz -C /opt</span><br><span class="line">（3）进入解压后目录，修改 OAP 配置文件：</span><br><span class="line">[root@node-1 ~]# cd /opt/apache-skywalking-apm-bin-es7/</span><br><span class="line">[root@node-1 apache-skywalking-apm-bin-es7]# vi config/application.yml</span><br><span class="line">...</span><br><span class="line">#集群配置使用单机版</span><br><span class="line">cluster:</span><br><span class="line">selector: $&#123;SW_CLUSTER:standalone&#125;</span><br><span class="line">standalone:</span><br><span class="line">...</span><br><span class="line">#数据库使用 elasticsearch7</span><br><span class="line">storage:</span><br><span class="line">selector: $&#123;SW_STORAGE:elasticsearch7&#125;</span><br><span class="line">...</span><br><span class="line">elasticsearch7:</span><br><span class="line">nameSpace: $&#123;SW_NAMESPACE:&quot;&quot;&#125;</span><br><span class="line">clusterNodes: $&#123;SW_STORAGE_ES_CLUSTER_NODES:10.24.194.203:9200&#125;</span><br><span class="line">...</span><br><span class="line">（4）启动 OAP 服务，查询端口，存在 11800 与 12800 则成功启动：</span><br><span class="line">[root@node-1 apache-skywalking-apm-bin-es7]# ./bin/oapService.sh</span><br><span class="line">SkyWalking OAP started successfully!</span><br><span class="line">[root@node-1 apache-skywalking-apm-bin-es7]# netstat -ntpl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program n</span><br><span class="line">ame</span><br><span class="line">tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1081/sshd</span><br><span class="line">tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1041/master</span><br><span class="line">tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 610/rpcbind</span><br><span class="line">tcp6 0 0 :::9300 :::* LISTEN 2261/java</span><br><span class="line">tcp6 0 0 :::22 :::* LISTEN 1081/sshd</span><br><span class="line">tcp6 0 0 :::11800 :::* LISTEN 2416/java</span><br><span class="line">tcp6 0 0 ::1:25 :::* LISTEN 1041/master</span><br><span class="line">tcp6 0 0 :::12800 :::* LISTEN 2416/java</span><br><span class="line">tcp6 0 0 :::111 :::* LISTEN 610/rpcbind</span><br><span class="line">tcp6 0 0 :::9200 :::* LISTEN 2261/java</span><br><span class="line">3. 部署 SkyWalking UI 服务</span><br><span class="line">（1）由于 SkyWalking UI 的默认地址是 8080，与很多中间件可能存在冲突，修改一下：</span><br><span class="line">[root@node-1 apache-skywalking-apm-bin-es7]# vi webapp/webapp.yml</span><br><span class="line">...</span><br><span class="line">server:</span><br><span class="line">port: 8888</span><br><span class="line">...</span><br><span class="line">（2）启动 SkyWalking UI 服务：</span><br><span class="line">[root@node-1 apache-skywalking-apm-bin-es7]# ./bin/webappService.sh</span><br><span class="line">SkyWalking Web Application started successfully!</span><br><span class="line">（3）查看端口，存在 8888 则成功启动：</span><br><span class="line">[root@node-1 apache-skywalking-apm-bin-es7]# netstat -ntpl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program n</span><br><span class="line">ame</span><br><span class="line">tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1081/sshd</span><br><span class="line">tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 1041/master</span><br><span class="line">tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 610/rpcbind</span><br><span class="line">tcp6 0 0 :::9300 :::* LISTEN 2261/java</span><br><span class="line">tcp6 0 0 :::22 :::* LISTEN 1081/sshd</span><br><span class="line">tcp6 0 0 :::8888 :::* LISTEN 3133/java</span><br><span class="line">tcp6 0 0 :::11800 :::* LISTEN 2416/java</span><br><span class="line">tcp6 0 0 ::1:25 :::* LISTEN 1041/master</span><br><span class="line">tcp6 0 0 :::12800 :::* LISTEN 2416/java</span><br><span class="line">tcp6 0 0 :::111 :::* LISTEN 610/rpcbind</span><br><span class="line">tcp6 0 0 :::9200 :::* LISTEN 2261/java</span><br><span class="line">（4）切换带有桌面的虚拟机环境，使用火狐浏览器访问 10.24.194.203:8888，此时访问页面</span><br><span class="line">无数据，如图 2 所示：</span><br><span class="line">图 2 访问 SkyWalking UI 服务</span><br><span class="line">4. 搭建并启动应用商城服务</span><br><span class="line">切换至提供 mall 节点，搭建并启动应用商城服务，并配置 SkyWalking Agent</span><br><span class="line">（1）修改 mall 节点主机名，修改完成后，单击实验环境左侧工具栏最后一个重连按钮以生</span><br><span class="line">效新主机名。</span><br><span class="line">[root@localhost ~]# hostnamectl set-hostname mall</span><br><span class="line">[root@mall ~]# hostnamectl</span><br><span class="line">Static hostname: mall</span><br><span class="line">Icon name: computer-vm</span><br><span class="line">Chassis: vm</span><br><span class="line">Machine ID: cc2c86fe566741e6a2ff6d399c5d5daa</span><br><span class="line">Boot ID: 51559d155ec14aafad2411ca8b85db42</span><br><span class="line">Virtualization: kvm</span><br><span class="line">Operating System: CentOS Linux 7 (Core)</span><br><span class="line">CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">Kernel: Linux 3.10.0-1160.el7.x86_64</span><br><span class="line">Architecture: x86-64</span><br><span class="line">修改/etc/hosts 配置文件如下：</span><br><span class="line">[root@mall ~]# vi /etc/hosts</span><br><span class="line">127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1 localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">10.24.193.168 mall</span><br><span class="line">（2）配置本地 Yum 源</span><br><span class="line">将提供的 gpmall 包下载到服务器的/root 目录下并解压 gpmall.tar.gz，配置本地 local.repo</span><br><span class="line">文件，具体代码如下所示：</span><br><span class="line">[root@mall ~]# curl -O http://mirrors.douxuedu.com/competition/gpmall.tar.gz</span><br><span class="line">[root@mall ~]# tar -zxvf gpmall.tar.gz</span><br><span class="line">[root@mall ~]# mv /etc/yum.repos.d/* /media/</span><br><span class="line">[root@mall ~]# cd gpmall/</span><br><span class="line">[root@mall gpmall]# tar -zxvf gpmall-repo.tar.gz -C /root/</span><br><span class="line">[root@mall ~]# vi /etc/yum.repos.d/local.repo</span><br><span class="line">[mall]</span><br><span class="line">name=mall</span><br><span class="line">baseurl=file:///root/gpmall-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line">（3）安装基础服务</span><br><span class="line">安装基础服务，包括 Java JDK 环境、数据库、Redis、Nginx 等，安装基础服务的命令具体</span><br><span class="line">如下。</span><br><span class="line">① 安装 Java 环境</span><br><span class="line">[root@mall ~]# yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel</span><br><span class="line">...</span><br><span class="line">[root@mall ~]# java -version</span><br><span class="line">openjdk version &quot;1.8.0_322&quot;</span><br><span class="line">OpenJDK Runtime Environment (build 1.8.0_322-b06)</span><br><span class="line">OpenJDK 64-Bit Server VM (build 25.322-b06, mixed mode)</span><br><span class="line">② 安装 Redis 缓存服务</span><br><span class="line">[root@mall ~]# yum install redis -y</span><br><span class="line">③ 安装 Nginx 服务</span><br><span class="line">[root@mall ~]# yum install nginx -y</span><br><span class="line">④ 安装 MariaDB 数据库</span><br><span class="line">[root@mall ~]# yum install mariadb mariadb-server -y</span><br><span class="line">安装 ZooKeeper 服务，将提供的 zookeeper-3.4.14.tar.gz 解压，命令如下：</span><br><span class="line">[root@mall gpmall]# tar -zxvf zookeeper-3.4.14.tar.gz -C /root/</span><br><span class="line">进入到 zookeeper-3.4.14/conf 目录下，将 zoo_sample.cfg 文件重命名为 zoo.cfg，命令如</span><br><span class="line">下：</span><br><span class="line">[root@mall gpmall]# cd /root/zookeeper-3.4.14/conf/</span><br><span class="line">[root@mall conf]# mv zoo_sample.cfg zoo.cfg</span><br><span class="line">进入到 zookeeper-3.4.14/bin 目录下，启动 ZooKeeper 服务，命令如下：</span><br><span class="line">[root@mall conf]# cd ../bin</span><br><span class="line">[root@mall bin]# ./zkServer.sh start</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /root/zookeeper-3.4.14/bin/../conf/zoo.cfg</span><br><span class="line">Starting zookeeper ... STARTED</span><br><span class="line">查看 ZooKeeper 状态，命令如下：</span><br><span class="line">[root@mall bin]# ./zkServer.sh status</span><br><span class="line">ZooKeeper JMX enabled by default</span><br><span class="line">Using config: /root/zookeeper-3.4.14/bin/../conf/zoo.cfg</span><br><span class="line">Mode: standalone</span><br><span class="line">安装 Kafka 服务，将提供的 kafka_2.11-1.1.1.tgz 解压至/root 目录下，命令如下：</span><br><span class="line">[root@mall bin]# tar -zxvf /root/gpmall/kafka_2.11-1.1.1.tgz -C /root</span><br><span class="line">进入到 kafka_2.11-1.1.1/bin 目录下，启动 Kafka 服务，命令如下：</span><br><span class="line">[root@mall bin]# cd /root/kafka_2.11-1.1.1/bin/</span><br><span class="line">[root@mall bin]# ./kafka-server-start.sh -daemon ../config/server.properties</span><br><span class="line">使用 jps 或者 netstat -ntpl 命令查看 Kafka 是否成功启动，命令如下：</span><br><span class="line">[root@mall bin]# jps</span><br><span class="line">6039 Kafka</span><br><span class="line">1722 QuorumPeerMain</span><br><span class="line">6126 Jps</span><br><span class="line">[root@mall bin]# netstat -ntpl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program n</span><br><span class="line">ame</span><br><span class="line">tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1008/sshd</span><br><span class="line">tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN 887/master</span><br><span class="line">tcp6 0 0 :::9092 :::* LISTEN 6039/java</span><br><span class="line">tcp6 0 0 :::46949 :::* LISTEN 6039/java</span><br><span class="line">tcp6 0 0 :::2181 :::* LISTEN 1722/java</span><br><span class="line">tcp6 0 0 :::48677 :::* LISTEN 1722/java</span><br><span class="line"></span><br><span class="line">tcp6 0 0 :::22 :::* LISTEN 1008/sshd</span><br><span class="line">tcp6 0 0 ::1:25 :::* LISTEN 887/mast</span><br><span class="line">运行结果查看到 Kafka 服务和 9092 端口，说明 Kafka 服务已启动。</span><br><span class="line">（4）启动服务</span><br><span class="line">① 启动数据库并配置</span><br><span class="line">修改数据库配置文件并启动 MariaDB 数据库，设置 root 用户密码为 123456，并创建 gpmall</span><br><span class="line">数据库，将提供的 gpmall.sql 导入。</span><br><span class="line">修改/etc/my.cnf 文件，添加字段如下所示：</span><br><span class="line">[root@mall bin]# cd</span><br><span class="line">[root@mall ~]# vi /etc/my.cnf</span><br><span class="line">[mysqld]</span><br><span class="line">...</span><br><span class="line">init_connect=&#x27;SET collation_connection = utf8_unicode_ci&#x27;</span><br><span class="line">init_connect=&#x27;SET NAMES utf8&#x27;</span><br><span class="line">character-set-server=utf8</span><br><span class="line">collation-server=utf8_unicode_ci</span><br><span class="line">skip-character-set-client-handshake</span><br><span class="line">启动数据库命令如下。</span><br><span class="line">[root@mall ~]# systemctl start mariadb</span><br><span class="line">设置 root 用户的密码为 123456 并登录。</span><br><span class="line">[root@mall ~]# mysqladmin -uroot password 123456</span><br><span class="line">[root@mall ~]# mysql -uroot -p123456</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 3</span><br><span class="line">Server version: 5.5.68-MariaDB MariaDB Server</span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line">MariaDB [(none)]&gt;</span><br><span class="line">设置 root 用户的权限，命令如下：</span><br><span class="line">MariaDB [(none)]&gt; grant all privileges on *.* to root@localhost identified by &#x27;12345</span><br><span class="line">6&#x27; with grant option;</span><br><span class="line">Query OK, 0 rows affected (0.001 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; grant all privileges on *.* to root@&quot;%&quot; identified by &#x27;123456&#x27; with</span><br><span class="line">grant option;</span><br><span class="line">Query OK, 0 rows affected (0.001 sec)</span><br><span class="line">创建数据库 gpmall 并导入/root/gpmall/目录中的 gpmall.sql 文件。</span><br><span class="line">MariaDB [(none)]&gt; create database gpmall;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">MariaDB [(none)]&gt; use gpmall;</span><br><span class="line">MariaDB [gpmall]&gt; source /root/gpmall/gpmall.sql</span><br><span class="line">退出数据库并设置开机自启。</span><br><span class="line">MariaDB [gpmall]&gt; exit</span><br><span class="line">Bye</span><br><span class="line">[root@mall ~]# systemctl enable mariadb</span><br><span class="line">Created symlink from /etc/systemd/system/mysql.service to /usr/lib/systemd/system/ma</span><br><span class="line">riadb.service.</span><br><span class="line">Created symlink from /etc/systemd/system/mysqld.service to /usr/lib/systemd/system/m</span><br><span class="line">ariadb.service.</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/mariadb.service to</span><br><span class="line">/usr/lib/systemd/system/mariadb.service.</span><br><span class="line">② 启动 Redis 服务</span><br><span class="line">修改 Redis 配置文件，编辑/etc/redis.conf 文件。</span><br><span class="line">将 bind 127.0.0.1 这一行注释掉；将 protected-mode yes 改为 protected-mode no。</span><br><span class="line">启动 Redis 服务命令如下。</span><br><span class="line">[root@mall ~]# vi /etc/redis.conf</span><br><span class="line">...</span><br><span class="line">\#bind 127.0.0.1</span><br><span class="line">protected-mode no</span><br><span class="line">...</span><br><span class="line">[root@mall ~]# systemctl start redis</span><br><span class="line">[root@mall ~]# systemctl enable redis</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/redis.service to /u</span><br><span class="line">sr/lib/systemd/system/redis.service.</span><br><span class="line">③ 启动 Nginx 服务</span><br><span class="line">启动 Nginx 服务命令如下。</span><br><span class="line">[root@mall ~]# systemctl start nginx</span><br><span class="line">[root@mall ~]# systemctl enable nginx</span><br><span class="line"></span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/nginx.service to /u</span><br><span class="line">sr/lib/systemd/system/nginx.service.</span><br><span class="line">（5）应用系统部署</span><br><span class="line">使 用 提 供 gpmall-shopping-0.0.1-SNAPSHOT.jar 、 gpmall-user-0.0.1-SNAPSHOT.jar 、</span><br><span class="line">shopping-provider-0.0.1-SNAPSHOT.jar、user-provider-0.0.1-SNAPSHOT.jar 、dist 这 5 个</span><br><span class="line">包部署应用系统，其中 4 个 jar 包为后端服务包，dist 为前端包。（包在 gpmall 目录下）</span><br><span class="line">① 全局变量配置</span><br><span class="line">修改/etc/hosts 文件，修改项目全局配置文件如下（原有的 172.128.11.42 mall 映射删除）：</span><br><span class="line">[root@mall ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1 localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">10.24.193.168 kafka.mall</span><br><span class="line">10.24.193.168 mysql.mall</span><br><span class="line">10.24.193.168 redis.mall</span><br><span class="line">10.24.193.168 zookeeper.mall</span><br><span class="line">② 部署前端</span><br><span class="line">清空默认项目路径下的文件，将 dist 目录下的文件，复制到 Nginx 默认项目路径（文件在</span><br><span class="line">gpmall 目录下）。</span><br><span class="line">[root@mall ~]# rm -rf /usr/share/nginx/html/*</span><br><span class="line">[root@mall ~]# cp -rvf gpmall/dist/* /usr/share/nginx/html/</span><br><span class="line">修改 Nginx 配置文件/etc/nginx/nginx.conf，添加映射如下所示：</span><br><span class="line">[root@mall ~]# vi /etc/nginx/nginx.conf</span><br><span class="line">...</span><br><span class="line">server &#123;</span><br><span class="line">listen 80;</span><br><span class="line">listen [::]:80;</span><br><span class="line">server_name _;</span><br><span class="line">root /usr/share/nginx/html;</span><br><span class="line">\# Load configuration files for the default server block.</span><br><span class="line">include /etc/nginx/default.d/*.conf;</span><br><span class="line">location / &#123;</span><br><span class="line">root /usr/share/nginx/html;</span><br><span class="line">index index.html index.htm;</span><br><span class="line">&#125;</span><br><span class="line">location /user &#123;</span><br><span class="line"></span><br><span class="line">proxy_pass http://127.0.0.1:8082;</span><br><span class="line">&#125;</span><br><span class="line">location /shopping &#123;</span><br><span class="line">proxy_pass http://127.0.0.1:8081;</span><br><span class="line">&#125;</span><br><span class="line">location /cashier &#123;</span><br><span class="line">proxy_pass http://127.0.0.1:8083;</span><br><span class="line">&#125;</span><br><span class="line">error_page 404 /404.html;</span><br><span class="line">...</span><br><span class="line">重启 Nginx 服务，命令如下：</span><br><span class="line">[root@mall ~]# systemctl restart nginx</span><br><span class="line">到此，前端部署完毕。</span><br><span class="line">③ 部署后端</span><br><span class="line">将 node-1 节点的/opt/apache-skywalking-apm-bin-es7 目录下的 agent 目录复制到 mall</span><br><span class="line">节点下(密码为 Abc@1234)：</span><br><span class="line">[root@mall ~]# scp -r 10.24.194.203:/opt/apache-skywalking-apm-bin-es7/agent /root</span><br><span class="line">修改 SkyWalking agent 配置文件：</span><br><span class="line">[root@mall ~]# vi agent/config/agent.config</span><br><span class="line">...</span><br><span class="line">agent.service_name=$&#123;SW_AGENT_NAME:my-application&#125;</span><br><span class="line">agent.sample_n_per_3_secs=$&#123;SW_AGENT_SAMPLE:1&#125;</span><br><span class="line">...</span><br><span class="line">collector.backend_service=$&#123;SW_AGENT_COLLECTOR_BACKEND_SERVICES:10.24.194.203:1180</span><br><span class="line">0&#125;</span><br><span class="line">...</span><br><span class="line">采样率修改：agent.sample_n_per_3_secs 配置说明：</span><br><span class="line">在访问量较少时，链路全量收集不会对系统带来太大负担，能够完整的观测到系统的运行状</span><br><span class="line">况。但是在访问量较大时，全量的链路收集，对链路收集的客户端（agent 探针）、服务端</span><br><span class="line">（SkyWalking OAP）、存储器（例如说 Elastcsearch）都会带来较大的性能开销，甚至会影</span><br><span class="line">响应用的正常运行。在访问量级较大的情况下，往往会选择抽样采样，只收集部分链路信息。</span><br><span class="line">SkyWalking Agent 在 agent/config/agent.config 配 置 文 件 中 ， 定 义 了</span><br><span class="line">agent.sample_n_per_3_secs 配置项，设置每 3 秒可收集的链路数据的数量。</span><br><span class="line">将 gpmall 软件包中提供的 4 个 jar 包，放置探针并启动，通过设置启动参数的方式检测系</span><br><span class="line">统，启动命令如下：</span><br><span class="line"></span><br><span class="line">[root@mall ~]# nohup java -javaagent:/root/agent/skywalking-agent.jar -jar gpmall/s</span><br><span class="line">hopping-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">[1] 20086</span><br><span class="line">[root@mall ~]# nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line">[root@mall ~]# nohup java -javaagent:/root/agent/skywalking-agent.jar -jar gpmall/u</span><br><span class="line">ser-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">[2] 20132</span><br><span class="line">[root@mall ~]# nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line">[root@mall ~]# nohup java -javaagent:/root/agent/skywalking-agent.jar -jar gpmall/g</span><br><span class="line">pmall-shopping-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">[3] 20177</span><br><span class="line">[root@mall ~]# nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line">[root@mall ~]# nohup java -javaagent:/root/agent/skywalking-agent.jar -jar gpmall/g</span><br><span class="line">pmall-user-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">[4] 20281</span><br><span class="line">[root@mall ~]# nohup: ignoring input and appending output to ‘nohup.out’</span><br><span class="line">\# httpd 访问网络配置</span><br><span class="line">[root@mall ~]# setsebool -P httpd_can_network_connect 1</span><br><span class="line">按照顺序运行 4 个 jar 包后，至此后端服务部署完毕。</span><br></pre></td></tr></table></figure>



<h4 id="【题目-11】Linux-内核优化-1-分"><a href="#【题目-11】Linux-内核优化-1-分" class="headerlink" title="【题目 11】Linux 内核优化[1 分]"></a>【题目 11】Linux 内核优化[1 分]</h4><p>在使用 Linux 服务器的时候，TCP 协议规定，对于已经建立的连接，网络双方要进行四次挥手才能成功断开连接，如果缺少了其中某个步骤，将会使连接处于假死状态，连接本身占用的资源不会被释放。因为服务器程序要同时管理大量连接，所以很有必要保证无用的连接完全断开，否则大量僵死的连接会浪费许多服务器资源。创建一台 CentOS7.9 云主机，修改相应的配置文件，分别开启 SYN Cookies；允许将 TIME-WAIT sockets 重新用于新的 TCP连接；开启 TCP 连接中 TIME-WAIT sockets 的快速回收；修改系統默认的 TIMEOUT 时间为 30。完成后提交修改节点的用户名、密码和 IP 地址到答题框。<br>1.检查内核优化正确计 1 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/sysctl.conf</span><br><span class="line">net.ipv4.tcp_syncookies = 1   #开启SYN Cookies</span><br><span class="line">net.ipv4.tcp_tw_reuse = 1     #开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接</span><br><span class="line">net.ipv4.tcp_tw_recycle = 1   #开启TCP连接中TIME-WAIT sockets的快速回收</span><br><span class="line">net.ipv4.tcp_fin_timeout = 30 #修改系統默认的 TIMEOUT 时间为 30</span><br></pre></td></tr></table></figure>

<h4 id="【题目-12】排错：Glance-服务排错-1-分"><a href="#【题目-12】排错：Glance-服务排错-1-分" class="headerlink" title="【题目 12】排错：Glance 服务排错[1 分]"></a>【题目 12】排错：Glance 服务排错[1 分]</h4><p>使用赛项提供的 chinaskill-error1 镜像启动云主机，flavor 使用 4vcpu&#x2F;12G 内存&#x2F;100G 硬盘。启动后存在错误的私有云平台，错误现象为查看不到 image 列表，试根据错误信息排查云平台错误，使云平台可以查询到 image 信息。完成后提交云主机节点的用户名、密码和 IP地址到答题框。<br>1.检查错误镜像 glance 服务启动正确计 1 分</p>
<h4 id="【题目-13】排错：数据库排错-1-5-分"><a href="#【题目-13】排错：数据库排错-1-5-分" class="headerlink" title="【题目 13】排错：数据库排错[1.5 分]"></a>【题目 13】排错：数据库排错[1.5 分]</h4><p>使用赛项提供的排错镜像 chinaskill-error2 创建一台云主机（云主机的登录用户名为 root，密码为 000000），该云主机中存在错误的数据库服务，错误现象为数据库服务无法启动。请将数据库服务修复并启动，将数据库的密码修改为 chinaskill123。修复完成后提交该云主机的用户名、密码和 IP 地址到答题框。<br>1.检查错误镜像数据库服务启动正确计 1.5 分</p>
<h3 id="【任务-3】私有云运维开发-10-分"><a href="#【任务-3】私有云运维开发-10-分" class="headerlink" title="【任务 3】私有云运维开发[10 分]"></a>【任务 3】私有云运维开发[10 分]</h3><h4 id="【题目-1】Ansible-服务部署：部署-MariaDB-集群-2-分"><a href="#【题目-1】Ansible-服务部署：部署-MariaDB-集群-2-分" class="headerlink" title="【题目 1】Ansible 服务部署：部署 MariaDB 集群[2 分]"></a>【题目 1】Ansible 服务部署：部署 MariaDB 集群[2 分]</h4><p>使用 OpenStack 私有云平台，创建 4 台系统为 centos7.9 的云主机，其中一台作为 Ansible的母机并命名为 ansible，另外三台云主机命名为 node1、node2、node3；使用这一台母机，编写 Ansible 脚本（在&#x2F;root 目录下创建 example 目录作为 Ansible 工作目录，部署的入口文件 命 名 为 cscc_install.yaml ） ， 对 其 他 三 台 云 主 机 进 行 安 装 高 可 用 数 据 库 集 群（MariaDB_Galera_cluster，数据库密码设置为 123456）的操作（所需的安装包在 HTTP 服<br>务中）。完成后提交 Ansible 节点的用户名、密码和 IP 地址到答题框。（考试系统会连接到你的 Ansible 节点，去执行 Ansible 脚本，请准备好 Ansible 运行环境，以便考试系统访问）<br>1.执行 yaml 文件正确计 0.5 分<br>2.检查数据库集群部署正确计 1 分<br>3.检查数据库集群数量为 3 正确计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver mariadb-ansible]# cat /etc/ansible/hosts </span><br><span class="line">[node-1]</span><br><span class="line">node-1</span><br><span class="line">[node-2]</span><br><span class="line">node-2</span><br><span class="line">[node-3]</span><br><span class="line">node-3</span><br><span class="line">[node]</span><br><span class="line">node-1  node_name=node-1</span><br><span class="line">node-2  node_name=node-2</span><br><span class="line">node-3  node_name=node-3</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver mariadb-ansible]# cat /etc/my.cnf.d/server.cnf </span><br><span class="line">[galera]</span><br><span class="line">wsrep_on=ON</span><br><span class="line">wsrep_provider=/usr/lib64/galera/libgalera_smm.so</span><br><span class="line">wsrep_cluster_address=&quot;gcomm://node-1,node-2,node-3&quot;</span><br><span class="line">binlog_format=row</span><br><span class="line">default_storage_engine=InnoDB</span><br><span class="line">innodb_autoinc_lock_mode=2</span><br><span class="line">bind-address=0.0.0.0</span><br><span class="line">wsrep_slave_threads=1</span><br><span class="line">innodb_flush_log_at_trx_commit=0</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver mariadb-ansible]# cat local.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=http://172.19.25.11/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[mariadb]</span><br><span class="line">name=mariadb</span><br><span class="line">baseurl=http://172.19.25.11/gpmall-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver mariadb-ansible]# cat cscc_install.yaml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">- hosts: node</span><br><span class="line">  tasks:</span><br><span class="line">  - name: copy hosts</span><br><span class="line">    copy:</span><br><span class="line">      src: /etc/hosts</span><br><span class="line">      dest: /etc/</span><br><span class="line">  - name: stop selinux </span><br><span class="line">    shell: setenforce 0</span><br><span class="line">  - name: rm repo</span><br><span class="line">    shell: rm -rf /etc/yum.repos.d/*</span><br><span class="line">  - name: copy repo</span><br><span class="line">    copy:</span><br><span class="line">      src: /root/mariadb-ansible/local.repo</span><br><span class="line">      dest: /etc/yum.repos.d/</span><br><span class="line">  - name: install mariadb</span><br><span class="line">    yum:</span><br><span class="line">      name: mariadb,mariadb-server</span><br><span class="line">  - name: start mariadb</span><br><span class="line">    service:</span><br><span class="line">      name: mariadb</span><br><span class="line">      state: started</span><br><span class="line">      enabled: true</span><br><span class="line">  - name: init_mysql</span><br><span class="line">    shell: mysqladmin -uroot password 123456</span><br><span class="line">  - name: stop mariadb</span><br><span class="line">    service:</span><br><span class="line">      name: mariadb</span><br><span class="line">      state: stopped</span><br><span class="line">  - name: copy server.cnf</span><br><span class="line">    copy:</span><br><span class="line">      src: /etc/my.cnf.d/server.cnf</span><br><span class="line">      dest: /etc/my.cnf.d/</span><br><span class="line"></span><br><span class="line">- hosts: node-1</span><br><span class="line">  tasks:</span><br><span class="line">  - name: start mariadb</span><br><span class="line">    shell: galera_new_cluster</span><br><span class="line">- hosts: node-2,node-3</span><br><span class="line">  tasks:</span><br><span class="line">  - name: start mariadb</span><br><span class="line">    service:</span><br><span class="line">      name: mariadb</span><br><span class="line">      state: started</span><br></pre></td></tr></table></figure>

<h4 id="【题目-2】Ansible-服务部署：部署-ELK-集群服务-2-分"><a href="#【题目-2】Ansible-服务部署：部署-ELK-集群服务-2-分" class="headerlink" title="【题目 2】Ansible 服务部署：部署 ELK 集群服务[2 分]"></a>【题目 2】Ansible 服务部署：部署 ELK 集群服务[2 分]</h4><p>使用赛项提供的 OpenStack 私有云平台，创建三台 CentOS7.9 系统的云主机分别命名为elk-1、elk-2 和 elk-3，Ansible 主机可以使用上一题的环境。要求 Ansible 节点编写剧本，执行 Ansible 剧本可以在这三个节点部署 ELK 集群服务（在&#x2F;root 目录下创建 install_elk 目录作为 ansible 工作目录，部署的入口文件命名为 install_elk.yaml）。具体要求为三个节点均安装Elasticserach 服务并配置为 Elasticserach 集群；kibana 安装在第一个节点；Logstash 安装在第二个节点。（需要用到的软件包在 HTTP 服务下）完成后提交 ansible 节点的用户名、密码和 IP 地址到答题框。（考试系统会连接到 ansible 节点，执行 ansible 脚本，准备好环境，<br>以便考试系统访问）<br>1.执行 yaml 文件正确计 0.5 分<br>2.检查 ELK 服务部署正确计 1.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver elk-ansible]# cat roles/es/tasks/main.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># tasks file for roles/es</span><br><span class="line"></span><br><span class="line">- name: 安装java环境</span><br><span class="line">  unarchive:</span><br><span class="line">    src: openjdk-11.0.1_linux-x64_bin.tar.gz</span><br><span class="line">    dest: /usr/local/</span><br><span class="line">- name: 添加环境变量</span><br><span class="line">  shell: echo &#x27;export JAVA_HOME=/usr/local/jdk-11.0.1&#x27; &gt;&gt; /etc/profile &amp;&amp; echo &#x27;export PATH=$PATH:/usr/local/jdk-11.0.1/bin&#x27; &gt;&gt; /etc/profile</span><br><span class="line">- name: 生效环境变量</span><br><span class="line">  shell: source /etc/profile</span><br><span class="line">- name: 创建es用户</span><br><span class="line">  user:</span><br><span class="line">    name: es</span><br><span class="line">- name: 传输本地包到目标并解压</span><br><span class="line">  unarchive:</span><br><span class="line">    src: elasticsearch-7.7.1-linux-x86_64.tar.gz</span><br><span class="line">    dest: /opt</span><br><span class="line">- name: mkdir data</span><br><span class="line">  shell: mkdir /opt/elasticsearch-7.7.1/data</span><br><span class="line">- name: 更改用户组</span><br><span class="line">  shell: chown -R es:es /opt/elasticsearch-7.7.1</span><br><span class="line">- name: 传输配置文件</span><br><span class="line">  template:</span><br><span class="line">    src: elasticsearch.j2</span><br><span class="line">    dest: /opt/elasticsearch-7.7.1/config/elasticsearch.yml</span><br><span class="line">    owner: es</span><br><span class="line">    group: es</span><br><span class="line">- name: 传输系统配置文件</span><br><span class="line">  copy: </span><br><span class="line">    src: limits.conf</span><br><span class="line">    dest: /etc/security/limits.conf</span><br><span class="line">- name: 传输系统配置文件</span><br><span class="line">  copy:</span><br><span class="line">    src: sysctl.conf</span><br><span class="line">    dest: /etc/sysctl.conf</span><br><span class="line">- name: 加载sysctl</span><br><span class="line">  shell: sysctl -p</span><br><span class="line">- name: 启动服务</span><br><span class="line">  become: yes</span><br><span class="line">  become_user: es</span><br><span class="line">  command: </span><br><span class="line">    argv:</span><br><span class="line">      - nohup</span><br><span class="line">      - /opt/elasticsearch-7.7.1/bin/elasticsearch</span><br><span class="line">      - -d</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver elk-ansible]# cat roles/es/templates/elasticsearch.j2 </span><br><span class="line">cluster.name: elk-cluster</span><br><span class="line">node.name: &#123;&#123;node_name&#125;&#125;</span><br><span class="line">#node.attr.rack: r1</span><br><span class="line">path.data: /opt/elasticsearch-7.7.1/data</span><br><span class="line">path.logs: /opt/elasticsearch-7.7.1/logs</span><br><span class="line">#bootstrap.memory_lock: true</span><br><span class="line">network.host: 0.0.0.0</span><br><span class="line">http.port: 9200</span><br><span class="line">discovery.seed_hosts: [&quot;192.168.20.181&quot;, &quot;192.168.20.167&quot;, &quot;192.168.20.188&quot;]</span><br><span class="line">cluster.initial_master_nodes: [&quot;node-1&quot;]</span><br><span class="line">#gateway.recover_after_nodes: 3</span><br><span class="line">#action.destructive_requires_name: true</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver elk-ansible]# cat roles/es/files/limits.conf </span><br><span class="line"></span><br><span class="line">*	soft	 nofile	     65535</span><br><span class="line">*	hard	 nofile	     65535</span><br><span class="line">*	soft	 nproc	     4096</span><br><span class="line">*	hard	 nproc	     4096</span><br><span class="line">[root@jumpserver elk-ansible]# cat roles/es/files/sysctl.conf </span><br><span class="line">vm.max_map_count=655360</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver elk-ansible]# cat roles/kibana/tasks/main.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># tasks file for roles/kibana</span><br><span class="line"></span><br><span class="line">- name: 传输本地文件到目标</span><br><span class="line">  unarchive:</span><br><span class="line">    src: kibana-7.7.1-linux-x86_64.tar.gz</span><br><span class="line">    dest: /opt/</span><br><span class="line">    owner: es</span><br><span class="line">    group: es</span><br><span class="line">- name: 创建日志与pid目录</span><br><span class="line">  shell: mkdir -p /var/log/kibana /run/kibana</span><br><span class="line">- name: 更改用户组</span><br><span class="line">  shell: chown -R es:es /var/log/kibana /run/kibana</span><br><span class="line">- name: 传输配置文件</span><br><span class="line">  copy:</span><br><span class="line">    src: kibana.yml</span><br><span class="line">    dest: /opt/kibana-7.7.1-linux-x86_64/config/kibana.yml</span><br><span class="line">- name: chown</span><br><span class="line">  shell: chown -R es:es /opt/kibana-7.7.1-linux-x86_64/config/kibana.yml</span><br><span class="line">- name: 启动服务</span><br><span class="line">  become: yes</span><br><span class="line">  become_user: es</span><br><span class="line">  shell: nohup /opt/kibana-7.7.1-linux-x86_64/bin/kibana &amp;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver elk-ansible]# cat roles/kibana/files/kibana.yml </span><br><span class="line">server.port: 5601</span><br><span class="line">server.host: &quot;0.0.0.0&quot;</span><br><span class="line">elasticsearch.hosts: [&quot;http://192.168.20.181:9200&quot;,&quot;http://192.168.20.167:9200&quot;,&quot;http://192.168.20.188:9200&quot;]</span><br><span class="line">i18n.locale: &quot;zh-CN&quot;</span><br><span class="line">pid.file: /run/kibana/kibana.pid</span><br><span class="line">logging.dest: /var/log/kibana/kibana.log</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver elk-ansible]# cat roles/log/tasks/main.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"># tasks file for roles/log</span><br><span class="line"></span><br><span class="line">- name: 创建日志目录</span><br><span class="line">  shell: mkdir /var/log/logstash</span><br><span class="line">- name: 更改权限</span><br><span class="line">  shell: chown -R es:es /var/log/logstash</span><br><span class="line">- name: 传输本地文件到目标</span><br><span class="line">  unarchive:</span><br><span class="line">    src: logstash-7.7.1.tar.gz</span><br><span class="line">    dest: /opt</span><br><span class="line">    owner: es</span><br><span class="line">    group: es</span><br><span class="line">- name: 传输配置文件</span><br><span class="line">  template:</span><br><span class="line">    src: logstash.yml</span><br><span class="line">    dest: /opt/logstash-7.7.1/config/logstash.yml</span><br><span class="line">- name: 传输管道配置文件</span><br><span class="line">  template:</span><br><span class="line">    src: logstash.conf</span><br><span class="line">    dest: /opt/logstash-7.7.1/config/logstash-sample.conf</span><br><span class="line">- name: 更改权限</span><br><span class="line">  shell: chown -R es:es /opt/logstash-7.7.1 </span><br><span class="line">- name: 启动服务</span><br><span class="line">  become: yes</span><br><span class="line">  become_user: es</span><br><span class="line">  shell: source /etc/profile &amp;&amp; sh /opt/logstash-7.7.1/bin/logstash -f /opt/logstash-7.7.1/config/logstash-sample.conf &amp;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@jumpserver elk-ansible]# cat roles/log/templates/logstash.yml </span><br><span class="line">http.host: &quot;0.0.0.0&quot;</span><br><span class="line">path.logs: /var/log/logstash/</span><br><span class="line">[root@jumpserver elk-ansible]# cat roles/log/templates/logstash.conf </span><br><span class="line">input &#123;</span><br><span class="line">  file &#123;</span><br><span class="line">    path =&gt; &quot;/var/log/yum.log&quot;</span><br><span class="line">    type =&gt; &quot;yum_log&quot;</span><br><span class="line">    start_position =&gt; &quot;beginning&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#标准输出到elasticsearch中</span><br><span class="line">output &#123;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt;  [&quot;192.168.20.181:9200&quot;,&quot;192.168.20.167:9200&quot;,&quot;192.168.20.188:9200&quot;] </span><br><span class="line">    index =&gt; &quot;%&#123;[@metadata][beat]&#125;-%&#123;[@metadata][version]&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">    #user =&gt; &quot;elastic&quot;</span><br><span class="line">    #password =&gt; &quot;changeme&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="【题目-3】Python-运维开发：基于-OpenStack-Restful-API-实现镜像上传-1-分"><a href="#【题目-3】Python-运维开发：基于-OpenStack-Restful-API-实现镜像上传-1-分" class="headerlink" title="【题目 3】Python 运维开发：基于 OpenStack Restful API 实现镜像上传[1 分]"></a>【题目 3】Python 运维开发：基于 OpenStack Restful API 实现镜像上传[1 分]</h4><p>使用 OpenStack all-in-one 镜像，创建 OpenStack Python 运维开发环境。云主机的用户&#x2F;密码为：“root&#x2F;Abc@1234”，OpenStack 的域名&#x2F;账号&#x2F;密码为：“demo&#x2F;admin&#x2F;000000”。提示说明：python 脚本文件头建议加入“#encoding:utf-8”避免编码错误；测试脚本代码用 python3 命令执行与测试。在 controller 节点的&#x2F;root 目录下创建 api_image_manager.py 脚本，编写 python 代码对接OpenStack API，完成镜像的创建与上传。创建之前查询是否存在“同名镜像”，如果存在先删除该镜像。<br>（1）创建镜像：要求在 OpenStack 私有云平台中上传镜像 cirros-0.3.4-x86_64-disk.img，名字为 cirros001，disk_format 为 qcow2，container_format 为 bare。<br>（2）查询镜像：查询 cirros001 的详细信息，并以 json 格式文本输出到控制台。完成后提交 OpenStack Python 运维开发环境 Controller 节点的 IP 地址，用户名和密码提交。<br>1.执行 api_image_manager.py 脚本,成功创建镜像，计 0.5 分<br>2.检查镜像状态正确，计 0.5 分</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat api_image_manager.py </span><br><span class="line">#coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">url = &quot;http://192.168.10.10&quot;</span><br><span class="line">body = &#123;</span><br><span class="line">    &quot;auth&quot;: &#123;</span><br><span class="line">        &quot;identity&quot;: &#123;</span><br><span class="line">            &quot;methods&quot;: [&quot;password&quot;],</span><br><span class="line">            &quot;password&quot;: &#123;</span><br><span class="line">                &quot;user&quot;: &#123;</span><br><span class="line">                    &quot;domain&quot;: &#123;</span><br><span class="line">                        &quot;name&quot;: &quot;demo&quot;</span><br><span class="line">                    &#125;,</span><br><span class="line">                    &quot;name&quot;: &quot;admin&quot;,</span><br><span class="line">                    &quot;password&quot;: &quot;000000&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">    &quot;scope&quot;: &#123;</span><br><span class="line">        &quot;project&quot;: &#123;</span><br><span class="line">            &quot;domain&quot;: &#123;</span><br><span class="line">                &quot;name&quot;: &quot;demo&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;name&quot;: &quot;admin&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &quot;Content-Type&quot;: &quot;application/json&quot;</span><br><span class="line">&#125;</span><br><span class="line">def get_token():</span><br><span class="line">    tokenurl = url+&quot;:5000/v3/auth/tokens&quot;</span><br><span class="line">    re = requests.post(url=tokenurl, headers=headers,data=json.dumps(body)).headers[&quot;X-Subject-Token&quot;]</span><br><span class="line">    return re</span><br><span class="line"></span><br><span class="line">def delete_image():</span><br><span class="line">   glanceurl = url+&quot;:9292/v2/images&quot;</span><br><span class="line">   headers[&quot;X-Auth-Token&quot;]=get_token()</span><br><span class="line">   re = requests.get(url=glanceurl,headers=headers).json()</span><br><span class="line">   for i in re[&#x27;images&#x27;]:</span><br><span class="line">       if i[&#x27;name&#x27;] == &quot;cirros001&quot;:</span><br><span class="line">           requests.delete(&quot;http://192.168.10.10:9292/v2/images/&quot;+i[&#x27;id&#x27;],headers=headers)</span><br><span class="line"></span><br><span class="line">def create_image():</span><br><span class="line">    glanceurl = url + &quot;:9292/v2/images&quot;</span><br><span class="line">    headers[&quot;X-Auth-Token&quot;] = get_token()</span><br><span class="line">    body1 = &#123;</span><br><span class="line">        &quot;name&quot;: &quot;cirros001&quot;,</span><br><span class="line">        &quot;disk_format&quot;: &quot;qcow2&quot;,</span><br><span class="line">        &quot;container_format&quot;: &quot;bare&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    re = requests.post(url=glanceurl,headers=headers,data=json.dumps(body1)).json()</span><br><span class="line">    id=re[&#x27;id&#x27;]</span><br><span class="line">    return id</span><br><span class="line"></span><br><span class="line">def put_image():</span><br><span class="line">    glanceurl = url + &quot;:9292/v2/images&quot;</span><br><span class="line">    id = create_image()</span><br><span class="line">    headers[&quot;Content-Type&quot;] = &quot;application/octet-stream&quot;</span><br><span class="line">    a = open(&quot;cirros-0.3.4-x86_64-disk.img&quot;,&quot;rb&quot;).read()</span><br><span class="line">    requests.put(&quot;http://192.168.10.10:9292/v2/images/&#123;&#125;/file&quot;.format(id),data=a,headers=headers)</span><br><span class="line">    return id</span><br><span class="line"></span><br><span class="line">def show_image():</span><br><span class="line">    glanceurl=url+&quot;:9292/v2/images&quot;</span><br><span class="line">    headers[&quot;X-Auth-Token&quot;]=get_token()</span><br><span class="line">    id = put_image()</span><br><span class="line">    re = requests.get(url=glanceurl,headers=headers).json()</span><br><span class="line">    for i in re[&#x27;images&#x27;]:</span><br><span class="line">        if i[&#x27;id&#x27;] == id:</span><br><span class="line">            print(i)</span><br><span class="line"></span><br><span class="line">delete_image()</span><br><span class="line">show_image()</span><br></pre></td></tr></table></figure>

<h4 id="【题目-4】Python-运维开发：基于-Openstack-Python-SDK-实现云主机创建-1-分"><a href="#【题目-4】Python-运维开发：基于-Openstack-Python-SDK-实现云主机创建-1-分" class="headerlink" title="【题目 4】Python 运维开发：基于 Openstack Python SDK 实现云主机创建[1 分]"></a>【题目 4】Python 运维开发：基于 Openstack Python SDK 实现云主机创建[1 分]</h4><p>使 用 已 建 好 的 OpenStack Python 运 维 开 发 环 境 ， 在 &#x2F;root 目 录 下 创 建sdk_server_manager.py 脚本，使用 python-openstacksdk Python 模块，完成云主机的创建和查询。创建之前查询是否存在“同名云主机”，如果存在先删除该镜像。<br>（1）创建 1 台云主机：云主机信息如下：云主机名称如下：server001镜像文件：cirros-0.3.4-x86_64-disk.img<br>云主机类型：m1.tiny网络等必要信</p>
<p>息自己补充。<br>（2）查询云主机：查询云主机 server001 的详细信息，并以 json 格式文本输出到控制台。完成后提交 OpenStack Python 运维开发环境 Controller 节点的 IP 地址，用户名和密码提交。</p>
<p>1.执行 sdk_server_manager.py 脚本,成功创建云主机，计 0.5 分<br>2.检查创建的云主机状态正确，计 0.5 分</p>
<p>sdk_server_manager.py：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line">import logging</span><br><span class="line">import openstack</span><br><span class="line"></span><br><span class="line">def create_connection():</span><br><span class="line">  connect=openstack.connect(</span><br><span class="line">    auth_url=&quot;http://192.168.20.146:5000/v3&quot;,</span><br><span class="line">    user_domain_name=&quot;demo&quot;,</span><br><span class="line">    username=&quot;admin&quot;,</span><br><span class="line">    password=&quot;000000&quot;,</span><br><span class="line">  )</span><br><span class="line">  return connect</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def delete_server():</span><br><span class="line">  connect = create_connection()</span><br><span class="line">  items = connect.compute.servers()</span><br><span class="line">  for i in items:</span><br><span class="line">      if i[&#x27;name&#x27;] == &quot;server001&quot;:</span><br><span class="line">         server = connect.compute.find_server(&quot;server001&quot;)</span><br><span class="line">         connect.compute.delete_server(server)</span><br><span class="line"></span><br><span class="line">def create_server():</span><br><span class="line">  connect = create_connection()</span><br><span class="line">  image = connect.compute.find_image(&quot;cirros&quot;)</span><br><span class="line">  flavor = connect.compute.find_flavor(&quot;m1.small&quot;)</span><br><span class="line">  network = connect.network.find_network(&quot;int-net&quot;)</span><br><span class="line">  server = connect.compute.create_server(name=&quot;server001&quot;,image_id=image.id,flavor_id=flavor.id,networks=[&#123;&quot;uuid&quot;: network.id&#125;])</span><br><span class="line">  id = server[&#x27;id&#x27;]</span><br><span class="line">  return id</span><br><span class="line">  </span><br><span class="line">def get_server():</span><br><span class="line">  connect = create_connection()</span><br><span class="line">  server_id = create_server()</span><br><span class="line">  server = connect.compute.find_server(server_id)</span><br><span class="line">  a = json.dumps(server,idnent=2)</span><br><span class="line">  print(a)</span><br><span class="line">  </span><br><span class="line">delete_server()</span><br><span class="line"></span><br><span class="line">get_server()</span><br></pre></td></tr></table></figure>

<h4 id="【题目-5】Python-运维开发：云主机类型管理的命令行工具开发-2-分"><a href="#【题目-5】Python-运维开发：云主机类型管理的命令行工具开发-2-分" class="headerlink" title="【题目 5】Python 运维开发：云主机类型管理的命令行工具开发[2 分]"></a>【题目 5】Python 运维开发：云主机类型管理的命令行工具开发[2 分]</h4><p>使用已建好的 OpenStack Python 运维开发环境，在&#x2F;root 目录下创建 flavor_manager.py脚本，完成云主机类型的管理，flavor_manager.py 程序支持命令行参数执行。提示说明：Python 标准库 argparse 模块，可以提供命令行参数的解析。要求如下：<br>（1）程序支持根据命令行参数，创建 1 个多云主机类型。返回 response。<br>位置参数“create”，表示创建；<br>参数“-n”支持指定 flavor 名称，数据类型为字符串类型；<br>参数“-m”支持指定内存大小，数据类型为 int，单位 M；<br>参数“-v”支持指定虚拟 cpu 个数，数据类型为 int；<br>参数“-d”支持磁盘大小，内存大小类型为 int，单位 G；<br>参数“-id”支持指定 ID，类型为字符串。<br>参考运行实例：</p>
<p>python3 flavor_manager.py create -n flavor_small -m 1024 -v 1 -d 10 -id 100000<br>（2）程序支持查询目前 admin 账号下所有的云主机类型。<br>位置参数“getall”，表示查询所有云主机类型；<br>查询结果，以 json 格式输出到控制台。<br>参考执行实例如下：<br>python3 flavor_manager.py getall<br>（3）支持查询给定具体名称的云主机类型查询。<br>位置参数“get”，表示查询 1 个云主机类型；<br>参数“-id”支持指定 ID 查询，类型为 string。<br>控制台以 json 格式输出创建结果。<br>参考执行实例如下：<br>python3 flavor_manager.py get -id 100000<br>（4）支持删除指定的 ID 云主机类型。<br>位置参数“delete”，表示删除一个云主机类型；<br>参数“-id”支持指定 ID 查询，返回 response，控制台输出 response。<br>参考执行实例如下：<br>python3 flavor_manager.py delete -id 100001<br>1.执行 flavor_manager.py 脚本，指定 create 和配置参数，成功创建 1 个云主机类型，计 0.5<br>分；<br>2.执行 flavor_manager.py 脚本，指定 getall 参数，成功查询所有云主机类型，计 0.5 分；<br>3.执行 flavor_manager.py 脚本，指定 get 和配置参数，成功查询具体名称的云主机类型，计<br>0.5 分；<br>4.执行 flavor_manager.py 脚本，指定 delete 和配置参数，成功删除指定 ID 云主机类型，计<br>0.5 分。</p>
<h4 id="【题目-6】Python-运维开发：用户管理的命令行工具开发-2-分"><a href="#【题目-6】Python-运维开发：用户管理的命令行工具开发-2-分" class="headerlink" title="【题目 6】Python 运维开发：用户管理的命令行工具开发[2 分]"></a>【题目 6】Python 运维开发：用户管理的命令行工具开发[2 分]</h4><p>使用已建好的 OpenStack Python 运维开发环境，在&#x2F;root 目录下创建 user_manager.py 脚本，完成用户管理功能开发，user_manager.py 程序支持命令行带参数执行。提示说明：Python 标准库 argparse 模块，可以提供命令行参数的解析。<br>（1）程序支持根据命令行参数，创建 1 个用户。<br>位置参数“create”，表示创建；</p>
<p>参数“-i 或–input”，格式为 json 格式文本用户数据。<br>查询结果，以 json 格式输出到控制台。<br>参考执行实例如下：<br>python3 user_manager.py create –input ‘{ “name”: “user01”, “password”: “000000”,<br>“description”: “description” } ‘<br>（2）支持查询给定具体名称的用户查询。<br>位置参数“get”，表示查询 1 个用户；<br>参数“-n 或 –name”支持指定名称查询，类型为 string。<br>参数“-o 或 output”支持查询该用户信息输出到文件，格式为 json 格式。<br>参考执行实例如下：<br>python3 user_manager.py get –name user01-o user.json<br>（3）程序支持查询目前 admin 账号下所有的用户。<br>位置参数“getall”，表示查询所有用户；<br>参数“-o 或–output”支持输出到文件，格式为 yaml 格式。<br>参考执行实例如下：<br>python3 user_manager.py getall -o openstack_all_user.yaml<br>（4）支持删除指定的名称的用户。<br>位置参数“delete”，表示删除一个用户；返回 response，通过控制台输出。<br>参数“-n 或–name”支持指定名称查询，类型为 string。<br>参考执行实例如下：<br>python3 user_manager.py delete -name user01<br>1.执行 user_manager.py 脚本，指定 create 和配置参数，成功创建 1 个用户，计 0.5 分；<br>2.执行 user_manager.py 脚本，指定 get 和配置参数，成功查询具体名称的用户，计 0.5 分；<br>3.执行 user_manager.py 脚本，指定 getall 和配置参数，成功查询 admin 账号下的所有用户，<br>计 0.5 分；<br>4.执行 user_manager.py 脚本，指定 delete 和配置参数，成功删除指定名称的用户，计 0.5 分。</p>
]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s学习日记day1</title>
    <url>/2022/05/16/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day1/</url>
    <content><![CDATA[<h1 id="kubernetes环境资源搭建"><a href="#kubernetes环境资源搭建" class="headerlink" title="kubernetes环境资源搭建"></a>kubernetes环境资源搭建<span id="more"></span></h1><h2 id="1-环境规划"><a href="#1-环境规划" class="headerlink" title="1 环境规划"></a>1 环境规划</h2><h3 id="1-1-集群类型"><a href="#1-1-集群类型" class="headerlink" title="1.1 集群类型"></a>1.1 集群类型</h3><p>●Kubernetes集群大致分为两类：一主多从和多主多从。</p>
<p>●一主多从：一个Master节点和多台Node节点，搭建简单，但是有单机故障风险，适合用于测试环境。</p>
<p>●多主多从：多台Master和多台Node节点，搭建麻烦，安全性高，适合用于生产环境。</p>
<p><strong>为了测试方便，本次搭建的是一主多从类型的集群。</strong></p>
<h3 id="1-2-安装方式"><a href="#1-2-安装方式" class="headerlink" title="1.2 安装方式"></a>1.2 安装方式</h3><p>●kubernetes有多种部署方式，目前主流的方式有kubeadm、minikube、二进制包。</p>
<p>●① minikube：一个用于快速搭建单节点的kubernetes工具。<br>●② kubeadm：一个用于快速搭建kubernetes集群的工具。<br>●③ 二进制包：从官网上下载每个组件的二进制包，依次去安装，此方式对于理解kubernetes组件更加有效。</p>
<p>●我们需要安装kubernetes的集群环境，但是又不想过于麻烦，所以选择kubeadm方式。</p>
<h3 id="1-3-主机规划"><a href="#1-3-主机规划" class="headerlink" title="1.3 主机规划"></a>1.3 主机规划</h3><table>
<thead>
<tr>
<th>角色</th>
<th>IP地址</th>
<th>操作系统</th>
<th>配置</th>
</tr>
</thead>
<tbody><tr>
<td>master</td>
<td>192.168.20.119</td>
<td>CentOS7.5，基础设施服务器</td>
<td>8核CPU，12G内存，100G硬盘</td>
</tr>
<tr>
<td>node1</td>
<td>192.168.20.115</td>
<td>CentOS7.5，基础设施服务器</td>
<td>4核CPU，8G内存，100G硬盘</td>
</tr>
<tr>
<td>node2</td>
<td>192.168.20.124</td>
<td>CentOS7.5，基础设施服务器</td>
<td>4核CPU，8G内存，100G硬盘</td>
</tr>
</tbody></table>
<h2 id="2-环境搭建"><a href="#2-环境搭建" class="headerlink" title="2 环境搭建"></a><strong>2 环境搭建</strong></h2><h3 id="2-1-前言"><a href="#2-1-前言" class="headerlink" title="2.1 前言"></a>2.1 前言</h3><p>本次环境搭建需要三台CentOS服务器（一主二从），然后在每台服务器中分别安装Docker（18.06.3）、kubeadm（1.18.0）、kubectl（1.18.0）和kubelet（1.18.0）。</p>
<p><strong>没有特殊说明，就是三台机器都需要执行。</strong></p>
<h3 id="2-2-环境初始化"><a href="#2-2-环境初始化" class="headerlink" title="2.2 环境初始化"></a><strong>2.2 环境初始化</strong></h3><h4 id="2-2-1-检查操作系统的版本"><a href="#2-2-1-检查操作系统的版本" class="headerlink" title="2.2.1 检查操作系统的版本"></a>2.2.1 检查操作系统的版本</h4><p>●检查操作系统的版本（要求操作系统的版本至少在7.5以上）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat /etc/redhat-release </span><br><span class="line">CentOS Linux release 7.5.1804 (Core)</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-关闭防火墙和禁止防火墙开机启动"><a href="#2-2-2-关闭防火墙和禁止防火墙开机启动" class="headerlink" title="2.2.2 关闭防火墙和禁止防火墙开机启动"></a><strong>2.2.2 关闭防火墙和禁止防火墙开机启动</strong></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# systemctl stop firewalld</span><br><span class="line">Failed to stop firewalld.service: Unit firewalld.service not loaded.</span><br><span class="line">[root@master ~]# systemctl disable firewalld</span><br><span class="line">Failed to execute operation: No such file or directory</span><br><span class="line"></span><br><span class="line">[root@master ~]# systemctl stop iptables</span><br><span class="line">Failed to stop iptables.service: Unit iptables.service not loaded.</span><br><span class="line">[root@master ~]# systemctl disable iptables</span><br><span class="line">Failed to execute operation: No such file or directory</span><br></pre></td></tr></table></figure>

<p>我这里是因为没有防火墙</p>
<h4 id="2-2-3-设置主机名"><a href="#2-2-3-设置主机名" class="headerlink" title="2.2.3 设置主机名"></a><strong>2.2.3 设置主机名</strong></h4><p><strong>master：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# hostnamectl set-hostname master</span><br><span class="line">[root@master ~]# login</span><br></pre></td></tr></table></figure>

<p><strong>node1：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-1 ~]# hostnamectl set-hostname node1</span><br><span class="line">[root@node-1 ~]# login</span><br></pre></td></tr></table></figure>

<p><strong>node2：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-2 ~]# hostnamectl set-hostname node2</span><br><span class="line">[root@node-2 ~]# login</span><br></pre></td></tr></table></figure>

<h4 id="2-2-4-主机名解析"><a href="#2-2-4-主机名解析" class="headerlink" title="2.2.4 主机名解析"></a>2.2.4 主机名解析</h4><p>为了方便后面集群节点间的直接调用，需要配置一下主机名解析，企业中推荐使用内部的DNS服务器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.20.119  master</span><br><span class="line">192.168.20.115  node1</span><br><span class="line">192.168.20.124  node2</span><br></pre></td></tr></table></figure>

<p><strong>三个节点都是一样的</strong></p>
<h4 id="2-2-5-源配置"><a href="#2-2-5-源配置" class="headerlink" title="2.2.5 源配置"></a><strong>2.2.5 源配置</strong></h4><p><strong>master节点：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# rm -rf /etc/yum.repos.d/*</span><br><span class="line">[root@master ~]# cat /etc/yum.repos.d/local.repo </span><br><span class="line">[k8s]</span><br><span class="line">name=k8s</span><br><span class="line">baseurl=file:///opt/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=http://172.19.25.11/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[root@master ~]# yum install -y vsftpd</span><br><span class="line">[root@master ~]# echo anon_root=/opt/ &gt;&gt; /etc/vsftpd/vsftpd.conf</span><br><span class="line">[root@master ~]# systemctl restart vsftpd</span><br><span class="line">[root@master ~]# systemctl enable vsftpd</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/vsftpd.service to /usr/lib/systemd/system/vsftpd.service.</span><br></pre></td></tr></table></figure>

<p><strong>node节点：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# rm -rf /etc/yum.repos.d/*</span><br><span class="line">[root@node1 ~]# cat /etc/yum.repos.d/local.repo </span><br><span class="line">[k8s]</span><br><span class="line">name=k8s</span><br><span class="line">baseurl=ftp://master/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=http://172.19.25.11/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[root@node2 ~]# rm -rf /etc/yum.repos.d/*</span><br><span class="line">[root@node2 ~]# cat /etc/yum.repos.d/local.repo </span><br><span class="line">[k8s]</span><br><span class="line">name=k8s</span><br><span class="line">baseurl=ftp://master/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=http://172.19.25.11/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<h4 id="2-2-6-时间同步"><a href="#2-2-6-时间同步" class="headerlink" title="2.2.6 时间同步"></a>2.2.6 时间同步</h4><p><strong>master节点：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# yum install -y chrony</span><br><span class="line">[root@master ~]# sed -i &#x27;3,6s/^/#/g&#x27; /etc/chrony.conf</span><br><span class="line">[root@master ~]# sed -i &quot;7s|^|server master iburst|g&quot; /etc/chrony.conf</span><br><span class="line">[root@master ~]# echo &quot;allow all&quot; &gt;&gt; /etc/chrony.conf</span><br><span class="line">[root@master ~]# echo &quot;local stratum 10&quot; &gt;&gt; /etc/chrony.conf</span><br><span class="line">[root@master ~]# systemctl restart chronyd</span><br><span class="line">[root@master ~]# systemctl enable chronyd</span><br><span class="line">[root@master ~]# timedatectl set-ntp true</span><br><span class="line">[root@master ~]# systemctl restart chronyd</span><br></pre></td></tr></table></figure>

<p><strong>node节点：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# sed -i &#x27;3,6s/^/#/g&#x27; /etc/chrony.conf</span><br><span class="line">[root@node1 ~]# sed -i &quot;7s|^|server master iburst|g&quot; /etc/chrony.conf</span><br><span class="line">[root@node1 ~]# systemctl restart chronyd</span><br><span class="line">[root@node1 ~]# systemctl enable chronyd</span><br><span class="line">[root@node1 ~]# timedatectl set-ntp true</span><br><span class="line">[root@node1 ~]# systemctl restart chronyd</span><br><span class="line">[root@node1 ~]# chronyc sources</span><br><span class="line">210 Number of sources = 1</span><br><span class="line">MS Name/IP address         Stratum Poll Reach LastRx Last sample               </span><br><span class="line">===============================================================================</span><br><span class="line"></span><br><span class="line">^* master                       11   6    17     2    +20us[  +18us] +/-   16ms</span><br></pre></td></tr></table></figure>

<h4 id="2-2-7-关闭selinux"><a href="#2-2-7-关闭selinux" class="headerlink" title="2.2.7 关闭selinux"></a>2.2.7 关闭selinux</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master]# sed -i &#x27;s/enforcing/disabled/&#x27; /etc/selinux/config</span><br></pre></td></tr></table></figure>

<h4 id="2-2-8-关闭swap分区"><a href="#2-2-8-关闭swap分区" class="headerlink" title="2.2.8 关闭swap分区"></a><strong>2.2.8 关闭swap分区</strong></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# sed -ri &#x27;s/.*swap.*/#&amp;/&#x27; /etc/fstab</span><br></pre></td></tr></table></figure>

<h4 id="2-2-9-将桥接的IPv4流量传递到iptables的链"><a href="#2-2-9-将桥接的IPv4流量传递到iptables的链" class="headerlink" title="2.2.9 将桥接的IPv4流量传递到iptables的链"></a><strong>2.2.9 将桥接的IPv4流量传递到iptables的链</strong></h4><p><strong>master节点：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master opt]# modprobe br_netfilter</span><br><span class="line">[root@master opt]# echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">[root@master opt]# echo &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">[root@master opt]# echo &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">[root@master opt]# sysctl -p</span><br></pre></td></tr></table></figure>

<p><strong>node节点：</strong></p>
<blockquote>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 opt]# modprobe br_netfilter</span><br><span class="line">[root@node1 opt]# cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF</span><br><span class="line">&gt; net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">&gt; net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">&gt; EOF</span><br><span class="line">[root@node1 opt]# sysctl -p /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line"></span><br><span class="line">[root@node2 ~]# cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF</span><br><span class="line">&gt; net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">&gt; net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">&gt; EOF</span><br><span class="line">[root@node2 ~]# sysctl -p /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br></pre></td></tr></table></figure>


</blockquote>
<h4 id="2-2-10-开启ipvs"><a href="#2-2-10-开启ipvs" class="headerlink" title="2.2.10 开启ipvs"></a><strong>2.2.10 开启ipvs</strong></h4><p><strong>三个节点都要</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master opt]# yum -y install ipset ipvsadm</span><br><span class="line">[root@master opt]# cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF</span><br><span class="line">&gt; #!/bin/bash</span><br><span class="line">&gt; modprobe -- ip_vs</span><br><span class="line">&gt; modprobe -- ip_vs_rr</span><br><span class="line">&gt; modprobe -- ip_vs_wrr</span><br><span class="line">&gt; modprobe -- ip_vs_sh</span><br><span class="line">&gt; modprobe -- nf_conntrack_ipv4</span><br><span class="line">&gt; EOF</span><br><span class="line">[root@master opt]# chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.modules &amp;&amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4</span><br><span class="line">nf_conntrack_ipv4      15053  0 </span><br><span class="line">nf_defrag_ipv4         12729  1 nf_conntrack_ipv4</span><br><span class="line">ip_vs_sh               12688  0 </span><br><span class="line">ip_vs_wrr              12697  0 </span><br><span class="line">ip_vs_rr               12600  0 </span><br><span class="line">ip_vs                 141432  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr</span><br><span class="line">nf_conntrack          133053  2 ip_vs,nf_conntrack_ipv4</span><br><span class="line">libcrc32c              12644  3 xfs,ip_vs,nf_conntrack</span><br></pre></td></tr></table></figure>

<h3 id="2-3-每个节点安装Docker、kubeadm、kubelet和kubectl"><a href="#2-3-每个节点安装Docker、kubeadm、kubelet和kubectl" class="headerlink" title="2.3 每个节点安装Docker、kubeadm、kubelet和kubectl"></a><strong>2.3 每个节点安装Docker、kubeadm、kubelet和kubectl</strong></h3><h4 id="2-3-1-安装docker-ce"><a href="#2-3-1-安装docker-ce" class="headerlink" title="2.3.1 安装docker-ce"></a>2.3.1 安装docker-ce</h4><p><strong>三节点</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master opt]# yum install -y yum-utils device-mapper-p* lvm2</span><br><span class="line">[root@master opt]# yum install -y docker-ce</span><br><span class="line">[root@master opt]# systemctl enable docker &amp;&amp; systemctl start docker</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/docker.service to /usr/lib/systemd/system/docker.service.</span><br><span class="line">[root@master opt]# tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line"> &#123;</span><br><span class="line">   &quot;insecure-registries&quot; : [&quot;0.0.0.0/0&quot;],</span><br><span class="line"> &quot;registry-mirrors&quot;: [&quot;https://5twf62k1.mirror.aliyuncs.com&quot;],</span><br><span class="line">   &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]</span><br><span class="line"> &#125;</span><br><span class="line"> EOF</span><br><span class="line">[root@master opt]# systemctl restart docker</span><br></pre></td></tr></table></figure>

<h4 id="2-3-2安装kubeadm、kubelet和kubectl"><a href="#2-3-2安装kubeadm、kubelet和kubectl" class="headerlink" title="2.3.2安装kubeadm、kubelet和kubectl"></a>2.3.2安装kubeadm、kubelet和kubectl</h4><p><strong>三节点</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master opt]# yum install -y kubelet-1.18.1 kubeadm-1.18.1 kubectl-1.18.1</span><br><span class="line">[root@master opt]# systemctl enable kubelet</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.</span><br><span class="line">[root@master opt]# systemctl start kubelet</span><br></pre></td></tr></table></figure>

<h4 id="2-3-3-master节点部署"><a href="#2-3-3-master节点部署" class="headerlink" title="2.3.3 master节点部署"></a>2.3.3 master节点部署</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master opt]# kubeadm init --kubernetes-version=1.18.1 --apiserver-advertise-address=$IP --image-repository 192.168.20.119/library --pod-network-cidr=10.244.0.0/16</span><br><span class="line">W0518 05:17:21.372074    3287 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]</span><br><span class="line">[init] Using Kubernetes version: v1.18.1</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#x27;kubeadm config images pull&#x27;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[certs] Using certificateDir folder &quot;/etc/kubernetes/pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.20.119]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd/server&quot; certificate and key</span><br><span class="line">[certs] etcd/server serving cert is signed for DNS names [master localhost] and IPs [192.168.20.119 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/peer&quot; certificate and key</span><br><span class="line">[certs] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [192.168.20.119 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd/healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;/etc/kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0518 05:17:30.213101    3287 manifests.go:225] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0518 05:17:30.216982    3287 manifests.go:225] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;/etc/kubernetes/manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;/etc/kubernetes/manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 21.516345 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.18&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node master as control-plane by adding the label &quot;node-role.kubernetes.io/master=&#x27;&#x27;&quot;</span><br><span class="line">[mark-control-plane] Marking the node master as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: qkrll1.aajjr3v4zcps0a94</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;/etc/kubernetes/kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME/.kube</span><br><span class="line">  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https://kubernetes.io/docs/concepts/cluster-administration/addons/</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 192.168.20.119:6443 --token qkrll1.aajjr3v4zcps0a94 \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:eca13ad31879f9f8cca8c719b685f239a06d2e1450e49380f8f3eec5121db792 </span><br><span class="line">[root@master opt]# mkdir -p /root/.kube</span><br><span class="line">[root@master opt]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span><br><span class="line">[root@master opt]# sudo chown $(id -u):$(id -g) $HOME/.kube/config</span><br><span class="line">[root@master opt]# kubectl get pod -n kube-system -owide</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">coredns-6fcfc67db4-6tdxr         0/1     Pending   0          76s   &lt;none&gt;           &lt;none&gt;   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">coredns-6fcfc67db4-b6m9j         0/1     Pending   0          76s   &lt;none&gt;           &lt;none&gt;   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">etcd-master                      1/1     Running   0          89s   192.168.20.119   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master            1/1     Running   0          89s   192.168.20.119   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-controller-manager-master   1/1     Running   0          89s   192.168.20.119   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-proxy-d7vxn                 1/1     Running   0          76s   192.168.20.119   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-scheduler-master            1/1     Running   0          89s   192.168.20.119   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@master opt]# sed -i &quot;s/quay.io\/coreos/$IP\/library/g&quot; /opt/yaml/flannel/kube-flannel.yaml</span><br><span class="line">[root@master opt]# kubectl apply -f /opt/yaml/flannel/kube-flannel.yaml </span><br><span class="line">podsecuritypolicy.policy/psp.flannel.unprivileged created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.apps/kube-flannel-ds created</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="2-3-4-dashboard部署"><a href="#2-3-4-dashboard部署" class="headerlink" title="2.3.4 dashboard部署"></a>2.3.4 dashboard部署</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# mkdir dashboard-certs</span><br><span class="line">[root@master ~]# cd dashboard-certs/</span><br><span class="line">[root@master dashboard-certs]# kubectl create namespace kubernetes-dashboard</span><br><span class="line">namespace/kubernetes-dashboard created</span><br><span class="line">[root@master dashboard-certs]# openssl genrsa -out dashboard.key 2048</span><br><span class="line">Generating RSA private key, 2048 bit long modulus</span><br><span class="line">..........+++</span><br><span class="line">.......................+++</span><br><span class="line">e is 65537 (0x10001)</span><br><span class="line">[root@master dashboard-certs]# openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=dashboard-cert&#x27;</span><br><span class="line">[root@master dashboard-certs]# openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt</span><br><span class="line">Signature ok</span><br><span class="line">subject=/CN=dashboard-cert</span><br><span class="line">Getting Private key</span><br><span class="line">[root@master dashboard-certs]# kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard</span><br><span class="line">secret/kubernetes-dashboard-certs created</span><br><span class="line">[root@master dashboard-certs]# sed -i &quot;s/kubernetesui/$IP\/library/g&quot; /opt/yaml/dashboard/recommended.yaml</span><br><span class="line">[root@master dashboard-certs]# kubectl apply -f /opt/yaml/dashboard/recommended.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">namespace/kubernetes-dashboard configured</span><br><span class="line">serviceaccount/kubernetes-dashboard created</span><br><span class="line">service/kubernetes-dashboard created</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">secret/kubernetes-dashboard-certs configured</span><br><span class="line">secret/kubernetes-dashboard-csrf created</span><br><span class="line">secret/kubernetes-dashboard-key-holder created</span><br><span class="line">configmap/kubernetes-dashboard-settings created</span><br><span class="line">role.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created</span><br><span class="line">deployment.apps/kubernetes-dashboard created</span><br><span class="line">service/dashboard-metrics-scraper created</span><br><span class="line">deployment.apps/dashboard-metrics-scraper created</span><br><span class="line">[root@master dashboard-certs]# kubectl apply -f /opt/yaml/dashboard/dashboard-adminuser.yaml</span><br><span class="line">serviceaccount/dashboard-admin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/dashboard-admin-bind-cluster-role created</span><br><span class="line">[root@master dashboard-certs]# token=`kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep dashboard-admin | awk &#x27;&#123;print $1&#125;&#x27;)`</span><br><span class="line">&#x27;[root@master dashboard-certs]# &#x27;</span><br><span class="line"></span><br><span class="line">&gt; ^C</span><br><span class="line">&gt; [root@master dashboard-certs]# echo &quot;登录令牌：$token&quot;</span><br><span class="line">&gt; 登录令牌：Name:         dashboard-admin-token-lppbw</span><br><span class="line">&gt; Namespace:    kubernetes-dashboard</span><br><span class="line">&gt; Labels:       &lt;none&gt;</span><br><span class="line">&gt; Annotations:  kubernetes.io/service-account.name: dashboard-admin</span><br><span class="line">&gt;          kubernetes.io/service-account.uid: f808d3d3-ffc2-4fbf-bb1d-d6da71c8b89e</span><br><span class="line"></span><br><span class="line">Type:  kubernetes.io/service-account-token</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line"></span><br><span class="line">ca.crt:     1025 bytes</span><br><span class="line">namespace:  20 bytes</span><br><span class="line">token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IjFqbHJxNFptcmZoZXBibEs1NHFwWHRRZGxDck8tWDM4UWRwV3M2ZkoyT3MifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4tbHBwYnciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiZjgwOGQzZDMtZmZjMi00ZmJmLWJiMWQtZDZkYTcxYzhiODllIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.O7GUtnsJxUBP0m-iDKpYx9Bn7XHd02lUiFXaVRv8LtM2M6pB5Snd9smY5Hj3voT--b8AEuizywnRYZtX6mIDxAiRSQhfea4uU5dlG0wuG0_JDnj1w5431RPedZVFwE3xO5YyecwzwMwmCE7XWx9uFFRvTj17ant3BkZN7TMPWOrab4VUU905RWYCzb33WpzCa8nYOiweNzfttopJVYmTpSlVSEQAZH3cx2vl7eW2dmny3Glqz0-OoK5eVk1gpWiAZhRFpMD0540wXBtmGcXnDVcijFxYlo-TzfiJLnh7Q8k9ydPbDok3wViVqqAdsbfGDpa_TjkTpNJJOqVmDPqY8Q</span><br></pre></td></tr></table></figure>

<h4 id="2-3-5node节点部署"><a href="#2-3-5node节点部署" class="headerlink" title="2.3.5node节点部署"></a>2.3.5node节点部署</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 opt]# docker login -u admin -p Harbor12345 192.168.20.119</span><br><span class="line">WARNING! Using --password via the CLI is insecure. Use --password-stdin.</span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span><br><span class="line">Login Succeeded</span><br><span class="line">[root@node1 opt]# ssh master &quot;kubeadm token create --print-join-command&quot; &gt;token.sh</span><br><span class="line">The authenticity of host &#x27;master (192.168.20.119)&#x27; can&#x27;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:FqTDtd28812m1IAFRjAbURuwoPQQRbq7gqGrEYh77C4.</span><br><span class="line">ECDSA key fingerprint is MD5:1a:d0:c6:aa:89:3a:1c:ed:c6:21:1d:dc:4d:63:e8:33.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &#x27;master,192.168.20.119&#x27; (ECDSA) to the list of known hosts.</span><br><span class="line">W0518 05:47:05.650872   30628 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]</span><br><span class="line">[root@node1 opt]# chmod +x token.sh &amp;&amp; source token.sh &amp;&amp; rm -rf token.sh</span><br><span class="line">W0518 05:47:14.165256   24770 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">        [WARNING Service-Kubelet]: kubelet service is not enabled, please run &#x27;systemctl enable kubelet.service&#x27;</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.18&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &#x27;kubectl get nodes&#x27; on the control-plane to see this node join the cluster.</span><br><span class="line">[root@node1 opt]# ssh master &quot;kubectl get nodes&quot;</span><br><span class="line">NAME     STATUS   ROLES    AGE   VERSION</span><br><span class="line">master   Ready    master   29m   v1.18.1</span><br><span class="line">node1    Ready    &lt;none&gt;   16s   v1.18.1</span><br><span class="line"></span><br><span class="line">[root@node2 opt]# docker login -u admin -p 123456 192.168.20.119</span><br><span class="line">WARNING! Using --password via the CLI is insecure. Use --password-stdin.</span><br><span class="line">WARNING! Your password will be stored unencrypted in /root/.docker/config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https://docs.docker.com/engine/reference/commandline/login/#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br><span class="line">[root@node2 opt]# ssh master &quot;kubeadm token create --print-join-command&quot; &gt;token.sh</span><br><span class="line">The authenticity of host &#x27;master (192.168.20.119)&#x27; can&#x27;t be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:FqTDtd28812m1IAFRjAbURuwoPQQRbq7gqGrEYh77C4.</span><br><span class="line">ECDSA key fingerprint is MD5:1a:d0:c6:aa:89:3a:1c:ed:c6:21:1d:dc:4d:63:e8:33.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added &#x27;master,192.168.20.119&#x27; (ECDSA) to the list of known hosts.</span><br><span class="line">W0518 05:50:32.342935    5574 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]</span><br><span class="line">[root@node2 opt]# chmod +x token.sh &amp;&amp; source token.sh &amp;&amp; rm -rf token.sh</span><br><span class="line">W0518 05:50:47.130633   27919 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">        [WARNING Service-Kubelet]: kubelet service is not enabled, please run &#x27;systemctl enable kubelet.service&#x27;</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &#x27;kubectl -n kube-system get cm kubeadm-config -oyaml&#x27;</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.18&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;/var/lib/kubelet/config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;/var/lib/kubelet/kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &#x27;kubectl get nodes&#x27; on the control-plane to see this node join the cluster.</span><br><span class="line">[root@node2 opt]# ssh master &quot;kubectl get nodes&quot;</span><br><span class="line">NAME     STATUS   ROLES    AGE     VERSION</span><br><span class="line">master   Ready    master   33m     v1.18.1</span><br><span class="line">node1    Ready    &lt;none&gt;   3m54s   v1.18.1</span><br><span class="line">node2    Ready    &lt;none&gt;   20s     v1.18.1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>2023云计算国赛样题</title>
    <url>/2023/05/12/2023%E4%BA%91%E8%AE%A1%E7%AE%97%E5%9B%BD%E8%B5%9B%E6%A0%B7%E9%A2%98/</url>
    <content><![CDATA[<h1 id="“云计算应用”赛项赛卷1"><a href="#“云计算应用”赛项赛卷1" class="headerlink" title="“云计算应用”赛项赛卷1"></a>“云计算应用”赛项赛卷1<span id="more"></span></h1><h2 id="模块一-私有云（30分）"><a href="#模块一-私有云（30分）" class="headerlink" title="模块一 私有云（30分）"></a>模块一 私有云（30分）</h2><h3 id="任务1-私有云服务搭建（5分）"><a href="#任务1-私有云服务搭建（5分）" class="headerlink" title="任务1 私有云服务搭建（5分）"></a>任务1 私有云服务搭建（5分）</h3><h4 id="1-1-1-基础环境配置"><a href="#1-1-1-基础环境配置" class="headerlink" title="1.1.1 基础环境配置"></a>1.1.1 基础环境配置</h4><p>1.控制节点主机名为controller，设置计算节点主机名为compute；</p>
<p>2.hosts文件将IP地址映射为主机名。</p>
<p><strong>controller</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# hostnamectl set-hostname controller</span><br><span class="line">[root@controller ~]# su</span><br><span class="line">[root@controller ~]# hostnamectl </span><br><span class="line">   Static hostname: controller</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: cc2c86fe566741e6a2ff6d399c5d5daa</span><br><span class="line">           Boot ID: 214933a71db6473cb11d2c126d890cdf</span><br><span class="line">    Virtualization: kvm</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-1160.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br></pre></td></tr></table></figure>

<p><strong>compute</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# hostnamectl set-hostname compute</span><br><span class="line">[root@compute ~]# su</span><br><span class="line">[root@compute ~]# hostnamectl </span><br><span class="line">   Static hostname: compute</span><br><span class="line">         Icon name: computer-vm</span><br><span class="line">           Chassis: vm</span><br><span class="line">        Machine ID: cc2c86fe566741e6a2ff6d399c5d5daa</span><br><span class="line">           Boot ID: 3f03732ccb29461b9a4f3772d76ea5c3</span><br><span class="line">    Virtualization: kvm</span><br><span class="line">  Operating System: CentOS Linux 7 (Core)</span><br><span class="line">       CPE OS Name: cpe:/o:centos:centos:7</span><br><span class="line">            Kernel: Linux 3.10.0-1160.el7.x86_64</span><br><span class="line">      Architecture: x86-64</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.20.161	controller</span><br><span class="line">192.168.20.195	compute</span><br></pre></td></tr></table></figure>

<h4 id="1-1-2-yum源配置"><a href="#1-1-2-yum源配置" class="headerlink" title="1.1.2 yum源配置"></a>1.1.2 yum源配置</h4><p>使用提供的http服务地址，分别设置controller节点和compute节点的yum源文件http.repo。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# rm -rf /etc/yum.repos.d/*</span><br><span class="line">[root@controller ~]# cat /etc/yum.repos.d/http.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=http://172.19.25.11/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[iaas]</span><br><span class="line">name=iaas</span><br><span class="line">baseurl=http://172.19.25.11/iaas/iaas-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[root@controller ~]# yum repolist</span><br><span class="line">Loaded plugins: fastestmirror</span><br><span class="line">Determining fastest mirrors</span><br><span class="line">centos                                                                                                                           | 3.6 kB  00:00:00     </span><br><span class="line">iaas                                                                                                                             | 2.9 kB  00:00:00     </span><br><span class="line">(1/3): centos/group_gz                                                                                                           | 153 kB  00:00:00     </span><br><span class="line">(2/3): iaas/primary_db                                                                                                           | 597 kB  00:00:00     </span><br><span class="line">(3/3): centos/primary_db                                                                                                         | 3.3 MB  00:00:00     </span><br><span class="line">repo id                                                                  repo name                                                                status</span><br><span class="line">centos                                                                   centos                                                                   4,070</span><br><span class="line">iaas                                                                     iaas                                                                       954</span><br><span class="line">repolist: 5,024</span><br></pre></td></tr></table></figure>

<h4 id="1-1-3-配置无秘钥ssh"><a href="#1-1-3-配置无秘钥ssh" class="headerlink" title="1.1.3 配置无秘钥ssh"></a>1.1.3 配置无秘钥ssh</h4><p>配置controller节点可以无秘钥访问compute节点。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# ssh-keygen </span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:eMrYvgTXlKEoQ4SVWEeZlTiVZMtbt2H5Z/LAjwq9K18 root@controller</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">| *=ooB=o.        |</span><br><span class="line">|o...=+oo o .     |</span><br><span class="line">|  o ..+ + =      |</span><br><span class="line">|   o   * o =     |</span><br><span class="line">|    . + S . = o  |</span><br><span class="line">|     * o .   O   |</span><br><span class="line">|    . = . . E o  |</span><br><span class="line">|     o  .. +     |</span><br><span class="line">|      o. o=.     |</span><br><span class="line">+----[SHA256]-----+</span><br><span class="line">[root@controller ~]# ssh-copy-id root@compute </span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys</span><br><span class="line">root@compute&#x27;s password: </span><br><span class="line"></span><br><span class="line">Number of key(s) added: 1</span><br><span class="line"></span><br><span class="line">Now try logging into the machine, with:   &quot;ssh &#x27;root@compute&#x27;&quot;</span><br><span class="line">and check to make sure that only the key(s) you wanted were added.</span><br><span class="line"></span><br><span class="line">[root@controller ~]# ssh root@compute </span><br><span class="line">Last login: Tue Mar  7 02:18:21 2023</span><br><span class="line">[root@compute ~]#                     #无密钥连接成功</span><br></pre></td></tr></table></figure>

<h4 id="1-1-4-基础安装"><a href="#1-1-4-基础安装" class="headerlink" title="1.1.4 基础安装"></a>1.1.4 基础安装</h4><p>在控制节点和计算节点上分别安装openstack-iaas软件包。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# yum install -y openstack-iaas</span><br><span class="line">[root@controller ~]# cat /etc/openstack/openrc.sh </span><br><span class="line">#--------------------system Config--------------------##</span><br><span class="line">#Controller Server Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP=192.168.20.161</span><br><span class="line"></span><br><span class="line">#Controller HOST Password. example:000000 </span><br><span class="line">HOST_PASS=000000</span><br><span class="line"></span><br><span class="line">#Controller Server hostname. example:controller</span><br><span class="line">HOST_NAME=controller</span><br><span class="line"></span><br><span class="line">#Compute Node Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP_NODE=192.168.20.195</span><br><span class="line"></span><br><span class="line">#Compute HOST Password. example:000000 </span><br><span class="line">HOST_PASS_NODE=000000</span><br><span class="line"></span><br><span class="line">#Compute Node hostname. example:compute</span><br><span class="line">HOST_NAME_NODE=compute</span><br><span class="line"></span><br><span class="line">#--------------------Chrony Config-------------------##</span><br><span class="line">#Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)</span><br><span class="line">network_segment_IP=192.168.20.0/24</span><br><span class="line"></span><br><span class="line">#--------------------Rabbit Config ------------------##</span><br><span class="line">#user for rabbit. example:openstack</span><br><span class="line">RABBIT_USER=openstack</span><br><span class="line"></span><br><span class="line">#Password for rabbit user .example:000000</span><br><span class="line">RABBIT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------MySQL Config---------------------##</span><br><span class="line">#Password for MySQL root user . exmaple:000000</span><br><span class="line">DB_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Keystone Config------------------##</span><br><span class="line">#Password for Keystore admin user. exmaple:000000</span><br><span class="line">DOMAIN_NAME=demo</span><br><span class="line">ADMIN_PASS=000000</span><br><span class="line">DEMO_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql keystore user. exmaple:000000</span><br><span class="line">KEYSTONE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Glance Config--------------------##</span><br><span class="line">#Password for Mysql glance user. exmaple:000000</span><br><span class="line">GLANCE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore glance user. exmaple:000000</span><br><span class="line">GLANCE_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Placement Config----------------------##</span><br><span class="line">#Password for Mysql placement user. exmaple:000000</span><br><span class="line">PLACEMENT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore placement user. exmaple:000000</span><br><span class="line">PLACEMENT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Nova Config----------------------##</span><br><span class="line">#Password for Mysql nova user. exmaple:000000</span><br><span class="line">NOVA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore nova user. exmaple:000000</span><br><span class="line">NOVA_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Neutron Config-------------------##</span><br><span class="line">#Password for Mysql neutron user. exmaple:000000</span><br><span class="line">NEUTRON_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore neutron user. exmaple:000000</span><br><span class="line">NEUTRON_PASS=000000</span><br><span class="line"></span><br><span class="line">#metadata secret for neutron. exmaple:000000</span><br><span class="line">METADATA_SECRET=000000</span><br><span class="line"></span><br><span class="line">#External Network Interface. example:eth1</span><br><span class="line">INTERFACE_NAME=eth0</span><br><span class="line"></span><br><span class="line">#External Network The Physical Adapter. example:provider</span><br><span class="line">Physical_NAME=provider</span><br><span class="line"></span><br><span class="line">#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101</span><br><span class="line">minvlan=101</span><br><span class="line"></span><br><span class="line">#Last Vlan ID in VLAN RANGE for VLAN Network. example:200</span><br><span class="line">maxvlan=200</span><br><span class="line"></span><br><span class="line">#--------------------Cinder Config--------------------##</span><br><span class="line">#Password for Mysql cinder user. exmaple:000000</span><br><span class="line">CINDER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore cinder user. exmaple:000000</span><br><span class="line">CINDER_PASS=000000</span><br><span class="line"></span><br><span class="line">#Cinder Block Disk. example:md126p3</span><br><span class="line">BLOCK_DISK=vdb1</span><br><span class="line"></span><br><span class="line">#--------------------Swift Config---------------------##</span><br><span class="line">#Password for Keystore swift user. exmaple:000000</span><br><span class="line">SWIFT_PASS=000000</span><br><span class="line"></span><br><span class="line">#The NODE Object Disk for Swift. example:md126p4.</span><br><span class="line">OBJECT_DISK=vdb2</span><br><span class="line"></span><br><span class="line">#The NODE IP for Swift Storage Network. example:x.x.x.x.</span><br><span class="line">STORAGE_LOCAL_NET_IP=192.168.20.195</span><br><span class="line"></span><br><span class="line">#--------------------Trove Config----------------------##</span><br><span class="line">#Password for Mysql trove user. exmaple:000000</span><br><span class="line">TROVE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore trove user. exmaple:000000</span><br><span class="line">TROVE_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Heat Config----------------------##</span><br><span class="line">#Password for Mysql heat user. exmaple:000000</span><br><span class="line">HEAT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore heat user. exmaple:000000</span><br><span class="line">HEAT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Ceilometer Config----------------##</span><br><span class="line">#Password for Gnocchi ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------AODH Config----------------##</span><br><span class="line">#Password for Mysql AODH user. exmaple:000000</span><br><span class="line">AODH_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore AODH user. exmaple:000000</span><br><span class="line">AODH_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------ZUN Config----------------##</span><br><span class="line">#Password for Mysql ZUN user. exmaple:000000</span><br><span class="line">ZUN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore ZUN user. exmaple:000000</span><br><span class="line">ZUN_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore KURYR user. exmaple:000000</span><br><span class="line">KURYR_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------OCTAVIA Config----------------##</span><br><span class="line">#Password for Mysql OCTAVIA user. exmaple:000000</span><br><span class="line">OCTAVIA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore OCTAVIA user. exmaple:000000</span><br><span class="line">OCTAVIA_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Manila Config----------------##</span><br><span class="line">#Password for Mysql Manila user. exmaple:000000</span><br><span class="line">MANILA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Manila user. exmaple:000000</span><br><span class="line">MANILA_PASS=000000</span><br><span class="line"></span><br><span class="line">#The NODE Object Disk for Manila. example:md126p5.</span><br><span class="line">SHARE_DISK=vdb3</span><br><span class="line"></span><br><span class="line">#--------------------Cloudkitty Config----------------##</span><br><span class="line">#Password for Mysql Cloudkitty user. exmaple:000000</span><br><span class="line">CLOUDKITTY_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Cloudkitty user. exmaple:000000</span><br><span class="line">CLOUDKITTY_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Barbican Config----------------##</span><br><span class="line">#Password for Mysql Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_PASS=000000</span><br><span class="line">###############################################################</span><br><span class="line">#####在vi编辑器中执行:%s/^.\&#123;1\&#125;//  删除每行前1个字符(#号)#####</span><br><span class="line">###############################################################</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-pre-host.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-pre-host.sh</span><br></pre></td></tr></table></figure>

<h4 id="1-1-5-数据库安装与调优"><a href="#1-1-5-数据库安装与调优" class="headerlink" title="1.1.5 数据库安装与调优"></a>1.1.5 数据库安装与调优</h4><p>在控制节点上使用安装Mariadb、RabbitMQ等服务。并进行相关操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-mysql.sh</span><br></pre></td></tr></table></figure>

<h4 id="1-1-6-Keystone服务安装与使用"><a href="#1-1-6-Keystone服务安装与使用" class="headerlink" title="1.1.6 Keystone服务安装与使用"></a>1.1.6 Keystone服务安装与使用</h4><p>在控制节点上安装Keystone服务并创建用户。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-keystone.sh </span><br><span class="line">[root@controller ~]# openstack user create --domain demo chinaskill --password 000000</span><br></pre></td></tr></table></figure>

<h4 id="1-1-7-Glance安装与使用"><a href="#1-1-7-Glance安装与使用" class="headerlink" title="1.1.7 Glance安装与使用"></a>1.1.7 Glance安装与使用</h4><p>在控制节点上安装Glance 服务。上传镜像至平台，并设置镜像启动的要求参数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-glance.sh </span><br><span class="line">[root@controller ~]# openstack image create cirros --disk qcow2 --container bare --min-disk 10 --min-ram 1024 &lt; cirros-0.3.4-x86_64-disk.img </span><br></pre></td></tr></table></figure>

<h4 id="1-1-8-Nova安装"><a href="#1-1-8-Nova安装" class="headerlink" title="1.1.8 Nova安装"></a>1.1.8 Nova安装</h4><p>在控制节点和计算节点上分别安装Nova服务。安装完成后，完成Nova相关配置。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-placement.sh </span><br><span class="line">[root@controller ~]# iaas-install-nova-controller.sh</span><br><span class="line">[root@compute ~]# iaas-install-nova-compute.sh </span><br></pre></td></tr></table></figure>

<h4 id="1-1-9-Neutron安装"><a href="#1-1-9-Neutron安装" class="headerlink" title="1.1.9 Neutron安装"></a>1.1.9 Neutron安装</h4><p>在控制和计算节点上正确安装Neutron服务。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-neutron-controller.sh</span><br><span class="line">[root@compute ~]# iaas-install-neutron-compute.sh </span><br></pre></td></tr></table></figure>

<h4 id="1-1-10-Dashboard安装"><a href="#1-1-10-Dashboard安装" class="headerlink" title="1.1.10 Dashboard安装"></a>1.1.10 Dashboard安装</h4><p>在控制节点上安装Dashboard服务。安装完成后，将Dashboard中的 Django数据修改为存储在文件中。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-dashboard.sh </span><br><span class="line"></span><br><span class="line">[root@controller ~]# cat /etc/openstack-dashboard/local_settings | grep SESSION_ENGINE</span><br><span class="line"></span><br><span class="line"># SESSION_ENGINE to django.contrib.sessions.backends.signed_cookies</span><br><span class="line"></span><br><span class="line">SESSION_ENGINE = &#x27;django.contrib.sessions.backends.file&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="1-1-11-Swift安装"><a href="#1-1-11-Swift安装" class="headerlink" title="1.1.11 Swift安装"></a>1.1.11 Swift安装</h4><p>在控制节点和计算节点上分别安装Swift服务。安装完成后，将cirros镜像进行分片存储。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-swift-controller.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-swift-compute.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack container create examcontaienr</span><br><span class="line">+---------------------------------------+---------------+------------------------------------+</span><br><span class="line">| account                               | container     | x-trans-id                         |</span><br><span class="line">+---------------------------------------+---------------+------------------------------------+</span><br><span class="line">| AUTH_5a824a04c09c4ba4b75767019900dc98 | examcontaienr | tx453b000251d94847a87e7-006406d19d |</span><br><span class="line">+---------------------------------------+---------------+------------------------------------+</span><br><span class="line">[root@controller ~]# swift upload examcontainer -S 10000000 cirros-0.3.4-x86_64-disk.img </span><br><span class="line">cirros-0.3.4-x86_64-disk.img segment 1</span><br><span class="line">cirros-0.3.4-x86_64-disk.img segment 0</span><br><span class="line">cirros-0.3.4-x86_64-disk.img</span><br></pre></td></tr></table></figure>

<h4 id="1-1-12-Cinder创建硬盘"><a href="#1-1-12-Cinder创建硬盘" class="headerlink" title="1.1.12 Cinder创建硬盘"></a>1.1.12 Cinder创建硬盘</h4><p>在控制节点和计算节点分别安装Cinder服务，请在计算节点，对块存储进行扩容操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-cinder-controller.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-cinder-compute.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# vgextend cinder-volumes /dev/vdb4</span><br><span class="line">[root@compute ~]# vgdisplay </span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               cinder-volumes</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        2</span><br><span class="line">  Metadata Sequence No  5</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                1</span><br><span class="line">  Open LV               0</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                2</span><br><span class="line">  Act PV                2</span><br><span class="line">  VG Size               24.99 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              6398</span><br><span class="line">  Alloc PE / Size       4874 / &lt;19.04 GiB</span><br><span class="line">  Free  PE / Size       1524 / 5.95 GiB</span><br><span class="line">  VG UUID               nM28QF-pGdI-Q1Nh-LvoZ-F6aV-GuDh-2rey4k</span><br></pre></td></tr></table></figure>

<h4 id="1-1-13-Manila服务安装与使用"><a href="#1-1-13-Manila服务安装与使用" class="headerlink" title="1.1.13 Manila服务安装与使用"></a>1.1.13 Manila服务安装与使用</h4><p>在控制和计算节点上分别在控制节点和计算节点安装Manila服务。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-manila-controller.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-manila-compute.sh</span><br></pre></td></tr></table></figure>

<h3 id="任务2-私有云服务运维（15分）"><a href="#任务2-私有云服务运维（15分）" class="headerlink" title="任务2 私有云服务运维（15分）"></a>任务2 私有云服务运维（15分）</h3><h4 id="1-2-1-OpenStack开放镜像权限"><a href="#1-2-1-OpenStack开放镜像权限" class="headerlink" title="1.2.1 OpenStack开放镜像权限"></a>1.2.1 OpenStack开放镜像权限</h4><p>在admin项目中存在glance-cirros镜像文件，将glance-cirros镜像指定demo项目进行共享使用。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image create glance-cirros &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Field            | Value                                                                                                                                                                                      |</span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                                                                                                                                                           |</span><br><span class="line">| container_format | bare                                                                                                                                                                                       |</span><br><span class="line">| created_at       | 2023-03-07T07:05:58Z                                                                                                                                                                       |</span><br><span class="line">| disk_format      | raw                                                                                                                                                                                        |</span><br><span class="line">| file             | /v2/images/54b116cf-e941-403c-9fa3-082e767712f6/file                                                                                                                                       |</span><br><span class="line">| id               | 54b116cf-e941-403c-9fa3-082e767712f6                                                                                                                                                       |</span><br><span class="line">| min_disk         | 0                                                                                                                                                                                          |</span><br><span class="line">| min_ram          | 0                                                                                                                                                                                          |</span><br><span class="line">| name             | glance-cirros                                                                                                                                                                              |</span><br><span class="line">| owner            | 5a824a04c09c4ba4b75767019900dc98                                                                                                                                                           |</span><br><span class="line">| properties       | os_hash_algo=&#x27;sha512&#x27;, os_hash_value=&#x27;1b03ca1bc3fafe448b90583c12f367949f8b0e665685979d95b004e48574b953316799e23240f4f739d1b5eb4c4ca24d38fdc6f4f9d8247a2bc64db25d6bbdb2&#x27;, os_hidden=&#x27;False&#x27; |</span><br><span class="line">| protected        | False                                                                                                                                                                                      |</span><br><span class="line">| schema           | /v2/schemas/image                                                                                                                                                                          |</span><br><span class="line">| size             | 13287936                                                                                                                                                                                   |</span><br><span class="line">| status           | active                                                                                                                                                                                     |</span><br><span class="line">| tags             |                                                                                                                                                                                            |</span><br><span class="line">| updated_at       | 2023-03-07T07:05:59Z                                                                                                                                                                       |</span><br><span class="line">| virtual_size     | None                                                                                                                                                                                       |</span><br><span class="line">| visibility       | shared                                                                                                                                                                                     |</span><br><span class="line">+------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">[root@controller ~]# openstack project list</span><br><span class="line">+----------------------------------+---------+</span><br><span class="line">| ID                               | Name    |</span><br><span class="line">+----------------------------------+---------+</span><br><span class="line">| 5a824a04c09c4ba4b75767019900dc98 | admin   |</span><br><span class="line">| a580b951935c454595e36921e15b4781 | service |</span><br><span class="line">| b8d9637523374f939026d268e033a77b | demo    |</span><br><span class="line">+----------------------------------+---------+</span><br><span class="line">[root@controller ~]# glance member-create 54b116cf-e941-403c-9fa3-082e767712f6 b8d9637523374f939026d268e033a77b</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">| Image ID                             | Member ID                        | Status  |</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">| 54b116cf-e941-403c-9fa3-082e767712f6 | b8d9637523374f939026d268e033a77b | pending |</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">[root@controller ~]# glance member-update 54b116cf-e941-403c-9fa3-082e767712f6 b8d9637523374f939026d268e033a77b accepted</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br><span class="line">| Image ID                             | Member ID                        | Status   |</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br><span class="line">| 54b116cf-e941-403c-9fa3-082e767712f6 | b8d9637523374f939026d268e033a77b | accepted |</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br></pre></td></tr></table></figure>

<h4 id="1-2-2-SkyWalking-应用部署"><a href="#1-2-2-SkyWalking-应用部署" class="headerlink" title="1.2.2   SkyWalking 应用部署"></a>1.2.2   SkyWalking 应用部署</h4><p>申请一台云主机，使用提供的软件包安装Elasticsearch服务和SkyWalking服务。再申请一台云主机，用于搭建gpmall商城应用，并配置SkyWalking 监控gpmall主机。</p>
<h4 id="1-2-3-OpenStack镜像压缩"><a href="#1-2-3-OpenStack镜像压缩" class="headerlink" title="1.2.3 OpenStack镜像压缩"></a>1.2.3 OpenStack镜像压缩</h4><p>在HTTP文件服务器中存在一个镜像为CentOS7.5-compress.qcow2的镜像，请对该镜像进行压缩操作。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# qemu-img convert -c -O qcow2 cirros-0.3.4-x86_64-disk.img chinaskill-cirros.img</span><br></pre></td></tr></table></figure>

<h4 id="1-2-4-Glance对接Cinder存储"><a href="#1-2-4-Glance对接Cinder存储" class="headerlink" title="1.2.4 Glance对接Cinder存储"></a>1.2.4 Glance对接Cinder存储</h4><p>在自行搭建的OpenStack平台中修改相关参数，使Glance可以使用Cinder作为后端存储。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# vi /etc/glance/glance-api.conf</span><br><span class="line">stores = file,http,swift,cinder</span><br><span class="line">show_multiple_locations = True</span><br><span class="line">[root@controller ~]# vi /etc/cinder/cinder.conf</span><br><span class="line">glance_api_version = 2</span><br><span class="line">allowed_direct_url_schemes = cinder</span><br><span class="line">image_upload_use_internal_tenant = True</span><br><span class="line">[root@controller ~]# systemctl restart openstack-glance*</span><br><span class="line">[root@controller ~]# systemctl restart openstack-cinder*</span><br><span class="line">去图形界面通过镜像创建一个云硬盘</span><br><span class="line">[root@controller ~]# cinder list</span><br><span class="line">+--------------------------------------+-----------+---------------+------+-------------+----------+-------------</span><br><span class="line">+</span><br><span class="line">| ID | Status | Name | Size | Volume Type</span><br><span class="line">| Bootable | Attached to |</span><br><span class="line">+--------------------------------------+-----------+---------------+------+-------------+----------+-------------</span><br><span class="line">+</span><br><span class="line">| 13b79150-8571-48e6-b3f9-478912d2b678 | available | cirros-cinder | 15 | - | true</span><br><span class="line">| |</span><br><span class="line">+--------------------------------------+-----------+---------------+------+-------------+----------+-------------</span><br><span class="line">+</span><br><span class="line">[root@controller ~]# glance image-create --name cirros-image --disk-format qcow2 --container</span><br><span class="line">bare</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| checksum | None |</span><br><span class="line">| container_format | bare |</span><br><span class="line">| created_at | 2022-05-27T10:29:35Z |</span><br><span class="line">| disk_format | qcow2 |</span><br><span class="line">| id | 9d163f0d-58f2-49bd-b438-a4e2704e57ad |</span><br><span class="line">| locations | [] |</span><br><span class="line">| min_disk | 0 |</span><br><span class="line">| min_ram | 0 |</span><br><span class="line">| name | cirros-image |</span><br><span class="line">| os_hash_algo | None |</span><br><span class="line">| os_hash_value | None |</span><br><span class="line">| os_hidden | False |</span><br><span class="line">| owner | 50957190677d4ef186e7b0c04b99b44f |</span><br><span class="line">| protected | False |</span><br><span class="line">| size | None |</span><br><span class="line">| status | queued |</span><br><span class="line">| tags | [] |</span><br><span class="line">| updated_at | 2022-05-27T10:29:35Z |</span><br><span class="line">| virtual_size | Not available |</span><br><span class="line">| visibility | shared |</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# glance location-add 9d163f0d-58f2-49bd-b438-a4e2704e57ad --url</span><br><span class="line">cinder://13b79150-8571-48e6-b3f9-478912d2b678</span><br></pre></td></tr></table></figure>

<h4 id="1-2-5-使用Heat模板创建容器"><a href="#1-2-5-使用Heat模板创建容器" class="headerlink" title="1.2.5 使用Heat模板创建容器"></a>1.2.5 使用Heat模板创建容器</h4><p>在自行搭建的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板文件，要求执行yaml文件可以创建名为heat-swift的容器。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_container.yaml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  server:</span><br><span class="line">    type: OS::Swift::Container</span><br><span class="line">    properties:</span><br><span class="line">      name: heat-swift</span><br><span class="line">[root@controller ~]# openstack stack create -t create_container.yaml heat-swift</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| Field               | Value                                |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| id                  | 81873448-b1ad-48d9-9bbc-31fb158e1881 |</span><br><span class="line">| stack_name          | heat-swift                           |</span><br><span class="line">| description         | No description                       |</span><br><span class="line">| creation_time       | 2023-03-07T07:50:52Z                 |</span><br><span class="line">| updated_time        | None                                 |</span><br><span class="line">| stack_status        | CREATE_IN_PROGRESS                   |</span><br><span class="line">| stack_status_reason | Stack CREATE started                 |</span><br><span class="line">+---------------------+--------------------------------------+   </span><br></pre></td></tr></table></figure>

<h4 id="1-2-6-Nova清除缓存"><a href="#1-2-6-Nova清除缓存" class="headerlink" title="1.2.6 Nova清除缓存"></a>1.2.6 Nova清除缓存</h4><p>在OpenStack平台上，修改相关配置，让长时间不用的镜像缓存在过一定的时间后会被自动删除。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/nova/nova.conf | grep remove_unused_base_images=true</span><br><span class="line">remove_unused_base_images=true</span><br><span class="line">[root@controller ~]# systemctl restart *nova*</span><br></pre></td></tr></table></figure>

<h4 id="1-2-7-Redis集群部署。"><a href="#1-2-7-Redis集群部署。" class="headerlink" title="1.2.7 Redis集群部署。"></a>1.2.7 Redis集群部署。</h4><p>部署Redis集群，Redis的一主二从三哨兵架构。</p>
<h4 id="1-2-8-Redis-AOF调优"><a href="#1-2-8-Redis-AOF调优" class="headerlink" title="1.2.8 Redis AOF调优"></a>1.2.8 Redis AOF调优</h4><p>修改在Redis相关配置，避免AOF文件过大，Redis会进行AOF重写。</p>
<h4 id="1-2-9-JumpServer堡垒机部署"><a href="#1-2-9-JumpServer堡垒机部署" class="headerlink" title="1.2.9 JumpServer堡垒机部署"></a>1.2.9 JumpServer堡垒机部署</h4><p>使用提供的软件包安装JumpServer堡垒机服务，并配置使用该堡垒机对接自己安装的控制和计算节点。</p>
<h4 id="1-2-10-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"><a href="#1-2-10-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="1.2.10 完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>1.2.10 完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-私有云运维开发（10分）"><a href="#任务3-私有云运维开发（10分）" class="headerlink" title="任务3 私有云运维开发（10分）"></a>任务3 私有云运维开发（10分）</h3><h4 id="1-3-1-编写Shell一键部署脚本"><a href="#1-3-1-编写Shell一键部署脚本" class="headerlink" title="1.3.1 编写Shell一键部署脚本"></a>1.3.1 编写Shell一键部署脚本</h4><p>编写一键部署脚本，要求可以一键部署gpmall商城应用系统。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">setenforce 0</span><br><span class="line">rm -rf /etc/yum.repos.d/*</span><br><span class="line">cat &gt;&gt; /etc/yum.repos.d/local.repo &lt;&lt; EOF</span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=http://172.19.25.222/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[gpmall]</span><br><span class="line">name=gpmall</span><br><span class="line">baseurl=file:///opt/gpmall-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"> </span><br><span class="line">EOF</span><br><span class="line">yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel</span><br><span class="line">java -version</span><br><span class="line">sleep 5</span><br><span class="line">yum install -y redis nginx </span><br><span class="line">yum install -y mariadb mariadb-server</span><br><span class="line">tar -zxvf /opt/zookeeper-3.4.14.tar.gz -C /usr/local/</span><br><span class="line">mv /usr/local/zookeeper-3.4.14/conf/zoo_sample.cfg /usr/local/zookeeper-3.4.14/conf/zoo.cfg</span><br><span class="line">sh /usr/local/zookeeper-3.4.14/bin/zkServer.sh start</span><br><span class="line">sleep 5</span><br><span class="line">sh /usr/local/zookeeper-3.4.14/bin/zkServer.sh status</span><br><span class="line">sleep 5</span><br><span class="line">tar -zxvf /opt/kafka_2.11-1.1.1.tgz -C /usr/local/</span><br><span class="line">sh /usr/local/kafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon /usr/local/kafka_2.11-1.1.1/config/server.properties</span><br><span class="line">sleep 3</span><br><span class="line">jps</span><br><span class="line">sleep 3</span><br><span class="line">netstat -ntpl</span><br><span class="line">sleep 5</span><br><span class="line">sed -i &quot;/^\[mysqld\]$/a character_set_server=utf8&quot; /etc/my.cnf.d/server.cnf</span><br><span class="line">systemctl start mariadb</span><br><span class="line">mysqladmin -uroot password 123456</span><br><span class="line">sleep 8</span><br><span class="line">mysql -uroot -p123456 -e &quot;grant all privileges on *.* to root@&#x27;%&#x27; identified by &#x27;123456&#x27;;&quot;</span><br><span class="line">mysql -uroot -p123456 -e &quot;create database gpmall;use gpmall;source /opt/gpmall.sql;&quot;</span><br><span class="line">systemctl start redis</span><br><span class="line">sed -i &quot;s/bind 127.0.0.1/bind 0.0.0.0/g&quot; /etc/redis.conf</span><br><span class="line">sed -i &quot;s/protected-mode yes/protected-mode no/g&quot; /etc/redis.conf</span><br><span class="line">systemctl restart redis</span><br><span class="line">systemctl start nginx</span><br><span class="line">cat &gt; /etc/hosts &lt;&lt; EOF</span><br><span class="line">192.168.20.187 mall</span><br><span class="line">192.168.20.187 kafka.mall</span><br><span class="line">192.168.20.187 mysql.mall</span><br><span class="line">192.168.20.187 redis.mall</span><br><span class="line">192.168.20.187 zookeeper.mall</span><br><span class="line">EOF</span><br><span class="line">rm -rf /usr/share/nginx/html/*</span><br><span class="line">cp -rf /opt/dist/* /usr/share/nginx/html/</span><br><span class="line">sed -i &quot;1 a location /user &#123;proxy_pass http://127.0.0.1:8082;&#125;&quot; /etc/nginx/conf.d/default.conf</span><br><span class="line">sed -i &quot;2 a location /shopping &#123;proxy_pass http://127.0.0.1:8081;&#125;&quot; /etc/nginx/conf.d/default.conf</span><br><span class="line">sed -i &quot;3 a location /cashier &#123;proxy_pass http://127.0.0.1:8083;&#125;&quot; /etc/nginx/conf.d/default.conf</span><br><span class="line">systemctl restart nginx</span><br><span class="line">nohup java -jar /opt/shopping-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /opt/user-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /opt/gpmall-shopping-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /opt/gpmall-user-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">jps</span><br></pre></td></tr></table></figure>

<h4 id="1-3-2-Ansible部署FTP服务"><a href="#1-3-2-Ansible部署FTP服务" class="headerlink" title="1.3.2 Ansible部署FTP服务"></a>1.3.2 Ansible部署FTP服务</h4><p>编写Ansible脚本，部署FTP服务。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ftp]# cat ftp.yaml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">  - hosts: node</span><br><span class="line">    remote_user: root</span><br><span class="line">    tasks:</span><br><span class="line">     - name: install vsftpd </span><br><span class="line">       yum:</span><br><span class="line">         name: vsftpd</span><br><span class="line">     - name: start vsftpd</span><br><span class="line">       service:</span><br><span class="line">         name: vsftpd</span><br><span class="line">         state: started</span><br><span class="line">         enabled: true</span><br></pre></td></tr></table></figure>

<h4 id="1-3-3-Ansible部署Kafka服务"><a href="#1-3-3-Ansible部署Kafka服务" class="headerlink" title="1.3.3 Ansible部署Kafka服务"></a>1.3.3 Ansible部署Kafka服务</h4><p>编写Playbook，部署的ZooKeeper和Kafka。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@owncloud zookeeper]# cat zookeeper.yaml </span><br><span class="line">---</span><br><span class="line">  - hosts: zookeeper</span><br><span class="line">    tasks:</span><br><span class="line">      - name: stop selinux</span><br><span class="line">        shell: setenforce 0</span><br><span class="line">      - name: remove repo</span><br><span class="line">        shell: rm -rf /etc/yum.repos.d/*</span><br><span class="line">      - name: copy repo</span><br><span class="line">        copy:</span><br><span class="line">          src: local.repo</span><br><span class="line">          dest: /etc/yum.repos.d/</span><br><span class="line">      - name: install java</span><br><span class="line">        yum:</span><br><span class="line">          name: java-1.8.0-openjdk,java-1.8.0-openjdk-devel</span><br><span class="line">      - name: tar zookeeper</span><br><span class="line">        unarchive:</span><br><span class="line">          src: zookeeper-3.4.14.tar.gz</span><br><span class="line">          dest: /usr/local</span><br><span class="line">      - name: copy config</span><br><span class="line">        copy:</span><br><span class="line">          src: zoo.cfg</span><br><span class="line">          dest: /usr/local/zookeeper-3.4.14/conf/zoo.cfg</span><br><span class="line">      - name: mkdir</span><br><span class="line">        shell: mkdir /tmp/zookeeper</span><br><span class="line">      - name: touch id file</span><br><span class="line">        shell: echo &#123;&#123;zk_id&#125;&#125; &gt; /tmp/zookeeper/myid</span><br><span class="line">      - name: start zk</span><br><span class="line">        shell: /bin/bash /usr/local/zookeeper-3.4.14/bin/zkServer.sh start</span><br><span class="line">      - name: tar kafka</span><br><span class="line">        unarchive:</span><br><span class="line">          src: kafka_2.11-1.1.1.tgz</span><br><span class="line">          dest: /usr/local</span><br><span class="line">      - name: mkdir</span><br><span class="line">        shell: mkdir /tmp/kafka-logs</span><br><span class="line">      - name: copy config</span><br><span class="line">        template:</span><br><span class="line">          src: server.properties.j2</span><br><span class="line">          dest: /usr/local/kafka_2.11-1.1.1/config/server.properties</span><br><span class="line">      - name: start kafka</span><br><span class="line">        shell: /bin/bash /usr/local/kafka_2.11-1.1.1/bin/kafka-server-start.sh -daemon /usr/local/kafka_2.11-1.1.1/config/server.properties</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@owncloud zookeeper]# cat /etc/ansible/hosts</span><br><span class="line">[zookeeper]</span><br><span class="line">192.168.20.190	ip=190 zk_id=1 </span><br><span class="line">192.168.20.194	ip=194 zk_id=2 </span><br><span class="line">192.168.20.189	ip=189 zk_id=3 </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@owncloud zookeeper]# cat zoo.cfg </span><br><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/tmp/zookeeper</span><br><span class="line">clientPort=2181</span><br><span class="line">#autopurge.purgeInterval=1</span><br><span class="line">server.1=192.168.20.190:2888:3888</span><br><span class="line">server.2=192.168.20.194:2888:3888</span><br><span class="line">server.3=192.168.20.189:2888:3888</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@owncloud zookeeper]# cat server.properties.j2 </span><br><span class="line">broker.id=&#123;&#123;zk_id&#125;&#125;</span><br><span class="line">listeners=PLAINTEXT://192.168.20.&#123;&#123;ip&#125;&#125;:9092</span><br><span class="line">num.network.threads=3</span><br><span class="line">num.io.threads=8</span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line">log.dirs=/tmp/kafka-logs</span><br><span class="line">num.partitions=1</span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line">transaction.state.log.replication.factor=1</span><br><span class="line">transaction.state.log.min.isr=1</span><br><span class="line">log.retention.hours=168</span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line">zookeeper.connect=192.168.20.190:2181,192.168.20.194:2181,192.168.20.189:2181</span><br><span class="line">zookeeper.connection.timeout.ms=6000</span><br><span class="line">group.initial.rebalance.delay.ms=0</span><br></pre></td></tr></table></figure>

<h2 id="模块二-容器云（30分）"><a href="#模块二-容器云（30分）" class="headerlink" title="模块二 容器云（30分）"></a>模块二 容器云（30分）</h2><p>企业构建Kubernetes容器云集群，引入KubeVirt实现OpenStack到Kubernetes的全面转型，用Kubernetes来管一切虚拟化运行时，包含裸金属、VM、容器。同时研发团队决定搭建基于Kubernetes 的CI&#x2F;CD环境，基于这个平台来实现DevOps流程。引入服务网格Istio，实现业务系统的灰度发布，治理和优化公司各种微服务，并开发自动化运维程序。</p>
<h3 id="任务1-容器云服务搭建（5分）"><a href="#任务1-容器云服务搭建（5分）" class="headerlink" title="任务1 容器云服务搭建（5分）"></a>任务1 容器云服务搭建（5分）</h3><h4 id="2-1-1-部署容器云平台"><a href="#2-1-1-部署容器云平台" class="headerlink" title="2.1.1 部署容器云平台"></a>2.1.1 部署容器云平台</h4><p>使用OpenStack私有云平台创建两台云主机，分别作为Kubernetes集群的master节点和node节点，然后完成Kubernetes集群的部署，并完成Istio服务网格、KubeVirt虚拟化和Harbor镜像仓库的部署。</p>
<h3 id="任务2-容器云服务运维（15分）"><a href="#任务2-容器云服务运维（15分）" class="headerlink" title="任务2 容器云服务运维（15分）"></a>任务2 容器云服务运维（15分）</h3><h4 id="2-2-1-容器化部署Node-Exporter"><a href="#2-2-1-容器化部署Node-Exporter" class="headerlink" title="2.2.1 容器化部署Node-Exporter"></a>2.2.1 容器化部署Node-Exporter</h4><p>编写Dockerfile文件构建exporter镜像，要求基于centos完成Node-Exporter服务的安装与配置，并设置服务开机自启。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat Dockerfile-exporter </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD node_exporter-0.18.1.linux-amd64.tar.gz /usr/local</span><br><span class="line">RUN chmod +x /usr/local/node_exporter-0.18.1.linux-amd64/node_exporter</span><br><span class="line">EXPOSE 9100</span><br><span class="line">CMD [&quot;/usr/local/node_exporter-0.18.1.linux-amd64/node_exporter&quot;]</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-容器化部署Alertmanager"><a href="#2-2-2-容器化部署Alertmanager" class="headerlink" title="2.2.2 容器化部署Alertmanager"></a>2.2.2 容器化部署Alertmanager</h4><p>编写Dockerfile文件构建alert镜像，要求基于centos：latest完成Alertmanager服务的安装与配置，并设置服务开机自启。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat Dockerfile-alert </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD alertmanager-0.19.0.linux-amd64.tar.gz /usr/local/</span><br><span class="line">RUN chmod +x /usr/local/alertmanager-0.19.0.linux-amd64/alertmanager</span><br><span class="line">EXPOSE 9093</span><br><span class="line">EXPOSE 9094</span><br><span class="line">CMD [&quot;/usr/local/alertmanager-0.19.0.linux-amd64/alertmanager&quot;,&quot;--config.file=/usr/local/alertmanager-0.19.0.linux-amd64/alertmanager.yml&quot;]</span><br></pre></td></tr></table></figure>

<h4 id="2-2-3-容器化部署Grafana"><a href="#2-2-3-容器化部署Grafana" class="headerlink" title="2.2.3 容器化部署Grafana"></a>2.2.3 容器化部署Grafana</h4><p>编写Dockerfile文件构建grafana镜像，要求基于centos完成Grafana服务的安装与配置，并设置服务开机自启。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat Dockerfile-grafana </span><br><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD grafana-6.4.1.linux-amd64.tar.gz /usr/local/</span><br><span class="line">RUN chmod +x /usr/local/grafana-6.4.1/bin/grafana-server</span><br><span class="line">EXPOSE 3000</span><br><span class="line">CMD [&quot;/usr/local/grafana-6.4.1/bin/grafana-server&quot;,&quot;--config=/usr/local/grafana-6.4.1/conf/defaults.ini&quot;,&quot;--homepath=/usr/local/grafana-6.4.1&quot;] </span><br></pre></td></tr></table></figure>

<h4 id="2-2-4-容器化部署Prometheus"><a href="#2-2-4-容器化部署Prometheus" class="headerlink" title="2.2.4 容器化部署Prometheus"></a>2.2.4 容器化部署Prometheus</h4><p>编写Dockerfile文件构建prometheus镜像，要求基于centos完成Promethues服务的安装与配置，并设置服务开机自启。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 monint]# cat prometheus.yml </span><br><span class="line"></span><br><span class="line"># my global config</span><br><span class="line"></span><br><span class="line">global:</span><br><span class="line">  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"></span><br><span class="line">  # scrape_timeout is set to the global default (10s).</span><br><span class="line"></span><br><span class="line"># Alertmanager configuration</span><br><span class="line"></span><br><span class="line">alerting:</span><br><span class="line">  alertmanagers:</span><br><span class="line">    - static_configs:</span><br><span class="line">        - targets: </span><br><span class="line">          - 192.168.20.133:9093</span><br><span class="line"></span><br><span class="line"># Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.</span><br><span class="line"></span><br><span class="line">rule_files:</span><br><span class="line"></span><br><span class="line">  - &quot;rules/*.yml&quot;</span><br><span class="line"></span><br><span class="line">  # - &quot;second_rules.yml&quot;</span><br><span class="line"></span><br><span class="line"># A scrape configuration containing exactly one endpoint to scrape:</span><br><span class="line"></span><br><span class="line"># Here it&#x27;s Prometheus itself.</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line"></span><br><span class="line">  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span><br><span class="line"></span><br><span class="line">  - job_name: &quot;prometheus&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.20.133:9090&quot;]</span><br><span class="line"></span><br><span class="line">  - job_name: &quot;node&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&quot;192.168.20.133:9100&quot;]</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:7.9.2009</span><br><span class="line">ADD prometheus-2.13.0.linux-amd64.tar.gz /usr/local/</span><br><span class="line">RUN chmod +x /usr/local/prometheus-2.13.0.linux-amd64/prometheus</span><br><span class="line">ADD prometheus.yml /usr/local/prometheus-2.13.0.linux-amd64/</span><br><span class="line">RUN mkdir /usr/local/prometheus-2.13.0.linux-amd64/rules</span><br><span class="line">ADD alert-rules.yml /usr/local/prometheus-2.13.0.linux-amd64/rules </span><br><span class="line">EXPOSE 9090</span><br><span class="line">CMD [&quot;/usr/local/prometheus-2.13.0.linux-amd64/prometheus&quot;,&quot;--config.file=/usr/local/prometheus-2.13.0.linux-amd64/prometheus.yml&quot;]</span><br></pre></td></tr></table></figure>

<h4 id="2-2-5-编排部署监控系统"><a href="#2-2-5-编排部署监控系统" class="headerlink" title="2.2.5 编排部署监控系统"></a>2.2.5 编排部署监控系统</h4><p>编写docker-compose.yaml文件，使用镜像exporter、alert、grafana和prometheus完成监控系统的编排部署。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@k8s-master-node1 Monitor]# cat docker-compose.yaml </span><br><span class="line">version: &#x27;3&#x27;</span><br><span class="line">services:</span><br><span class="line">  node-exporter:</span><br><span class="line">    image: monitor-exporter:v1.0</span><br><span class="line">    container_name: &quot;monitor-node&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9100:9100&quot;</span><br><span class="line">    restart: always</span><br><span class="line">  alertmanager:</span><br><span class="line">    image: monitor-alert:v1.0</span><br><span class="line">    container_name: &quot;monitor-alertmanager&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9093:9093&quot;</span><br><span class="line">      - &quot;9094:9094&quot;</span><br><span class="line">    restart: always</span><br><span class="line">  grafana:</span><br><span class="line">    image: monitor-grafana:v1.0</span><br><span class="line">    container_name: &quot;monitor-grafana&quot;</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;3000:3000&quot;</span><br><span class="line">    restart: always</span><br><span class="line"></span><br><span class="line">  prometheus:</span><br><span class="line">    image: monitor-prometheus:v1.0</span><br><span class="line">    container_name: &quot;monitor-prometheus&quot;</span><br><span class="line">    depends_on:</span><br><span class="line">      - node-exporter</span><br><span class="line">      - alertmanager</span><br><span class="line">      - grafana</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9090:9090&quot;</span><br><span class="line">    restart: always </span><br></pre></td></tr></table></figure>

<h4 id="2-2-6-安装Jenkins"><a href="#2-2-6-安装Jenkins" class="headerlink" title="2.2.6 安装Jenkins"></a>2.2.6 安装Jenkins</h4><p>将Jenkins部署到default命名空间下。要求完成离线插件的安装，设置Jenkins的登录信息和授权策略。</p>
<h4 id="2-2-7-安装GitLab"><a href="#2-2-7-安装GitLab" class="headerlink" title="2.2.7 安装GitLab"></a>2.2.7 安装GitLab</h4><p>将GitLab部署到default命名空间下，要求设置root用户密码，新建公开项目，并将提供的代码上传到该项目。</p>
<h4 id="2-2-8-配置Jenkins连接GitLab"><a href="#2-2-8-配置Jenkins连接GitLab" class="headerlink" title="2.2.8 配置Jenkins连接GitLab"></a>2.2.8 配置Jenkins连接GitLab</h4><p>在Jenkins中新建流水线任务，配置GitLab连接Jenkins，并完成WebHook的配置。</p>
<h4 id="2-2-9-构建CI-x2F-CD"><a href="#2-2-9-构建CI-x2F-CD" class="headerlink" title="2.2.9 构建CI&#x2F;CD"></a>2.2.9 构建CI&#x2F;CD</h4><p>在流水线任务中编写流水线脚本，完成后触发构建，要求基于GitLab中的项目自动完成代码编译、镜像构建与推送、并自动发布服务到Kubernetes集群中。</p>
<h4 id="2-2-10-服务网格：创建Ingress-Gateway"><a href="#2-2-10-服务网格：创建Ingress-Gateway" class="headerlink" title="2.2.10 服务网格：创建Ingress Gateway"></a>2.2.10 服务网格：创建Ingress Gateway</h4><p>将Bookinfo应用部署到default命名空间下，请为Bookinfo应用创建一个网关，使外部可以访问Bookinfo应用。</p>
<h4 id="2-2-11-KubeVirt运维：创建VM"><a href="#2-2-11-KubeVirt运维：创建VM" class="headerlink" title="2.2.11 KubeVirt运维：创建VM"></a>2.2.11 KubeVirt运维：创建VM</h4><p>使用提供的镜像在default命名空间下创建一台VM，名称为exam，指定VM的内存、CPU、网卡和磁盘等配置。</p>
<h4 id="2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"><a href="#2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="2.2.12 完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>2.2.12 完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-容器云运维开发（10分）"><a href="#任务3-容器云运维开发（10分）" class="headerlink" title="任务3 容器云运维开发（10分）"></a>任务3 容器云运维开发（10分）</h3><h4 id="2-3-1-管理job服务"><a href="#2-3-1-管理job服务" class="headerlink" title="2.3.1 管理job服务"></a>2.3.1 管理job服务</h4><p>Kubernetes Python运维脚本开发-使用SDK方式管理job服务。</p>
<h4 id="2-3-2-自定义调度器"><a href="#2-3-2-自定义调度器" class="headerlink" title="2.3.2 自定义调度器"></a>2.3.2 自定义调度器</h4><p>Kubernetes Python运维脚本开发-使用Restful API方式管理调度器。</p>
<h4 id="2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"><a href="#2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="2.3.3 编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>2.3.3 编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块三-公有云（40分）"><a href="#模块三-公有云（40分）" class="headerlink" title="模块三 公有云（40分）"></a>模块三 公有云（40分）</h2><p>企业选择国内公有云提供商，选择云主机、云网络、云硬盘、云防火墙、负载均衡等服务，可创建Web服务，共享文件存储服务，数据库服务，数据库集群等服务。搭建基于云原生的DevOps相关服务，构建云、边、端一体化的边缘计算系统，并开发云应用程序。</p>
<p>根据上述公有云平台的特性，完成公有云中的各项运维工作。</p>
<h3 id="任务1-公有云服务搭建（5分）"><a href="#任务1-公有云服务搭建（5分）" class="headerlink" title="任务1 公有云服务搭建（5分）"></a>任务1 公有云服务搭建（5分）</h3><h4 id="3-1-1-私有网络管理"><a href="#3-1-1-私有网络管理" class="headerlink" title="3.1.1 私有网络管理"></a>3.1.1 私有网络管理</h4><p>在公有云中完成虚拟私有云网络的创建。</p>
<h4 id="3-1-2-云实例管理"><a href="#3-1-2-云实例管理" class="headerlink" title="3.1.2 云实例管理"></a>3.1.2 云实例管理</h4><p>登录公有云平台，创建两台云实例虚拟机。</p>
<h4 id="3-1-3-管理数据库"><a href="#3-1-3-管理数据库" class="headerlink" title="3.1.3 管理数据库"></a>3.1.3 管理数据库</h4><p>使用intnetX-mysql网络创建两台chinaskill-sql-1和chinaskill-sql-2云服务器，并完成MongoDB安装。</p>
<h4 id="3-1-4-主从数据库"><a href="#3-1-4-主从数据库" class="headerlink" title="3.1.4 主从数据库"></a>3.1.4 主从数据库</h4><p>在chinaskill-sql-1和chinaskill-sql-2云服务器中配置MongoDB主从数据库。</p>
<h4 id="3-1-5-node环境管理"><a href="#3-1-5-node环境管理" class="headerlink" title="3.1.5 node环境管理"></a>3.1.5 node环境管理</h4><p>使用提供的压缩文件，安装Node.js环境。</p>
<h4 id="3-1-6-安全组管理"><a href="#3-1-6-安全组管理" class="headerlink" title="3.1.6 安全组管理"></a>3.1.6 安全组管理</h4><p>根据要求，创建一个安全组。</p>
<h4 id="3-1-7-RocketChat上云"><a href="#3-1-7-RocketChat上云" class="headerlink" title="3.1.7 RocketChat上云"></a>3.1.7 RocketChat上云</h4><p>使用http服务器提供文件，将Rocket.Chat应用部署上云。</p>
<h4 id="3-1-8-NAT网关"><a href="#3-1-8-NAT网关" class="headerlink" title="3.1.8 NAT网关"></a>3.1.8 NAT网关</h4><p>根据要求创建一个公网NAT网关。</p>
<h4 id="3-1-9云服务器备份"><a href="#3-1-9云服务器备份" class="headerlink" title="3.1.9云服务器备份"></a>3.1.9云服务器备份</h4><p>创建一个云服务器备份存储库名为server_backup，容量为100G。将ChinaSkill-node-1云服务器制作镜像文件chinaskill-image。</p>
<h4 id="3-1-10-负载均衡器"><a href="#3-1-10-负载均衡器" class="headerlink" title="3.1.10 负载均衡器"></a>3.1.10 负载均衡器</h4><p>根据要求创建一个负载均衡器chinaskill-elb。</p>
<h4 id="3-1-11-弹性伸缩管理"><a href="#3-1-11-弹性伸缩管理" class="headerlink" title="3.1.11 弹性伸缩管理"></a>3.1.11 弹性伸缩管理</h4><p>根据要求新建一个弹性伸缩启动配置。</p>
<h3 id="任务2-公有云服务运维（10分）"><a href="#任务2-公有云服务运维（10分）" class="headerlink" title="任务2 公有云服务运维（10分）"></a>任务2 公有云服务运维（10分）</h3><h4 id="3-2-1-云容器引擎"><a href="#3-2-1-云容器引擎" class="headerlink" title="3.2.1 云容器引擎"></a>3.2.1 云容器引擎</h4><p>在公有云上，按照要求创建一个x86架构的容器云集群。</p>
<h4 id="3-2-2-云容器管理"><a href="#3-2-2-云容器管理" class="headerlink" title="3.2.2 云容器管理"></a>3.2.2 云容器管理</h4><p>使用插件管理在kcloud容器集群中安装Dashboard可视化监控界面。</p>
<h4 id="3-2-3-使用kubectl操作集群"><a href="#3-2-3-使用kubectl操作集群" class="headerlink" title="3.2.3 使用kubectl操作集群"></a>3.2.3 使用kubectl操作集群</h4><p>在kcloud集群中安装kubectl命令，使用kubectl命令管理kcloud集群。</p>
<h4 id="3-2-4-安装Helm"><a href="#3-2-4-安装Helm" class="headerlink" title="3.2.4 安装Helm"></a>3.2.4 安装Helm</h4><p>使用提供的Helm软件包，在kcloud集群中安装Helm服务。</p>
<h4 id="3-2-5-根据提供的chart包mariadb-7-3-14-tgz部署mariadb服务，修改mariadb使用NodePort模式对其进行访问。"><a href="#3-2-5-根据提供的chart包mariadb-7-3-14-tgz部署mariadb服务，修改mariadb使用NodePort模式对其进行访问。" class="headerlink" title="3.2.5 根据提供的chart包mariadb-7.3.14.tgz部署mariadb服务，修改mariadb使用NodePort模式对其进行访问。"></a>3.2.5 根据提供的chart包mariadb-7.3.14.tgz部署mariadb服务，修改mariadb使用NodePort模式对其进行访问。</h4><h4 id="3-2-6-在k8s集群中创建mariadb命名空间，根据提供的chart包mariadb-7-3-14-tgz修改其配置，使用NodePort模式对其进行访问。"><a href="#3-2-6-在k8s集群中创建mariadb命名空间，根据提供的chart包mariadb-7-3-14-tgz修改其配置，使用NodePort模式对其进行访问。" class="headerlink" title="3.2.6 在k8s集群中创建mariadb命名空间，根据提供的chart包mariadb-7.3.14.tgz修改其配置，使用NodePort模式对其进行访问。"></a>3.2.6 在k8s集群中创建mariadb命名空间，根据提供的chart包mariadb-7.3.14.tgz修改其配置，使用NodePort模式对其进行访问。</h4><h3 id="任务3-公有云运维开发（10分）"><a href="#任务3-公有云运维开发（10分）" class="headerlink" title="任务3 公有云运维开发（10分）"></a>任务3 公有云运维开发（10分）</h3><h4 id="3-3-1-开发环境搭建"><a href="#3-3-1-开发环境搭建" class="headerlink" title="3.3.1 开发环境搭建"></a>3.3.1 开发环境搭建</h4><p>创建一台云主机，并登录此云服务器，安装Python3.68运行环境与SDK依赖库。</p>
<h4 id="3-3-2-安全组管理"><a href="#3-3-2-安全组管理" class="headerlink" title="3.3.2 安全组管理"></a>3.3.2 安全组管理</h4><p>调用api安全组的接口，实现安全组的增删查改。</p>
<h4 id="3-3-3-安全组规则管理"><a href="#3-3-3-安全组规则管理" class="headerlink" title="3.3.3 安全组规则管理"></a>3.3.3 安全组规则管理</h4><p>调用SDK安全组规则的方法，实现安全组规则的增删查改。</p>
<h4 id="3-3-4-云主机管理"><a href="#3-3-4-云主机管理" class="headerlink" title="3.3.4 云主机管理"></a>3.3.4 云主机管理</h4><p>调用SDK云主机管理的方法，实现云主机的的增删查改。</p>
<h4 id="3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）"><a href="#3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="3.3.5 完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）"></a>3.3.5 完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务4-边缘计算系统运维（10分）"><a href="#任务4-边缘计算系统运维（10分）" class="headerlink" title="任务4 边缘计算系统运维（10分）"></a>任务4 边缘计算系统运维（10分）</h3><h4 id="3-4-1-云端部署"><a href="#3-4-1-云端部署" class="headerlink" title="3.4.1 云端部署"></a>3.4.1 云端部署</h4><p>构建Kubernetes容器云平台，云端部署KubeEdge CloudCore云测模块，并启动cloudcore服务。</p>
<h4 id="3-4-2-边端部署"><a href="#3-4-2-边端部署" class="headerlink" title="3.4.2 边端部署"></a>3.4.2 边端部署</h4><p>在边侧部署KubeEdge EdgeCore边侧模块，并启动edgecore服务。</p>
<h4 id="3-4-3-边缘应用部署"><a href="#3-4-3-边缘应用部署" class="headerlink" title="3.4.3 边缘应用部署"></a>3.4.3 边缘应用部署</h4><p>通过边缘计算平台完成应用场景镜像部署与调试。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务5-边缘计算云应用开发（5分）"><a href="#任务5-边缘计算云应用开发（5分）" class="headerlink" title="任务5 边缘计算云应用开发（5分）"></a>任务5 边缘计算云应用开发（5分）</h3><h4 id="3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）"><a href="#3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="3.5.1 对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）"></a>3.5.1 对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）</h4><h1 id="“云计算应用”赛项赛卷2"><a href="#“云计算应用”赛项赛卷2" class="headerlink" title="“云计算应用”赛项赛卷2"></a>“云计算应用”赛项赛卷2</h1><h2 id="模块一-私有云（30分）-1"><a href="#模块一-私有云（30分）-1" class="headerlink" title="模块一 私有云（30分）"></a>模块一 私有云（30分）</h2><h3 id="任务1-私有云服务搭建（5分）-1"><a href="#任务1-私有云服务搭建（5分）-1" class="headerlink" title="任务1 私有云服务搭建（5分）"></a>任务1 私有云服务搭建（5分）</h3><h4 id="1-1-1-基础环境配置-1"><a href="#1-1-1-基础环境配置-1" class="headerlink" title="1.1.1   基础环境配置"></a>1.1.1   基础环境配置</h4><p>1.控制节点主机名为controller，设置计算节点主机名为compute；</p>
<p>2.hosts文件将IP地址映射为主机名。</p>
<h4 id="1-1-2-Yum源配置"><a href="#1-1-2-Yum源配置" class="headerlink" title="1.1.2   Yum源配置"></a>1.1.2   Yum源配置</h4><p>使用提供的http服务地址，分别设置controller节点和compute节点的Yum源文件http.repo。</p>
<h4 id="1-1-3-配置无秘钥ssh-1"><a href="#1-1-3-配置无秘钥ssh-1" class="headerlink" title="1.1.3   配置无秘钥ssh"></a>1.1.3   配置无秘钥ssh</h4><p>配置controller节点可以无秘钥访问compute节点。</p>
<h4 id="1-1-4-基础安装-1"><a href="#1-1-4-基础安装-1" class="headerlink" title="1.1.4   基础安装"></a>1.1.4   基础安装</h4><p>在控制节点和计算节点上分别安装openstack-iaas软件包。</p>
<h4 id="1-1-5-数据库安装与调优-1"><a href="#1-1-5-数据库安装与调优-1" class="headerlink" title="1.1.5   数据库安装与调优"></a>1.1.5   数据库安装与调优</h4><p>在控制节点上使用安装Mariadb、RabbitMQ等服务。并进行相关操作。</p>
<h4 id="1-1-6-Keystone服务安装与使用-1"><a href="#1-1-6-Keystone服务安装与使用-1" class="headerlink" title="1.1.6   Keystone服务安装与使用"></a>1.1.6   Keystone服务安装与使用</h4><p>在控制节点上安装Keystone服务并创建用户。</p>
<h4 id="1-1-7-Glance安装与使用-1"><a href="#1-1-7-Glance安装与使用-1" class="headerlink" title="1.1.7   Glance安装与使用"></a>1.1.7   Glance安装与使用</h4><p>在控制节点上安装Glance 服务。上传镜像至平台，并设置镜像启动的要求参数。</p>
<h4 id="1-1-8-Nova安装-1"><a href="#1-1-8-Nova安装-1" class="headerlink" title="1.1.8   Nova安装"></a>1.1.8   Nova安装</h4><p>在控制节点和计算节点上分别安装Nova服务。安装完成后，完成Nova相关配置。</p>
<h4 id="1-1-9-Neutron安装-1"><a href="#1-1-9-Neutron安装-1" class="headerlink" title="1.1.9   Neutron安装"></a>1.1.9   Neutron安装</h4><p>在控制和计算节点上正确安装Neutron服务。</p>
<h4 id="1-1-10-Dashboard安装-1"><a href="#1-1-10-Dashboard安装-1" class="headerlink" title="1.1.10  Dashboard安装"></a>1.1.10  Dashboard安装</h4><p>在控制节点上安装Dashboard服务。安装完成后，将Dashboard中的 Django数据修改为存储在文件中。</p>
<h4 id="1-1-11-Swift安装-1"><a href="#1-1-11-Swift安装-1" class="headerlink" title="1.1.11  Swift安装"></a>1.1.11  Swift安装</h4><p>在控制节点和计算节点上分别安装Swift服务。安装完成后，将cirros镜像进行分片存储。</p>
<h4 id="1-1-12-Cinder创建硬盘-1"><a href="#1-1-12-Cinder创建硬盘-1" class="headerlink" title="1.1.12  Cinder创建硬盘"></a>1.1.12  Cinder创建硬盘</h4><p>在控制节点和计算节点分别安装Cinder服务，请在计算节点，对块存储进行扩容操作。</p>
<h4 id="1-1-13-Cloudkitty服务安装与使用"><a href="#1-1-13-Cloudkitty服务安装与使用" class="headerlink" title="1.1.13  Cloudkitty服务安装与使用"></a>1.1.13  Cloudkitty服务安装与使用</h4><p>在控制节点安装cloudkitty服务，安装完毕后，启用hashmap评级模块，并设置计费规则。</p>
<h3 id="任务2-私有云服务运维（15分）-1"><a href="#任务2-私有云服务运维（15分）-1" class="headerlink" title="任务2 私有云服务运维（15分）"></a>任务2 私有云服务运维（15分）</h3><h4 id="1-2-1-OpenStack平台内存优化"><a href="#1-2-1-OpenStack平台内存优化" class="headerlink" title="1.2.1   OpenStack平台内存优化"></a>1.2.1   OpenStack平台内存优化</h4><p>搭建完OpenStack平台后，关闭系统的内存共享，打开透明大页。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">[always] madvise never</span><br><span class="line">[root@controller ~]# echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">[root@controller ~]# cat /sys/kernel/mm/transparent_hugepage/defrag </span><br><span class="line">always madvise [never]</span><br></pre></td></tr></table></figure>

<h4 id="1-2-2-修改文件句柄数"><a href="#1-2-2-修改文件句柄数" class="headerlink" title="1.2.2   修改文件句柄数"></a>1.2.2   修改文件句柄数</h4><p>修改相关参数，将控制节点的最大文件句柄数永久修改为65535。</p>
<h4 id="1-2-3-Linux系统调优-防止SYN攻击"><a href="#1-2-3-Linux系统调优-防止SYN攻击" class="headerlink" title="1.2.3   Linux系统调优-防止SYN攻击"></a>1.2.3   Linux系统调优-防止SYN攻击</h4><p>修改控制节点的相关配置文件，开启SYN cookie，防止SYN洪水攻击。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">net.ipv4.tcp_syncookies = 1</span><br></pre></td></tr></table></figure>



<h4 id="1-2-4-Keystone权限控制"><a href="#1-2-4-Keystone权限控制" class="headerlink" title="1.2.4   Keystone权限控制"></a>1.2.4   Keystone权限控制</h4><p>使用自行搭建的OpenStack私有云平台，修改普通用户权限，使普通用户不能对镜像进行创建和删除操作。</p>
<h4 id="1-2-5-Nova保持云主机状态"><a href="#1-2-5-Nova保持云主机状态" class="headerlink" title="1.2.5   Nova保持云主机状态"></a>1.2.5   Nova保持云主机状态</h4><p>修改OpenStack相关参数，使得云平台在意外断电又开启后，云主机可以保持断电前的状态。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# cat /etc/nova/nova.conf | grep resume</span><br><span class="line">resume_guests_state_on_host_boot=true</span><br></pre></td></tr></table></figure>



<h4 id="1-2-6-镜像转换"><a href="#1-2-6-镜像转换" class="headerlink" title="1.2.6   镜像转换"></a>1.2.6   镜像转换</h4><p>使用CentOS7.5-compress.qcow2 的镜像，将该镜像转换为RAW格式。</p>
<h4 id="1-2-7-使用Heat模板创建网络"><a href="#1-2-7-使用Heat模板创建网络" class="headerlink" title="1.2.7   使用Heat模板创建网络"></a>1.2.7   使用Heat模板创建网络</h4><p>在自行搭建的OpenStack私有云平台上，编写Heat模板文件，完成网络的创建。</p>
<h4 id="1-2-8-Glance镜像存储限制"><a href="#1-2-8-Glance镜像存储限制" class="headerlink" title="1.2.8   Glance镜像存储限制"></a>1.2.8   Glance镜像存储限制</h4><p>在OpenStack平台上，请修改Glance后端配置文件，将用户的镜像存储配额限制为20GB。</p>
<h4 id="1-2-9-KVM-I-x2F-O优化"><a href="#1-2-9-KVM-I-x2F-O优化" class="headerlink" title="1.2.9   KVM I&#x2F;O优化"></a>1.2.9   KVM I&#x2F;O优化</h4><p>使用自行搭建的OpenStack私有云平台，优化KVM的I&#x2F;O调度算法，将默认的模式修改为none模式。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /sys/block/vda/queue/scheduler </span><br><span class="line">[mq-deadline] kyber none</span><br><span class="line">[root@controller ~]# echo none &gt;&gt; /sys/block/vda/queue/scheduler </span><br><span class="line">[root@controller ~]# cat /sys/block/vda/queue/scheduler </span><br><span class="line">[none] mq-deadline kyber</span><br></pre></td></tr></table></figure>



<h4 id="1-2-10-Cinder限速"><a href="#1-2-10-Cinder限速" class="headerlink" title="1.2.10  Cinder限速"></a>1.2.10  Cinder限速</h4><p>请修改cinder后端配置文件将卷复制带宽限制为最高100 MiB&#x2F;s。</p>
<h4 id="1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"><a href="#1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-私有云运维开发（10分）-1"><a href="#任务3-私有云运维开发（10分）-1" class="headerlink" title="任务3 私有云运维开发（10分）"></a>任务3 私有云运维开发（10分）</h3><h4 id="1-3-1-编写Shell一键部署脚本-1"><a href="#1-3-1-编写Shell一键部署脚本-1" class="headerlink" title="1.3.1   编写Shell一键部署脚本"></a>1.3.1   编写Shell一键部署脚本</h4><p>编写一键部署owncloud云网盘应用系统。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node ~]# cat install_owncloud.sh</span><br><span class="line">#!/bin/bash</span><br><span class="line">setenforce 0</span><br><span class="line">mv /etc/yum.repos.d/* /home</span><br><span class="line">cat&gt;&gt;/etc/yum.repos.d/local.repo&lt;&lt;EOF</span><br><span class="line">[owncloud]</span><br><span class="line">name=owncloud</span><br><span class="line">baseurl=file:///root/lamp-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line">EOF</span><br><span class="line">yum install httpd mariadb mariadb-server php php-fpm php-mysqlnd php-gd php-xml php-intl php-zip php-dom php-mbstring unzip -y</span><br><span class="line">systemctl start httpd mariadb php-fpm</span><br><span class="line">mysqladmin -uroot password 123456</span><br><span class="line">mysql -uroot -p123456&lt;&lt;EOF</span><br><span class="line">create database owncloud;</span><br><span class="line">create user owncloud@localhost identified by &#x27;123456&#x27;;</span><br><span class="line">grant all privileges on owncloud.* to owncloud@localhost identified by &#x27;123456&#x27;;</span><br><span class="line">flush privileges;</span><br><span class="line">EOF</span><br><span class="line">unzip owncloud-complete.zip</span><br><span class="line">mv owncloud /var/www/html/</span><br><span class="line">chown -R apache:apache /var/www/html/owncloud</span><br></pre></td></tr></table></figure>

<h4 id="1-3-2编写Ansible脚本，部署FTP服务。"><a href="#1-3-2编写Ansible脚本，部署FTP服务。" class="headerlink" title="1.3.2编写Ansible脚本，部署FTP服务。"></a>1.3.2编写Ansible脚本，部署FTP服务。</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ftp]# cat ftp.yaml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">  - hosts: node</span><br><span class="line">    remote_user: root</span><br><span class="line">    tasks:</span><br><span class="line">     - name: install vsftpd </span><br><span class="line">       yum:</span><br><span class="line">         name: vsftpd</span><br><span class="line">     - name: start vsftpd</span><br><span class="line">       service:</span><br><span class="line">         name: vsftpd</span><br><span class="line">         state: started</span><br><span class="line">         enabled: true</span><br></pre></td></tr></table></figure>

<h4 id="1-3-3-Ansible部署zabbix服务"><a href="#1-3-3-Ansible部署zabbix服务" class="headerlink" title="1.3.3   Ansible部署zabbix服务"></a>1.3.3   Ansible部署zabbix服务</h4><p>编写Ansible脚本，部署zabbix服务。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@owncloud zabbix]# cat zabbix.yaml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">  - hosts: zabbix</span><br><span class="line">    tasks:</span><br><span class="line">      - name: stop selinux</span><br><span class="line">        shell: setenforce 0</span><br><span class="line">      - name: tar zabbix</span><br><span class="line">        unarchive:</span><br><span class="line">          src: zabbix-repo.tar.gz</span><br><span class="line">          dest: /opt/</span><br><span class="line">      - name: rm -rf /etc/yum.repos.d/*</span><br><span class="line">        shell: rm -rf /etc/yum.repos.d/*</span><br><span class="line">      - name: copy repo</span><br><span class="line">        copy:</span><br><span class="line">          src: local.repo</span><br><span class="line">          dest: /etc/yum.repos.d/</span><br><span class="line">      - name: install httpd mariadb-server mariadb</span><br><span class="line">        yum:</span><br><span class="line">          name: httpd,mariadb-server,mariadb</span><br><span class="line">      - name: start httpd mariadb</span><br><span class="line">        shell: systemctl start httpd mariadb</span><br><span class="line">      - name: install zabbix</span><br><span class="line">        yum:</span><br><span class="line">          name: zabbix-server-mysql,zabbix-web-mysql,zabbix-agent</span><br><span class="line">      - name: init mariadb</span><br><span class="line">        shell: mysqladmin -uroot password 123456</span><br><span class="line">      - name: create database</span><br><span class="line">        shell: mysql -uroot -p123456 -e &quot;create database zabbix character set utf8 collate utf8_bin;&quot;</span><br><span class="line">      - name: create user</span><br><span class="line">        shell: mysql -uroot -p123456 -e &quot;grant all privileges on zabbix.* to zabbix@localhost identified by &#x27;zabbix&#x27;;&quot;</span><br><span class="line">      - name: load sql</span><br><span class="line">        shell: zcat /usr/share/doc/zabbix-server-mysql-4.0.45/create.sql.gz | mysql -uzabbix -pzabbix zabbix</span><br><span class="line">      - name: cp zabbix-web</span><br><span class="line">        shell: cp -rf /usr/share/zabbix/* /var/www/html</span><br><span class="line">      - name: restart httpd</span><br><span class="line">        shell: systemctl restart httpd</span><br><span class="line">      - name: sed config</span><br><span class="line">        shell: sed -i &quot;2a DBPassword = zabbix&quot; /etc/zabbix/zabbix_server.conf</span><br><span class="line">      - name: start zabbix-server</span><br><span class="line">        shell: systemctl start zabbix-server</span><br></pre></td></tr></table></figure>



<h4 id="1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"><a href="#1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块二-容器云（30分）-1"><a href="#模块二-容器云（30分）-1" class="headerlink" title="模块二 容器云（30分）"></a>模块二 容器云（30分）</h2><p>企业构建Kubernetes容器云集群，引入KubeVirt实现OpenStack到Kubernetes的全面转型，用Kubernetes来管一切虚拟化运行时，包含裸金属、VM、容器。同时研发团队决定搭建基于Kubernetes 的CI&#x2F;CD环境，基于这个平台来实现DevOps流程。引入服务网格Istio，实现业务系统的灰度发布，治理和优化公司各种微服务，并开发自动化运维程序。</p>
<h3 id="任务1-容器云服务搭建（5分）-1"><a href="#任务1-容器云服务搭建（5分）-1" class="headerlink" title="任务1 容器云服务搭建（5分）"></a>任务1 容器云服务搭建（5分）</h3><h4 id="2-1-1-部署容器云平台-1"><a href="#2-1-1-部署容器云平台-1" class="headerlink" title="2.1.1   部署容器云平台"></a>2.1.1   部署容器云平台</h4><p>使用OpenStack私有云平台创建两台云主机，分别作为Kubernetes集群的master节点和node节点，然后完成Kubernetes集群的部署，并完成Istio服务网格、KubeVirt虚拟化和Harbor镜像仓库的部署。</p>
<h3 id="任务2-容器云服务运维（15分）-1"><a href="#任务2-容器云服务运维（15分）-1" class="headerlink" title="任务2 容器云服务运维（15分）"></a>任务2 容器云服务运维（15分）</h3><h4 id="2-2-1-容器化Memcache服务："><a href="#2-2-1-容器化Memcache服务：" class="headerlink" title="2.2.1   容器化Memcache服务："></a>2.2.1   容器化Memcache服务：</h4><p>编写Dockerfile文件构建memcached镜像，要求基于centos完成memcached服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-2-容器化MariaDB服务"><a href="#2-2-2-容器化MariaDB服务" class="headerlink" title="2.2.2   容器化MariaDB服务"></a>2.2.2   容器化MariaDB服务</h4><p>编写Dockerfile文件构建mysql镜像，要求基于centos完成MariaDB服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-3-容器化前端服务"><a href="#2-2-3-容器化前端服务" class="headerlink" title="2.2.3   容器化前端服务"></a>2.2.3   容器化前端服务</h4><p>编写Dockerfile文件构建nginx镜像，要求基于centos完成Nginx服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-4-容器化Blog服务"><a href="#2-2-4-容器化Blog服务" class="headerlink" title="2.2.4   容器化Blog服务"></a>2.2.4   容器化Blog服务</h4><p>编写Dockerfile文件构建blog镜像，要求基于centos完成Python3.6环境和DjangoBlog服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-5-编排部署博客系统"><a href="#2-2-5-编排部署博客系统" class="headerlink" title="2.2.5   编排部署博客系统"></a>2.2.5   编排部署博客系统</h4><p>编写docker-compose.yaml文件，要求使用镜像：memcached、mysql、nginx和blog完成Blog博客系统的编排部署。</p>
<h4 id="2-2-6-安装GitLab环境"><a href="#2-2-6-安装GitLab环境" class="headerlink" title="2.2.6   安装GitLab环境"></a>2.2.6   安装GitLab环境</h4><p>新建命名空间kube-ops，将GitLab部署到该命名空间下，然后完成GitLab服务的配置。</p>
<h4 id="2-2-7-部署GitLab-Runner"><a href="#2-2-7-部署GitLab-Runner" class="headerlink" title="2.2.7   部署GitLab Runner"></a>2.2.7   部署GitLab Runner</h4><p>将GitLab Runner部署到kube-ops命名空间下，并完成GitLab Runner在GitLab中的注册。</p>
<h4 id="2-2-8-配置GitLab"><a href="#2-2-8-配置GitLab" class="headerlink" title="2.2.8   配置GitLab"></a>2.2.8   配置GitLab</h4><p>在GitLab中新建公开项目并导入离线项目包，然后将Kubernetes集群添加到GitLab中。</p>
<h4 id="2-2-9-构建CI-x2F-CD-1"><a href="#2-2-9-构建CI-x2F-CD-1" class="headerlink" title="2.2.9   构建CI&#x2F;CD"></a>2.2.9   构建CI&#x2F;CD</h4><p>在项目中编写流水线脚本，然后触发自动构建，要求完成构建代码、构建镜像、推送镜像Harbor、并发布服务到Kubernetes集群。</p>
<h4 id="2-2-10-服务网格：创建DestinationRule"><a href="#2-2-10-服务网格：创建DestinationRule" class="headerlink" title="2.2.10  服务网格：创建DestinationRule"></a>2.2.10  服务网格：创建DestinationRule</h4><p>将Bookinfo应用部署到default命名空间下，为Bookinfo应用的四个微服务设置默认目标规则，指定各个服务的可用版本。</p>
<h4 id="2-2-11-KubeVirt运维：创建VMI"><a href="#2-2-11-KubeVirt运维：创建VMI" class="headerlink" title="2.2.11  KubeVirt运维：创建VMI"></a>2.2.11  KubeVirt运维：创建VMI</h4><p>使用提供的镜像在default命名空间下创建一台VMI，名称为exam，指定VMI的内存、CPU、启动参数等配置。</p>
<p>2.2.12  完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务3-容器云运维开发（10分）-1"><a href="#任务3-容器云运维开发（10分）-1" class="headerlink" title="任务3 容器云运维开发（10分）"></a>任务3 容器云运维开发（10分）</h3><p>2.3.1   管理job服务</p>
<p>Kubernetes Python运维脚本开发，使用SDK方式管理job服务。</p>
<h4 id="2-3-2-管理service服务"><a href="#2-3-2-管理service服务" class="headerlink" title="2.3.2   管理service服务"></a>2.3.2   管理service服务</h4><p>Kubernetes Python运维脚本开发，使用Restful APIs方式管理service服务。</p>
<h4 id="2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-1"><a href="#2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-1" class="headerlink" title="2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块三-公有云（40分）-1"><a href="#模块三-公有云（40分）-1" class="headerlink" title="模块三 公有云（40分）"></a>模块三 公有云（40分）</h2><p>企业选择国内公有云提供商，选择云主机、云网络、云硬盘、云防火墙、负载均衡等服务，可创建Web服务，共享文件存储服务，数据库服务，数据库集群等服务。搭建基于云原生的DevOps相关服务，构建云、边、端一体化的边缘计算系统，并开发云应用程序。</p>
<p>根据上述公有云平台的特性，完成公有云中的各项运维工作。</p>
<h3 id="任务1-公有云服务搭建（5分）-1"><a href="#任务1-公有云服务搭建（5分）-1" class="headerlink" title="任务1 公有云服务搭建（5分）"></a>任务1 公有云服务搭建（5分）</h3><h4 id="3-1-1-私有网络管理-1"><a href="#3-1-1-私有网络管理-1" class="headerlink" title="3.1.1   私有网络管理"></a>3.1.1   私有网络管理</h4><p>在公有云中完成虚拟私有云网络的创建。</p>
<h4 id="3-1-2-云实例管理-1"><a href="#3-1-2-云实例管理-1" class="headerlink" title="3.1.2   云实例管理"></a>3.1.2   云实例管理</h4><p>登录公有云平台，创建两台云实例虚拟机。</p>
<h4 id="3-1-3-管理数据库-1"><a href="#3-1-3-管理数据库-1" class="headerlink" title="3.1.3   管理数据库"></a>3.1.3   管理数据库</h4><p>使用intnetX-mysql网络创建两台chinaskill-sql-1和chinaskill-sql-2云服务器，并完成MongoDB安装。</p>
<h4 id="3-1-4-主从数据库-1"><a href="#3-1-4-主从数据库-1" class="headerlink" title="3.1.4   主从数据库"></a>3.1.4   主从数据库</h4><p>在chinaskill-sql-1和chinaskill-sql-2云服务器中配置MongoDB主从数据库。</p>
<h4 id="3-1-5-node环境管理-1"><a href="#3-1-5-node环境管理-1" class="headerlink" title="3.1.5   node环境管理"></a>3.1.5   node环境管理</h4><p>使用提供的压缩文件，安装Node.js环境。</p>
<h4 id="3-1-6-安全组管理-1"><a href="#3-1-6-安全组管理-1" class="headerlink" title="3.1.6   安全组管理"></a>3.1.6   安全组管理</h4><p>根据要求，创建一个安全组。</p>
<h4 id="3-1-7-RocketChat上云-1"><a href="#3-1-7-RocketChat上云-1" class="headerlink" title="3.1.7   RocketChat上云"></a>3.1.7   RocketChat上云</h4><p>使用http服务器提供文件，将Rocket.Chat应用部署上云。</p>
<h4 id="3-1-8-NAT网关-1"><a href="#3-1-8-NAT网关-1" class="headerlink" title="3.1.8   NAT网关"></a>3.1.8   NAT网关</h4><p>根据要求创建一个公网NAT网关。</p>
<h4 id="3-1-9-云服务器备份"><a href="#3-1-9-云服务器备份" class="headerlink" title="3.1.9   云服务器备份"></a>3.1.9   云服务器备份</h4><p>创建一个云服务器备份存储库名为server_backup，容量为100G。将ChinaSkill-node-1云服务器制作镜像文件chinaskill-image。</p>
<h4 id="3-1-10-负载均衡器-1"><a href="#3-1-10-负载均衡器-1" class="headerlink" title="3.1.10  负载均衡器"></a>3.1.10  负载均衡器</h4><p>根据要求创建一个负载均衡器chinaskill-elb。</p>
<h4 id="3-1-11-弹性伸缩管理-1"><a href="#3-1-11-弹性伸缩管理-1" class="headerlink" title="3.1.11  弹性伸缩管理"></a>3.1.11  弹性伸缩管理</h4><p>根据要求新建一个弹性伸缩启动配置。</p>
<h3 id="任务2-公有云服务运维（10分）-1"><a href="#任务2-公有云服务运维（10分）-1" class="headerlink" title="任务2 公有云服务运维（10分）"></a>任务2 公有云服务运维（10分）</h3><h4 id="3-2-1-云容器引擎-1"><a href="#3-2-1-云容器引擎-1" class="headerlink" title="3.2.1   云容器引擎"></a>3.2.1   云容器引擎</h4><p>在公有云上，按照要求创建一个x86架构的容器云集群。</p>
<h4 id="3-2-2-云容器管理-1"><a href="#3-2-2-云容器管理-1" class="headerlink" title="3.2.2   云容器管理"></a>3.2.2   云容器管理</h4><p>使用插件管理在kcloud容器集群中安装Dashboard可视化监控界面。</p>
<h4 id="3-2-3-使用kubectl操作集群-1"><a href="#3-2-3-使用kubectl操作集群-1" class="headerlink" title="3.2.3   使用kubectl操作集群"></a>3.2.3   使用kubectl操作集群</h4><p>在kcloud集群中安装kubectl命令，使用kubectl命令管理kcloud集群。</p>
<h4 id="3-2-4-云硬盘存储卷"><a href="#3-2-4-云硬盘存储卷" class="headerlink" title="3.2.4   云硬盘存储卷"></a>3.2.4   云硬盘存储卷</h4><p>按照要求购买云硬盘存储卷。</p>
<h4 id="3-2-5-多容器Pod管理"><a href="#3-2-5-多容器Pod管理" class="headerlink" title="3.2.5   多容器Pod管理"></a>3.2.5   多容器Pod管理</h4><p>在kcloud集群节点&#x2F;root目录下编写YAML文件mu-pod.yaml，要求一个pod中包含两个容器。</p>
<h4 id="3-2-6-Namespace管理"><a href="#3-2-6-Namespace管理" class="headerlink" title="3.2.6   Namespace管理"></a>3.2.6   Namespace管理</h4><p>在kcloud集群节点&#x2F;root目录下编写YAML文件my-namespace.yaml并创建namespace。</p>
<h3 id="任务3-公有云运维开发（10分）-1"><a href="#任务3-公有云运维开发（10分）-1" class="headerlink" title="任务3 公有云运维开发（10分）"></a>任务3 公有云运维开发（10分）</h3><h4 id="3-3-1-开发环境搭建-1"><a href="#3-3-1-开发环境搭建-1" class="headerlink" title="3.3.1   开发环境搭建"></a>3.3.1   开发环境搭建</h4><p>创建一台云主机，并登录此云服务器，安装Python3.68运行环境与SDK依赖库。</p>
<h4 id="3-3-2-虚拟私有云VPC管理"><a href="#3-3-2-虚拟私有云VPC管理" class="headerlink" title="3.3.2   虚拟私有云VPC管理"></a>3.3.2   虚拟私有云VPC管理</h4><p>调用api安全组的接口，实现VPC的增删查改。</p>
<h4 id="3-3-3-虚拟私有云VPC子网管理"><a href="#3-3-3-虚拟私有云VPC子网管理" class="headerlink" title="3.3.3   虚拟私有云VPC子网管理"></a>3.3.3   虚拟私有云VPC子网管理</h4><p>调用api安全组的接口，实现虚拟私有云VPC子网的增删查改。</p>
<h4 id="3-3-4-容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。"><a href="#3-3-4-容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。" class="headerlink" title="3.3.4   容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。"></a>3.3.4   容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。</h4><h4 id="3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-1"><a href="#3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-1" class="headerlink" title="3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）"></a>3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务4-边缘计算系统运维（10分）-1"><a href="#任务4-边缘计算系统运维（10分）-1" class="headerlink" title="任务4 边缘计算系统运维（10分）"></a>任务4 边缘计算系统运维（10分）</h3><h4 id="3-4-1-云端部署-1"><a href="#3-4-1-云端部署-1" class="headerlink" title="3.4.1   云端部署"></a>3.4.1   云端部署</h4><p>构建Kubernetes容器云平台，云端部署KubeEdge CloudCore云测模块，并启动cloudcore服务。</p>
<h4 id="3-4-2-边端部署-1"><a href="#3-4-2-边端部署-1" class="headerlink" title="3.4.2   边端部署"></a>3.4.2   边端部署</h4><p>在边侧部署KubeEdge EdgeCore边侧模块，并启动edgecore服务。</p>
<h4 id="3-4-3-边缘应用部署-1"><a href="#3-4-3-边缘应用部署-1" class="headerlink" title="3.4.3   边缘应用部署"></a>3.4.3   边缘应用部署</h4><p>通过边缘计算平台完成应用场景镜像部署与调试。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务5-边缘计算云应用开发（5分）-1"><a href="#任务5-边缘计算云应用开发（5分）-1" class="headerlink" title="任务5 边缘计算云应用开发（5分）"></a>任务5 边缘计算云应用开发（5分）</h3><h4 id="3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-1"><a href="#3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-1" class="headerlink" title="3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）"></a>3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）</h4><h1 id="“云计算应用”赛项赛卷3"><a href="#“云计算应用”赛项赛卷3" class="headerlink" title="“云计算应用”赛项赛卷3"></a>“云计算应用”赛项赛卷3</h1><h2 id="模块一-私有云（30分）-2"><a href="#模块一-私有云（30分）-2" class="headerlink" title="模块一 私有云（30分）"></a>模块一 私有云（30分）</h2><p>企业首先完成私有云平台搭建和运维，私有云平台提供云主机、云网络、云存储等基础架构云服务，并开发自动化运维程序。</p>
<h3 id="任务1-私有云服务搭建（5分）-2"><a href="#任务1-私有云服务搭建（5分）-2" class="headerlink" title="任务1 私有云服务搭建（5分）"></a>任务1 私有云服务搭建（5分）</h3><h4 id="1-1-1-基础环境配置-2"><a href="#1-1-1-基础环境配置-2" class="headerlink" title="1.1.1   基础环境配置"></a>1.1.1   基础环境配置</h4><p>1.控制节点主机名为controller，设置计算节点主机名为compute；</p>
<p>2.hosts文件将IP地址映射为主机名。</p>
<h4 id="1-1-2-yum源配置-1"><a href="#1-1-2-yum源配置-1" class="headerlink" title="1.1.2   yum源配置"></a>1.1.2   yum源配置</h4><p>使用提供的http服务地址，分别设置controller节点和compute节点的yum源文件http.repo。</p>
<h4 id="1-1-3-配置无秘钥ssh-2"><a href="#1-1-3-配置无秘钥ssh-2" class="headerlink" title="1.1.3   配置无秘钥ssh"></a>1.1.3   配置无秘钥ssh</h4><p>配置controller节点可以无秘钥访问compute节点。</p>
<h4 id="1-1-4-基础安装-2"><a href="#1-1-4-基础安装-2" class="headerlink" title="1.1.4   基础安装"></a>1.1.4   基础安装</h4><p>在控制节点和计算节点上分别安装openstack-iaas软件包。</p>
<h4 id="1-1-5-数据库安装与调优-2"><a href="#1-1-5-数据库安装与调优-2" class="headerlink" title="1.1.5   数据库安装与调优"></a>1.1.5   数据库安装与调优</h4><p>在控制节点上使用安装Mariadb、RabbitMQ等服务。并进行相关操作。</p>
<h4 id="1-1-6-Keystone服务安装与使用-2"><a href="#1-1-6-Keystone服务安装与使用-2" class="headerlink" title="1.1.6   Keystone服务安装与使用"></a>1.1.6   Keystone服务安装与使用</h4><p>在控制节点上安装Keystone服务并创建用户。</p>
<h4 id="1-1-7-Glance安装与使用-2"><a href="#1-1-7-Glance安装与使用-2" class="headerlink" title="1.1.7   Glance安装与使用"></a>1.1.7   Glance安装与使用</h4><p>在控制节点上安装Glance 服务。上传镜像至平台，并设置镜像启动的要求参数。</p>
<h4 id="1-1-8-Nova安装-2"><a href="#1-1-8-Nova安装-2" class="headerlink" title="1.1.8   Nova安装"></a>1.1.8   Nova安装</h4><p>在控制节点和计算节点上分别安装Nova服务。安装完成后，完成Nova相关配置。</p>
<h4 id="1-1-9-Neutron安装-2"><a href="#1-1-9-Neutron安装-2" class="headerlink" title="1.1.9   Neutron安装"></a>1.1.9   Neutron安装</h4><p>在控制和计算节点上正确安装Neutron服务。</p>
<h4 id="1-1-10-Dashboard安装-2"><a href="#1-1-10-Dashboard安装-2" class="headerlink" title="1.1.10  Dashboard安装"></a>1.1.10  Dashboard安装</h4><p>在控制节点上安装Dashboard服务。安装完成后，将Dashboard中的 Django数据修改为存储在文件中。</p>
<h4 id="1-1-11-Swift安装-2"><a href="#1-1-11-Swift安装-2" class="headerlink" title="1.1.11  Swift安装"></a>1.1.11  Swift安装</h4><p>在控制节点和计算节点上分别安装Swift服务。安装完成后，将cirros镜像进行分片存储。</p>
<h4 id="1-1-12-Cinder创建硬盘-2"><a href="#1-1-12-Cinder创建硬盘-2" class="headerlink" title="1.1.12  Cinder创建硬盘"></a>1.1.12  Cinder创建硬盘</h4><p>在控制节点和计算节点分别安装Cinder服务，请在计算节点，对块存储进行扩容操作。</p>
<h4 id="1-1-13-Barbican服务安装与使用"><a href="#1-1-13-Barbican服务安装与使用" class="headerlink" title="1.1.13  Barbican服务安装与使用"></a>1.1.13  Barbican服务安装与使用</h4><p>在控制节点安装barbican服务，安装服务完毕后，创建一个密钥。</p>
<h3 id="任务2-私有云服务运维（15分）-2"><a href="#任务2-私有云服务运维（15分）-2" class="headerlink" title="任务2 私有云服务运维（15分）"></a>任务2 私有云服务运维（15分）</h3><h4 id="1-2-1-Glance镜像上传"><a href="#1-2-1-Glance镜像上传" class="headerlink" title="1.2.1   Glance镜像上传"></a>1.2.1   Glance镜像上传</h4><p>在OpenStack私有云平台上，使用cirros-0.3.4-x86_64-disk.img镜像创建一个名为cirros的镜像。</p>
<h4 id="1-2-2-RPM包管理"><a href="#1-2-2-RPM包管理" class="headerlink" title="1.2.2   RPM包管理"></a>1.2.2   RPM包管理</h4><p>解决依赖包冲突，安装libguestfs-tools工具。</p>
<h4 id="1-2-3-Raid磁盘阵列管理"><a href="#1-2-3-Raid磁盘阵列管理" class="headerlink" title="1.2.3   Raid磁盘阵列管理"></a>1.2.3   Raid磁盘阵列管理</h4><p>在云主机上对云硬盘进行操作，先进行分区，然后创建名为&#x2F;dev&#x2F;md5、raid级别为5的磁盘阵列加一个热备盘。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# mdadm -C /dev/md5 -l 5 -n 3 -x 1 /dev/vdb1 /dev/vdb2 /dev/vdb3 /dev/vdb4</span><br></pre></td></tr></table></figure>



<h4 id="1-2-4-使用Heat模板创建flavor"><a href="#1-2-4-使用Heat模板创建flavor" class="headerlink" title="1.2.4   使用Heat模板创建flavor"></a>1.2.4   使用Heat模板创建flavor</h4><p>编写模板server.yaml，按要求创建云主机类型。</p>
<h4 id="1-2-5-虚拟机调整flavor"><a href="#1-2-5-虚拟机调整flavor" class="headerlink" title="1.2.5   虚拟机调整flavor"></a>1.2.5   虚拟机调整flavor</h4><p>使用OpenStack私有云平台，请修改相应配置，实现云主机调整实例大小可以使用。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">allow_resize_to_same_host=true</span><br></pre></td></tr></table></figure>



<h4 id="1-2-6-快照管理"><a href="#1-2-6-快照管理" class="headerlink" title="1.2.6   快照管理"></a>1.2.6   快照管理</h4><p>在OpenStack私有云平台上，创建云主机VM1并打快照，使用qemu-img相关命令，修改compat版本。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server image create vm1 --name csccvm</span><br><span class="line">[root@controller ~]# mkdir /root/cloudsave</span><br><span class="line">[root@controller ~]# openstack image save --file /root/cloudsave/csccvm.qcow2 csccvm</span><br><span class="line">[root@controller ~]# cd /root/cloudsave/</span><br><span class="line">[root@controller cloudsave]# qemu-img amend -f qcow2 -o compat=0.10 csccvm.qcow2</span><br></pre></td></tr></table></figure>



<h4 id="1-2-7-Swift配置Glance后端存储"><a href="#1-2-7-Swift配置Glance后端存储" class="headerlink" title="1.2.7   Swift配置Glance后端存储"></a>1.2.7   Swift配置Glance后端存储</h4><p>使用OpenStack私有云平台，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储。</p>
<h4 id="1-2-8-Glance镜像存储限制-1"><a href="#1-2-8-Glance镜像存储限制-1" class="headerlink" title="1.2.8   Glance镜像存储限制"></a>1.2.8   Glance镜像存储限制</h4><p>在OpenStack平台上，请修改Glance后端配置文件，将用户的镜像存储配额限制为20GB。</p>
<h4 id="1-2-9-RabbitMQ集群部署"><a href="#1-2-9-RabbitMQ集群部署" class="headerlink" title="1.2.9   RabbitMQ集群部署"></a>1.2.9   RabbitMQ集群部署</h4><p>使用OpenStack私有云平台，创建三个云主机搭建RabbitMQ集群。</p>
<h4 id="1-2-10-云平台安全策略提升"><a href="#1-2-10-云平台安全策略提升" class="headerlink" title="1.2.10  云平台安全策略提升"></a>1.2.10  云平台安全策略提升</h4><p>使用OpenStack，安装必要组件，将私有云平台的访问策略从http提升至https。</p>
<p><code>yum -y install mod_ssl</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/openstack-dashboard/local_settings</span><br><span class="line">##在DEBUG = False下增加4行</span><br><span class="line">USE_SSL = True</span><br><span class="line">CSRF_COOKIE_SECURE = True                              ##原文中有，去掉注释即可</span><br><span class="line">SESSION_COOKIE_SECURE = True                       ##原文中有，去掉注释即可</span><br><span class="line">SESSION_COOKIE_HTTPONLY = True</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/httpd/conf.d/ssl.conf</span><br><span class="line">##将SSLProtocol all -SSLv2 -SSLv3改成：</span><br><span class="line">SSLProtocol all -SSLv2</span><br></pre></td></tr></table></figure>

<p> <code>systemctl restart httpd</code></p>
<p><code>systemctl restart memcached</code></p>
<h4 id="1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-1"><a href="#1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-1" class="headerlink" title="1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-私有云运维开发（10分）-2"><a href="#任务3-私有云运维开发（10分）-2" class="headerlink" title="任务3 私有云运维开发（10分）"></a>任务3 私有云运维开发（10分）</h3><h4 id="1-3-1-编写Shell脚本备份数据库"><a href="#1-3-1-编写Shell脚本备份数据库" class="headerlink" title="1.3.1   编写Shell脚本备份数据库"></a>1.3.1   编写Shell脚本备份数据库</h4><p>编写数据库的定期备份shell脚本。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@zookeeper-1 ~]# cat mysql_back.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">datetime=`date +%Y-%m-%d-%H%M%S`</span><br><span class="line">echo $datetime</span><br><span class="line">mysqldump -uroot -p123456 --all-database &gt;&gt; $datetime.sql</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">root@zookeeper-1 ~]# crontab -e </span><br><span class="line">0 23 * * * /bin/bash /root/mysql_back.sh</span><br></pre></td></tr></table></figure>

<h4 id="1-3-2-Ansible部署MariaDB服务"><a href="#1-3-2-Ansible部署MariaDB服务" class="headerlink" title="1.3.2   Ansible部署MariaDB服务"></a>1.3.2   Ansible部署MariaDB服务</h4><p>编写Ansible脚本，部署MariaDB服务。</p>
<h4 id="1-3-3-Ansible部署ELK服务"><a href="#1-3-3-Ansible部署ELK服务" class="headerlink" title="1.3.3   Ansible部署ELK服务"></a>1.3.3   Ansible部署ELK服务</h4><p>编写Playbook，部署的ELK。</p>
<h4 id="1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-1"><a href="#1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-1" class="headerlink" title="1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块二-容器云（30分）-2"><a href="#模块二-容器云（30分）-2" class="headerlink" title="模块二 容器云（30分）"></a>模块二 容器云（30分）</h2><p>企业构建Kubernetes容器云集群，引入KubeVirt实现OpenStack到Kubernetes的全面转型，用Kubernetes来管一切虚拟化运行时，包含裸金属、VM、容器。同时研发团队决定搭建基于Kubernetes 的CI&#x2F;CD环境，基于这个平台来实现DevOps流程。引入服务网格Istio，实现业务系统的灰度发布，治理和优化公司各种微服务，并开发自动化运维程序。</p>
<h3 id="任务1-容器云服务搭建（5分）-2"><a href="#任务1-容器云服务搭建（5分）-2" class="headerlink" title="任务1 容器云服务搭建（5分）"></a>任务1 容器云服务搭建（5分）</h3><h4 id="2-1-1-部署容器云平台-2"><a href="#2-1-1-部署容器云平台-2" class="headerlink" title="2.1.1   部署容器云平台"></a>2.1.1   部署容器云平台</h4><p>使用OpenStack私有云平台创建两台云主机，分别作为Kubernetes集群的master节点和node节点，然后完成Kubernetes集群的部署，并完成Istio服务网格、KubeVirt虚拟化和Harbor镜像仓库的部署。</p>
<h3 id="任务2-容器云服务运维（15分）-2"><a href="#任务2-容器云服务运维（15分）-2" class="headerlink" title="任务2 容器云服务运维（15分）"></a>任务2 容器云服务运维（15分）</h3><h4 id="2-2-1-容器化部署MariaDB："><a href="#2-2-1-容器化部署MariaDB：" class="headerlink" title="2.2.1   容器化部署MariaDB："></a>2.2.1   容器化部署MariaDB：</h4><p>编写Dockerfile文件构建mariadb镜像，要求基于centos完成MariaDB服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-2-容器化部署Redis"><a href="#2-2-2-容器化部署Redis" class="headerlink" title="2.2.2   容器化部署Redis"></a>2.2.2   容器化部署Redis</h4><p>编写Dockerfile文件构建redis镜像，要求基于centos完成Redis服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-3-容器化部署Pig"><a href="#2-2-3-容器化部署Pig" class="headerlink" title="2.2.3   容器化部署Pig"></a>2.2.3   容器化部署Pig</h4><p>编写Dockerfile文件构建pig镜像，要求基于centos完成JDK环境和Pig服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-4-容器化部署Nginx"><a href="#2-2-4-容器化部署Nginx" class="headerlink" title="2.2.4   容器化部署Nginx"></a>2.2.4   容器化部署Nginx</h4><p>编写Dockerfile文件构建nginx镜像，要求基于centos完成Nginx服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-5-编排部署Pig快发开发平台"><a href="#2-2-5-编排部署Pig快发开发平台" class="headerlink" title="2.2.5   编排部署Pig快发开发平台"></a>2.2.5   编排部署Pig快发开发平台</h4><p>编写docker-compose.yaml文件，要求使用镜像mariadb、redis、pig和nginx完成Pig快速开发平台的编排部署。</p>
<h4 id="2-2-6-部署GitLab"><a href="#2-2-6-部署GitLab" class="headerlink" title="2.2.6   部署GitLab"></a>2.2.6   部署GitLab</h4><p>新建命名空间devops，使用Deployment将GitLab部署到该命名空间下，并完成GitLab服务的初始化配置。</p>
<h4 id="2-2-7-配置GitLab"><a href="#2-2-7-配置GitLab" class="headerlink" title="2.2.7   配置GitLab"></a>2.2.7   配置GitLab</h4><p>在GitLab中创建一个名为drone的GitLab OAuth应用程序，创建一个新项目，并将提供的项目包导入到该项目中。</p>
<h4 id="2-2-8-部署Drone"><a href="#2-2-8-部署Drone" class="headerlink" title="2.2.8   部署Drone"></a>2.2.8   部署Drone</h4><p>使用Deployment将Drone服务部署到devops命名空间下，并使用Service暴露服务。</p>
<h4 id="2-2-9-构建CI-x2F-CD-2"><a href="#2-2-9-构建CI-x2F-CD-2" class="headerlink" title="2.2.9   构建CI&#x2F;CD"></a>2.2.9   构建CI&#x2F;CD</h4><p>编写流水线脚本，触发流水线自动构建，完成流水线的构建，构建成功后访问自动发布的服务。</p>
<h4 id="2-2-10-服务网格：创建VirtualService"><a href="#2-2-10-服务网格：创建VirtualService" class="headerlink" title="2.2.10  服务网格：创建VirtualService"></a>2.2.10  服务网格：创建VirtualService</h4><p>将Bookinfo应用部署到default命名空间下，为Bookinfo应用的四个微服务设置默认版本的VirtualService，将所有流量路由到每个微服务的v1版本。</p>
<h4 id="2-2-11-KubeVirt运维：创建VM-1"><a href="#2-2-11-KubeVirt运维：创建VM-1" class="headerlink" title="2.2.11  KubeVirt运维：创建VM"></a>2.2.11  KubeVirt运维：创建VM</h4><p>使用云端镜像在default命名空间下创建一台VM，名称为exam，指定VM的内存、CPU、运行策略、启动参数等配置。</p>
<h4 id="2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-1"><a href="#2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-1" class="headerlink" title="2.2.12  完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>2.2.12  完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-容器云运维开发（10分）-2"><a href="#任务3-容器云运维开发（10分）-2" class="headerlink" title="任务3 容器云运维开发（10分）"></a>任务3 容器云运维开发（10分）</h3><h4 id="2-3-1-管理Pod服务"><a href="#2-3-1-管理Pod服务" class="headerlink" title="2.3.1   管理Pod服务"></a>2.3.1   管理Pod服务</h4><p>Kubernetes Python运维脚本开发-使用SDK方式管理Pod服务。</p>
<h4 id="2-3-2-自定义调度器-1"><a href="#2-3-2-自定义调度器-1" class="headerlink" title="2.3.2   自定义调度器"></a>2.3.2   自定义调度器</h4><p>Kubernetes Python运维脚本开发-使用Restful API方式管理调度器。</p>
<h4 id="2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-2"><a href="#2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-2" class="headerlink" title="2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块三-公有云（40分）-2"><a href="#模块三-公有云（40分）-2" class="headerlink" title="模块三 公有云（40分）"></a>模块三 公有云（40分）</h2><p>企业选择国内公有云提供商，选择云主机、云网络、云硬盘、云防火墙、负载均衡等服务，可创建Web服务，共享文件存储服务，数据库服务，数据库集群等服务。搭建基于云原生的DevOps相关服务，构建云、边、端一体化的边缘计算系统，并开发云应用程序。</p>
<p>根据上述公有云平台的特性，完成公有云中的各项运维工作。</p>
<h3 id="任务1-公有云服务搭建（5分）-2"><a href="#任务1-公有云服务搭建（5分）-2" class="headerlink" title="任务1 公有云服务搭建（5分）"></a>任务1 公有云服务搭建（5分）</h3><h4 id="3-1-1-私有网络管理-2"><a href="#3-1-1-私有网络管理-2" class="headerlink" title="3.1.1   私有网络管理"></a>3.1.1   私有网络管理</h4><p>在公有云中完成虚拟私有云网络的创建。</p>
<h4 id="3-1-2-云实例管理-2"><a href="#3-1-2-云实例管理-2" class="headerlink" title="3.1.2   云实例管理"></a>3.1.2   云实例管理</h4><p>登录公有云平台，创建两台云实例虚拟机。</p>
<h4 id="3-1-3-管理数据库-2"><a href="#3-1-3-管理数据库-2" class="headerlink" title="3.1.3   管理数据库"></a>3.1.3   管理数据库</h4><p>使用intnetX-mysql网络创建两台chinaskill-sql-1和chinaskill-sql-2云服务器，并完成MongoDB安装。</p>
<h4 id="3-1-4-主从数据库-2"><a href="#3-1-4-主从数据库-2" class="headerlink" title="3.1.4   主从数据库"></a>3.1.4   主从数据库</h4><p>在chinaskill-sql-1和chinaskill-sql-2云服务器中配置MongoDB主从数据库。</p>
<h4 id="3-1-5-node环境管理-2"><a href="#3-1-5-node环境管理-2" class="headerlink" title="3.1.5   node环境管理"></a>3.1.5   node环境管理</h4><p>使用提供的压缩文件，安装Node.js环境。</p>
<h4 id="3-1-6-安全组管理-2"><a href="#3-1-6-安全组管理-2" class="headerlink" title="3.1.6   安全组管理"></a>3.1.6   安全组管理</h4><p>根据要求，创建一个安全组。</p>
<h4 id="3-1-7-RocketChat上云-2"><a href="#3-1-7-RocketChat上云-2" class="headerlink" title="3.1.7   RocketChat上云"></a>3.1.7   RocketChat上云</h4><p>使用http服务器提供文件，将Rocket.Chat应用部署上云。</p>
<h4 id="3-1-8-NAT网关-2"><a href="#3-1-8-NAT网关-2" class="headerlink" title="3.1.8   NAT网关"></a>3.1.8   NAT网关</h4><p>根据要求创建一个公网NAT网关。</p>
<h4 id="3-1-9-云服务器备份-1"><a href="#3-1-9-云服务器备份-1" class="headerlink" title="3.1.9   云服务器备份"></a>3.1.9   云服务器备份</h4><p>创建一个云服务器备份存储库名为server_backup，容量为100G。将ChinaSkill-node-1云服务器制作镜像文件chinaskill-image。</p>
<h4 id="3-1-10-负载均衡器-2"><a href="#3-1-10-负载均衡器-2" class="headerlink" title="3.1.10  负载均衡器"></a>3.1.10  负载均衡器</h4><p>根据要求创建一个负载均衡器chinaskill-elb。</p>
<h4 id="3-1-11-弹性伸缩管理-2"><a href="#3-1-11-弹性伸缩管理-2" class="headerlink" title="3.1.11  弹性伸缩管理"></a>3.1.11  弹性伸缩管理</h4><p>根据要求新建一个弹性伸缩启动配置。</p>
<h3 id="任务2-公有云服务运维（10分）-2"><a href="#任务2-公有云服务运维（10分）-2" class="headerlink" title="任务2 公有云服务运维（10分）"></a>任务2 公有云服务运维（10分）</h3><h4 id="3-2-1-云容器引擎-2"><a href="#3-2-1-云容器引擎-2" class="headerlink" title="3.2.1   云容器引擎"></a>3.2.1   云容器引擎</h4><p>在公有云上，按照要求创建一个x86架构的容器云集群。</p>
<h4 id="3-2-2-云容器管理-2"><a href="#3-2-2-云容器管理-2" class="headerlink" title="3.2.2   云容器管理"></a>3.2.2   云容器管理</h4><p>使用插件管理在kcloud容器集群中安装Dashboard可视化监控界面。</p>
<h4 id="3-2-3-使用kubectl操作集群-2"><a href="#3-2-3-使用kubectl操作集群-2" class="headerlink" title="3.2.3   使用kubectl操作集群"></a>3.2.3   使用kubectl操作集群</h4><p>在kcloud集群中安装kubectl命令，使用kubectl命令管理kcloud集群。</p>
<h4 id="3-2-4-安装Helm-1"><a href="#3-2-4-安装Helm-1" class="headerlink" title="3.2.4   安装Helm"></a>3.2.4   安装Helm</h4><p>使用提供的Helm软件包，在kcloud集群中安装Helm服务。</p>
<h4 id="3-2-5-Secrets管理–Opaque"><a href="#3-2-5-Secrets管理–Opaque" class="headerlink" title="3.2.5   Secrets管理–Opaque"></a>3.2.5   Secrets管理–Opaque</h4><p>在master节点&#x2F;root目录下编写YAML文件secret.yaml，要求执行文件创建密钥。</p>
<h4 id="3-2-6-私有仓库管理"><a href="#3-2-6-私有仓库管理" class="headerlink" title="3.2.6   私有仓库管理"></a>3.2.6   私有仓库管理</h4><p>在master节点添加搭建的本地私有chart仓库源，并上传wordpress-13.0.23.tgz包至chartmuseum私有仓库中。可以使用本地仓库chart源部署应用。</p>
<h3 id="任务3-公有云运维开发（10分）-2"><a href="#任务3-公有云运维开发（10分）-2" class="headerlink" title="任务3 公有云运维开发（10分）"></a>任务3 公有云运维开发（10分）</h3><h4 id="3-3-1-开发环境搭建-2"><a href="#3-3-1-开发环境搭建-2" class="headerlink" title="3.3.1   开发环境搭建"></a>3.3.1   开发环境搭建</h4><p>创建一台云主机，并登录此云服务器，安装Python3.68运行环境与SDK依赖库。</p>
<h4 id="3-3-2-密钥对管理"><a href="#3-3-2-密钥对管理" class="headerlink" title="3.3.2   密钥对管理"></a>3.3.2   密钥对管理</h4><p>编写Python代码，实现密钥对的创建。</p>
<h4 id="3-3-4-云硬盘管理"><a href="#3-3-4-云硬盘管理" class="headerlink" title="3.3.4   云硬盘管理"></a>3.3.4   云硬盘管理</h4><p>调用SDK云硬盘管理的方法，实现云主机的的增删查改。</p>
<h4 id="3-3-4-云主机管理-1"><a href="#3-3-4-云主机管理-1" class="headerlink" title="3.3.4   云主机管理"></a>3.3.4   云主机管理</h4><p>调用SDK云主机管理的方法，实现云主机的的增删查改。</p>
<p>3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务4-边缘计算系统运维（10分）-2"><a href="#任务4-边缘计算系统运维（10分）-2" class="headerlink" title="任务4 边缘计算系统运维（10分）"></a>任务4 边缘计算系统运维（10分）</h3><h4 id="3-4-1-云端部署-2"><a href="#3-4-1-云端部署-2" class="headerlink" title="3.4.1   云端部署"></a>3.4.1   云端部署</h4><p>构建Kubernetes容器云平台，云端部署KubeEdge CloudCore云测模块，并启动cloudcore服务。</p>
<h4 id="3-4-2-边端部署-2"><a href="#3-4-2-边端部署-2" class="headerlink" title="3.4.2   边端部署"></a>3.4.2   边端部署</h4><p>在边侧部署KubeEdge EdgeCore边侧模块，并启动edgecore服务。</p>
<h4 id="3-4-3-边缘应用部署-2"><a href="#3-4-3-边缘应用部署-2" class="headerlink" title="3.4.3   边缘应用部署"></a>3.4.3   边缘应用部署</h4><p>通过边缘计算平台完成应用场景镜像部署与调试。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务5-边缘计算云应用开发（5分）-2"><a href="#任务5-边缘计算云应用开发（5分）-2" class="headerlink" title="任务5 边缘计算云应用开发（5分）"></a>任务5 边缘计算云应用开发（5分）</h3><h4 id="3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-2"><a href="#3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-2" class="headerlink" title="3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）"></a>3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）</h4><h1 id="“云计算应用”赛项赛卷4"><a href="#“云计算应用”赛项赛卷4" class="headerlink" title="“云计算应用”赛项赛卷4"></a>“云计算应用”赛项赛卷4</h1><h2 id="模块一-私有云（30分）-3"><a href="#模块一-私有云（30分）-3" class="headerlink" title="模块一 私有云（30分）"></a>模块一 私有云（30分）</h2><p>企业首先完成私有云平台搭建和运维，私有云平台提供云主机、云网络、云存储等基础架构云服务，并开发自动化运维程序。</p>
<h3 id="任务1-私有云服务搭建（5分）-3"><a href="#任务1-私有云服务搭建（5分）-3" class="headerlink" title="任务1 私有云服务搭建（5分）"></a>任务1 私有云服务搭建（5分）</h3><h4 id="1-1-1-基础环境配置-3"><a href="#1-1-1-基础环境配置-3" class="headerlink" title="1.1.1   基础环境配置"></a>1.1.1   基础环境配置</h4><p>1.控制节点主机名为controller，设置计算节点主机名为compute；</p>
<p>2.hosts文件将IP地址映射为主机名。</p>
<h4 id="1-1-2-yum源配置-2"><a href="#1-1-2-yum源配置-2" class="headerlink" title="1.1.2   yum源配置"></a>1.1.2   yum源配置</h4><p>使用提供的http服务地址，分别设置controller节点和compute节点的yum源文件http.repo。</p>
<h4 id="1-1-3-配置无秘钥ssh-3"><a href="#1-1-3-配置无秘钥ssh-3" class="headerlink" title="1.1.3   配置无秘钥ssh"></a>1.1.3   配置无秘钥ssh</h4><p>配置controller节点可以无秘钥访问compute节点。</p>
<h4 id="1-1-4-基础安装-3"><a href="#1-1-4-基础安装-3" class="headerlink" title="1.1.4   基础安装"></a>1.1.4   基础安装</h4><p>在控制节点和计算节点上分别安装openstack-iaas软件包。</p>
<h4 id="1-1-5-数据库安装与调优-3"><a href="#1-1-5-数据库安装与调优-3" class="headerlink" title="1.1.5   数据库安装与调优"></a>1.1.5   数据库安装与调优</h4><p>在控制节点上使用安装Mariadb、RabbitMQ等服务。并进行相关操作。</p>
<h4 id="1-1-6-Keystone服务安装与使用-3"><a href="#1-1-6-Keystone服务安装与使用-3" class="headerlink" title="1.1.6   Keystone服务安装与使用"></a>1.1.6   Keystone服务安装与使用</h4><p>在控制节点上安装Keystone服务并创建用户。</p>
<h4 id="1-1-7-Glance安装与使用-3"><a href="#1-1-7-Glance安装与使用-3" class="headerlink" title="1.1.7   Glance安装与使用"></a>1.1.7   Glance安装与使用</h4><p>在控制节点上安装Glance 服务。上传镜像至平台，并设置镜像启动的要求参数。</p>
<h4 id="1-1-8-Nova安装-3"><a href="#1-1-8-Nova安装-3" class="headerlink" title="1.1.8   Nova安装"></a>1.1.8   Nova安装</h4><p>在控制节点和计算节点上分别安装Nova服务。安装完成后，完成Nova相关配置。</p>
<h4 id="1-1-9-Neutron安装-3"><a href="#1-1-9-Neutron安装-3" class="headerlink" title="1.1.9   Neutron安装"></a>1.1.9   Neutron安装</h4><p>在控制和计算节点上正确安装Neutron服务。</p>
<h4 id="1-1-10-Dashboard安装-3"><a href="#1-1-10-Dashboard安装-3" class="headerlink" title="1.1.10  Dashboard安装"></a>1.1.10  Dashboard安装</h4><p>在控制节点上安装Dashboard服务。安装完成后，将Dashboard中的 Django数据修改为存储在文件中。</p>
<h4 id="1-1-11-Swift安装-3"><a href="#1-1-11-Swift安装-3" class="headerlink" title="1.1.11  Swift安装"></a>1.1.11  Swift安装</h4><p>在控制节点和计算节点上分别安装Swift服务。安装完成后，将cirros镜像进行分片存储。</p>
<h4 id="1-1-12-Cinder创建硬盘-3"><a href="#1-1-12-Cinder创建硬盘-3" class="headerlink" title="1.1.12  Cinder创建硬盘"></a>1.1.12  Cinder创建硬盘</h4><p>在控制节点和计算节点分别安装Cinder服务，请在计算节点，对块存储进行扩容操作。</p>
<h4 id="1-1-13-Cloudkitty服务安装与使用-1"><a href="#1-1-13-Cloudkitty服务安装与使用-1" class="headerlink" title="1.1.13  Cloudkitty服务安装与使用"></a>1.1.13  Cloudkitty服务安装与使用</h4><p>在控制节点安装cloudkitty服务，安装完毕后，启用hashmap评级模块，并设置计费规则。</p>
<h3 id="任务2-私有云服务运维（15分）-3"><a href="#任务2-私有云服务运维（15分）-3" class="headerlink" title="任务2 私有云服务运维（15分）"></a>任务2 私有云服务运维（15分）</h3><h4 id="1-2-1-使用Heat模板创建用户"><a href="#1-2-1-使用Heat模板创建用户" class="headerlink" title="1.2.1   使用Heat模板创建用户"></a>1.2.1   使用Heat模板创建用户</h4><p>编写Heat模板create_user.yaml，创建名为heat-user的用户。</p>
<h4 id="1-2-2-KVM优化"><a href="#1-2-2-KVM优化" class="headerlink" title="1.2.2   KVM优化"></a>1.2.2   KVM优化</h4><p>在OpenStack平台上修改相关配置文件，启用-device virtio-net-pci in kvm。</p>
<h4 id="1-2-3-NFS对接Glance后端存储"><a href="#1-2-3-NFS对接Glance后端存储" class="headerlink" title="1.2.3   NFS对接Glance后端存储"></a>1.2.3   NFS对接Glance后端存储</h4><p>使用OpenStack私有云平台，创建一台云主机，安装NFS服务，然后对接Glance后端存储。</p>
<h4 id="1-2-4-Redis主从"><a href="#1-2-4-Redis主从" class="headerlink" title="1.2.4   Redis主从"></a>1.2.4   Redis主从</h4><p>使用赛项提供的OpenStack私有云平台，创建两台云主机，配置为redis的主从架构。</p>
<h4 id="1-2-5-Linux系统调优-脏数据回写"><a href="#1-2-5-Linux系统调优-脏数据回写" class="headerlink" title="1.2.5   Linux系统调优-脏数据回写"></a>1.2.5   Linux系统调优-脏数据回写</h4><p>修改系统配置文件，要求将回写磁盘的时间临时调整为60秒。</p>
<h4 id="1-2-6-Glance调优"><a href="#1-2-6-Glance调优" class="headerlink" title="1.2.6   Glance调优"></a>1.2.6   Glance调优</h4><p>在OpenStack平台中，修改相关配置文件，将子进程数量相应的配置修改成2。</p>
<h4 id="1-2-7-Cinder数据加密"><a href="#1-2-7-Cinder数据加密" class="headerlink" title="1.2.7   Cinder数据加密"></a>1.2.7   Cinder数据加密</h4><p>使用自行创建的OpenStack云计算平台，通过相关配置，开启Cinder块存储的数据加密功能。</p>
<h4 id="1-2-8-Linux内核优化"><a href="#1-2-8-Linux内核优化" class="headerlink" title="1.2.8   Linux内核优化"></a>1.2.8   Linux内核优化</h4><p>修改相应的配置文件，对Linux系统进行内核优化操作。</p>
<h4 id="1-2-9-JumpServer堡垒机部署-1"><a href="#1-2-9-JumpServer堡垒机部署-1" class="headerlink" title="1.2.9   JumpServer堡垒机部署"></a>1.2.9   JumpServer堡垒机部署</h4><p>使用提供的软件包安装JumpServer堡垒机服务，并配置使用该堡垒机对接自己安装的控制和计算节点。</p>
<h4 id="1-2-10-SkyWalking-应用部署"><a href="#1-2-10-SkyWalking-应用部署" class="headerlink" title="1.2.10  SkyWalking 应用部署"></a>1.2.10  SkyWalking 应用部署</h4><p>申请一台云主机，使用提供的软件包安装Elasticsearch服务和SkyWalking服务。再申请一台云主机，用于搭建gpmall商城应用，并配置SkyWalking 监控gpmall主机。</p>
<h4 id="1-2-101完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"><a href="#1-2-101完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="1.2.101完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>1.2.101完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-私有云运维开发（10分）-3"><a href="#任务3-私有云运维开发（10分）-3" class="headerlink" title="任务3 私有云运维开发（10分）"></a>任务3 私有云运维开发（10分）</h3><h4 id="1-3-1-编写Shell一键部署脚本-2"><a href="#1-3-1-编写Shell一键部署脚本-2" class="headerlink" title="1.3.1   编写Shell一键部署脚本"></a>1.3.1   编写Shell一键部署脚本</h4><p>编写一键部署nfs云网盘应用系统。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@zookeeper-1 ~]# cat nfs.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">mkdir -p /data/NFS</span><br><span class="line">yum install -y nfs-utils rpcbind</span><br><span class="line">cat &gt; /etc/exports &lt;&lt; EOF</span><br><span class="line">/data/NFS *(rw,async,no_root_squash)</span><br><span class="line">EOF</span><br><span class="line">systemctl restart nfs rpcbind</span><br><span class="line">systemctl enable nfs rpcbind</span><br></pre></td></tr></table></figure>

<h4 id="1-3-2-Ansible部署MariaDB服务-1"><a href="#1-3-2-Ansible部署MariaDB服务-1" class="headerlink" title="1.3.2   Ansible部署MariaDB服务"></a>1.3.2   Ansible部署MariaDB服务</h4><p>编写Ansible脚本，部署MariaDB服务。</p>
<h4 id="1-3-3-Ansible部署zabbix服务-1"><a href="#1-3-3-Ansible部署zabbix服务-1" class="headerlink" title="1.3.3   Ansible部署zabbix服务"></a>1.3.3   Ansible部署zabbix服务</h4><p>编写Ansible脚本，部署zabbix服务。</p>
<h4 id="1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-2"><a href="#1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-2" class="headerlink" title="1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块二-容器云（30分）-3"><a href="#模块二-容器云（30分）-3" class="headerlink" title="模块二 容器云（30分）"></a>模块二 容器云（30分）</h2><p>企业构建Kubernetes容器云集群，引入KubeVirt实现OpenStack到Kubernetes的全面转型，用Kubernetes来管一切虚拟化运行时，包含裸金属、VM、容器。同时研发团队决定搭建基于Kubernetes 的CI&#x2F;CD环境，基于这个平台来实现DevOps流程。引入服务网格Istio，实现业务系统的灰度发布，治理和优化公司各种微服务，并开发自动化运维程序。</p>
<h3 id="任务1-容器云服务搭建（5分）-3"><a href="#任务1-容器云服务搭建（5分）-3" class="headerlink" title="任务1 容器云服务搭建（5分）"></a>任务1 容器云服务搭建（5分）</h3><h4 id="2-1-1-部署容器云平台-3"><a href="#2-1-1-部署容器云平台-3" class="headerlink" title="2.1.1   部署容器云平台"></a>2.1.1   部署容器云平台</h4><p>使用OpenStack私有云平台创建两台云主机，分别作为Kubernetes集群的master节点和node节点，然后完成Kubernetes集群的部署，并完成Istio服务网格、KubeVirt虚拟化和Harbor镜像仓库的部署。</p>
<h3 id="任务2-容器云服务运维（15分）-3"><a href="#任务2-容器云服务运维（15分）-3" class="headerlink" title="任务2 容器云服务运维（15分）"></a>任务2 容器云服务运维（15分）</h3><h4 id="2-2-1-容器化部署Node-Exporter-1"><a href="#2-2-1-容器化部署Node-Exporter-1" class="headerlink" title="2.2.1   容器化部署Node-Exporter"></a>2.2.1   容器化部署Node-Exporter</h4><p>编写Dockerfile文件构建exporter镜像，要求基于centos完成Node-Exporter服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-2-容器化部署Alertmanager-1"><a href="#2-2-2-容器化部署Alertmanager-1" class="headerlink" title="2.2.2   容器化部署Alertmanager"></a>2.2.2   容器化部署Alertmanager</h4><p>编写Dockerfile文件构建alert镜像，要求基于centos：latest完成Alertmanager服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-3-容器化部署Grafana-1"><a href="#2-2-3-容器化部署Grafana-1" class="headerlink" title="2.2.3   容器化部署Grafana"></a>2.2.3   容器化部署Grafana</h4><p>编写Dockerfile文件构建grafana镜像，要求基于centos完成Grafana服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-4-容器化部署Prometheus-1"><a href="#2-2-4-容器化部署Prometheus-1" class="headerlink" title="2.2.4   容器化部署Prometheus"></a>2.2.4   容器化部署Prometheus</h4><p>编写Dockerfile文件构建prometheus镜像，要求基于centos完成Promethues服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-5-编排部署监控系统-1"><a href="#2-2-5-编排部署监控系统-1" class="headerlink" title="2.2.5   编排部署监控系统"></a>2.2.5   编排部署监控系统</h4><p>编写docker-compose.yaml文件，使用镜像exporter、alert、grafana和prometheus完成监控系统的编排部署。</p>
<h4 id="2-2-6-部署GitLab-1"><a href="#2-2-6-部署GitLab-1" class="headerlink" title="2.2.6   部署GitLab"></a>2.2.6   部署GitLab</h4><p>新建命名空间devops，使用Deployment将GitLab部署到该命名空间下，并完成GitLab服务的初始化配置。</p>
<h4 id="2-2-7-配置GitLab-1"><a href="#2-2-7-配置GitLab-1" class="headerlink" title="2.2.7   配置GitLab"></a>2.2.7   配置GitLab</h4><p>在GitLab中创建一个名为drone的GitLab OAuth应用程序，创建一个新项目，并将提供的项目包导入到该项目中。</p>
<h4 id="2-2-8-部署Drone-1"><a href="#2-2-8-部署Drone-1" class="headerlink" title="2.2.8   部署Drone"></a>2.2.8   部署Drone</h4><p>使用Deployment将Drone服务部署到devops命名空间下，并使用Service暴露服务。</p>
<h4 id="2-2-9-构建CI-x2F-CD-3"><a href="#2-2-9-构建CI-x2F-CD-3" class="headerlink" title="2.2.9   构建CI&#x2F;CD"></a>2.2.9   构建CI&#x2F;CD</h4><p>编写流水线脚本，触发流水线自动构建，完成流水线的构建，构建成功后访问自动发布的服务。 </p>
<h4 id="2-2-10-服务网格：创建VirtualService-1"><a href="#2-2-10-服务网格：创建VirtualService-1" class="headerlink" title="2.2.10  服务网格：创建VirtualService"></a>2.2.10  服务网格：创建VirtualService</h4><p>将Bookinfo应用部署到default命名空间下，为Bookinfo应用创建一个名为reviews的VirtualService，要求来自指定用户的所有流量将被路由到reviews服务的v2版本。</p>
<h4 id="2-2-11-KubeVirt运维：快照管理"><a href="#2-2-11-KubeVirt运维：快照管理" class="headerlink" title="2.2.11  KubeVirt运维：快照管理"></a>2.2.11  KubeVirt运维：快照管理</h4><p>使用提供的镜像在default命名空间下创建一台VM，名称为exam，指定VM的配置信息，并为VM创建名为exam的快照。</p>
<h4 id="2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"><a href="#2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）" class="headerlink" title="2.12   完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>2.12   完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-容器云运维开发（10分）-3"><a href="#任务3-容器云运维开发（10分）-3" class="headerlink" title="任务3 容器云运维开发（10分）"></a>任务3 容器云运维开发（10分）</h3><h4 id="2-3-1-管理service资源"><a href="#2-3-1-管理service资源" class="headerlink" title="2.3.1   管理service资源"></a>2.3.1   管理service资源</h4><p>Kubernetes Python运维脚本开发，使用Restful APIs方式管理service服务。</p>
<h4 id="2-3-2-管理Pod资源"><a href="#2-3-2-管理Pod资源" class="headerlink" title="2.3.2   管理Pod资源"></a>2.3.2   管理Pod资源</h4><p>Kubernetes Python运维脚本开发，使用SDK方式管理Pod服务。</p>
<h4 id="2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-3"><a href="#2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-3" class="headerlink" title="2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块三-公有云（40分）-3"><a href="#模块三-公有云（40分）-3" class="headerlink" title="模块三 公有云（40分）"></a>模块三 公有云（40分）</h2><p>企业选择国内公有云提供商，选择云主机、云网络、云硬盘、云防火墙、负载均衡等服务，可创建Web服务，共享文件存储服务，数据库服务，数据库集群等服务。搭建基于云原生的DevOps相关服务，构建云、边、端一体化的边缘计算系统，并开发云应用程序。</p>
<p>根据上述公有云平台的特性，完成公有云中的各项运维工作。</p>
<h3 id="任务1-公有云服务搭建（5分）-3"><a href="#任务1-公有云服务搭建（5分）-3" class="headerlink" title="任务1 公有云服务搭建（5分）"></a>任务1 公有云服务搭建（5分）</h3><h4 id="3-1-1-私有网络管理-3"><a href="#3-1-1-私有网络管理-3" class="headerlink" title="3.1.1   私有网络管理"></a>3.1.1   私有网络管理</h4><p>在公有云中完成虚拟私有云网络的创建。</p>
<h4 id="3-1-2-云实例管理-3"><a href="#3-1-2-云实例管理-3" class="headerlink" title="3.1.2   云实例管理"></a>3.1.2   云实例管理</h4><p>登录公有云平台，创建两台云实例虚拟机。</p>
<h4 id="3-1-3-管理数据库-3"><a href="#3-1-3-管理数据库-3" class="headerlink" title="3.1.3   管理数据库"></a>3.1.3   管理数据库</h4><p>使用intnetX-mysql网络创建两台chinaskill-sql-1和chinaskill-sql-2云服务器，并完成MongoDB安装。</p>
<h4 id="3-1-4-主从数据库-3"><a href="#3-1-4-主从数据库-3" class="headerlink" title="3.1.4   主从数据库"></a>3.1.4   主从数据库</h4><p>在chinaskill-sql-1和chinaskill-sql-2云服务器中配置MongoDB主从数据库。</p>
<h4 id="3-1-5-node环境管理-3"><a href="#3-1-5-node环境管理-3" class="headerlink" title="3.1.5   node环境管理"></a>3.1.5   node环境管理</h4><p>使用提供的压缩文件，安装Node.js环境。</p>
<h4 id="3-1-6-安全组管理-3"><a href="#3-1-6-安全组管理-3" class="headerlink" title="3.1.6   安全组管理"></a>3.1.6   安全组管理</h4><p>根据要求，创建一个安全组。</p>
<h4 id="3-1-7-RocketChat上云-3"><a href="#3-1-7-RocketChat上云-3" class="headerlink" title="3.1.7   RocketChat上云"></a>3.1.7   RocketChat上云</h4><p>使用http服务器提供文件，将Rocket.Chat应用部署上云。</p>
<h4 id="3-1-8-NAT网关-3"><a href="#3-1-8-NAT网关-3" class="headerlink" title="3.1.8   NAT网关"></a>3.1.8   NAT网关</h4><p>根据要求创建一个公网NAT网关。</p>
<h4 id="3-1-9-云服务器备份-2"><a href="#3-1-9-云服务器备份-2" class="headerlink" title="3.1.9   云服务器备份"></a>3.1.9   云服务器备份</h4><p>创建一个云服务器备份存储库名为server_backup，容量为100G。将ChinaSkill-node-1云服务器制作镜像文件chinaskill-image。</p>
<h4 id="3-1-10-负载均衡器-3"><a href="#3-1-10-负载均衡器-3" class="headerlink" title="3.1.10  负载均衡器"></a>3.1.10  负载均衡器</h4><p>根据要求创建一个负载均衡器chinaskill-elb。</p>
<h4 id="3-1-11-弹性伸缩管理-3"><a href="#3-1-11-弹性伸缩管理-3" class="headerlink" title="3.1.11  弹性伸缩管理"></a>3.1.11  弹性伸缩管理</h4><p>根据要求新建一个弹性伸缩启动配置。</p>
<h3 id="任务2-公有云服务运维（10分）-3"><a href="#任务2-公有云服务运维（10分）-3" class="headerlink" title="任务2 公有云服务运维（10分）"></a>任务2 公有云服务运维（10分）</h3><h4 id="3-2-1-云容器引擎-3"><a href="#3-2-1-云容器引擎-3" class="headerlink" title="3.2.1   云容器引擎"></a>3.2.1   云容器引擎</h4><p>在公有云上，按照要求创建一个x86架构的容器云集群。</p>
<h4 id="3-2-2-云容器管理-3"><a href="#3-2-2-云容器管理-3" class="headerlink" title="3.2.2   云容器管理"></a>3.2.2   云容器管理</h4><p>使用插件管理在kcloud容器集群中安装Dashboard可视化监控界面。</p>
<h4 id="3-2-3-使用kubectl操作集群-3"><a href="#3-2-3-使用kubectl操作集群-3" class="headerlink" title="3.2.3   使用kubectl操作集群"></a>3.2.3   使用kubectl操作集群</h4><p>在kcloud集群中安装kubectl命令，使用kubectl命令管理kcloud集群。</p>
<h4 id="3-2-4-Secrets管理–Opaque"><a href="#3-2-4-Secrets管理–Opaque" class="headerlink" title="3.2.4   Secrets管理–Opaque"></a>3.2.4   Secrets管理–Opaque</h4><p>在master节点&#x2F;root目录下编写YAML文件secret.yaml，要求执行文件创建密钥。</p>
<h4 id="3-2-5-公有云安全：入侵检测系统"><a href="#3-2-5-公有云安全：入侵检测系统" class="headerlink" title="3.2.5   公有云安全：入侵检测系统"></a>3.2.5   公有云安全：入侵检测系统</h4><p>使用提供的makechk.tar.gz包安装chkrootkit入侵检测工具，安装完毕后使用chkrootkit工具扫描系。</p>
<h4 id="3-2-6-公有云安全：日志分析服务"><a href="#3-2-6-公有云安全：日志分析服务" class="headerlink" title="3.2.6   公有云安全：日志分析服务"></a>3.2.6   公有云安全：日志分析服务</h4><p>然后使用提供的sepb_elk_latest.tar镜像安装ELK服务。</p>
<h3 id="任务3-公有云运维开发（10分）-3"><a href="#任务3-公有云运维开发（10分）-3" class="headerlink" title="任务3 公有云运维开发（10分）"></a>任务3 公有云运维开发（10分）</h3><h4 id="3-3-1-开发环境搭建-3"><a href="#3-3-1-开发环境搭建-3" class="headerlink" title="3.3.1   开发环境搭建"></a>3.3.1   开发环境搭建</h4><p>创建一台云主机，并登录此云服务器，安装Python3.68运行环境与SDK依赖库。</p>
<h4 id="3-3-2-云数据库管理：调用SDK云数据库管理的方法，实现云数据库的增删查改。"><a href="#3-3-2-云数据库管理：调用SDK云数据库管理的方法，实现云数据库的增删查改。" class="headerlink" title="3.3.2   云数据库管理：调用SDK云数据库管理的方法，实现云数据库的增删查改。"></a>3.3.2   云数据库管理：调用SDK云数据库管理的方法，实现云数据库的增删查改。</h4><h4 id="3-3-3-容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。"><a href="#3-3-3-容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。" class="headerlink" title="3.3.3   容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。"></a>3.3.3   容器集群管理：调用SDK容器集群方法，实现容器集群增、删查、改。</h4><h4 id="3-3-4-弹性伸缩组管理"><a href="#3-3-4-弹性伸缩组管理" class="headerlink" title="3.3.4   弹性伸缩组管理"></a>3.3.4   弹性伸缩组管理</h4><p>编写Python代码，调用弹性伸缩组API，创建弹性伸缩组。</p>
<h4 id="3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-2"><a href="#3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-2" class="headerlink" title="3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）"></a>3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务4-边缘计算系统运维（10分）-3"><a href="#任务4-边缘计算系统运维（10分）-3" class="headerlink" title="任务4 边缘计算系统运维（10分）"></a>任务4 边缘计算系统运维（10分）</h3><h4 id="3-4-1-云端部署-3"><a href="#3-4-1-云端部署-3" class="headerlink" title="3.4.1   云端部署"></a>3.4.1   云端部署</h4><p>构建Kubernetes容器云平台，云端部署KubeEdge CloudCore云测模块，并启动cloudcore服务。</p>
<h4 id="3-4-2-边端部署-3"><a href="#3-4-2-边端部署-3" class="headerlink" title="3.4.2   边端部署"></a>3.4.2   边端部署</h4><p>在边侧部署KubeEdge EdgeCore边侧模块，并启动edgecore服务。</p>
<h4 id="3-4-3-边缘应用部署-3"><a href="#3-4-3-边缘应用部署-3" class="headerlink" title="3.4.3   边缘应用部署"></a>3.4.3   边缘应用部署</h4><p>通过边缘计算平台完成应用场景镜像部署与调试。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务5-边缘计算云应用开发（5分）-3"><a href="#任务5-边缘计算云应用开发（5分）-3" class="headerlink" title="任务5 边缘计算云应用开发（5分）"></a>任务5 边缘计算云应用开发（5分）</h3><h4 id="3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-3"><a href="#3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-3" class="headerlink" title="3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）"></a>3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）</h4><h1 id="“云计算应用”赛项赛卷5"><a href="#“云计算应用”赛项赛卷5" class="headerlink" title="“云计算应用”赛项赛卷5"></a>“云计算应用”赛项赛卷5</h1><h2 id="模块一-私有云（30分）-4"><a href="#模块一-私有云（30分）-4" class="headerlink" title="模块一 私有云（30分）"></a>模块一 私有云（30分）</h2><p>企业首先完成私有云平台搭建和运维，私有云平台提供云主机、云网络、云存储等基础架构云服务，并开发自动化运维程序。</p>
<h3 id="任务1-私有云服务搭建（5分）-4"><a href="#任务1-私有云服务搭建（5分）-4" class="headerlink" title="任务1 私有云服务搭建（5分）"></a>任务1 私有云服务搭建（5分）</h3><h4 id="1-1-1-基础环境配置-4"><a href="#1-1-1-基础环境配置-4" class="headerlink" title="1.1.1   基础环境配置"></a>1.1.1   基础环境配置</h4><p>1.控制节点主机名为controller，设置计算节点主机名为compute；</p>
<p>2.hosts文件将IP地址映射为主机名。</p>
<h4 id="1-1-2-yum源配置-3"><a href="#1-1-2-yum源配置-3" class="headerlink" title="1.1.2   yum源配置"></a>1.1.2   yum源配置</h4><p>使用提供的http服务地址，分别设置controller节点和compute节点的yum源文件http.repo。</p>
<h4 id="1-1-3-配置无秘钥ssh-4"><a href="#1-1-3-配置无秘钥ssh-4" class="headerlink" title="1.1.3   配置无秘钥ssh"></a>1.1.3   配置无秘钥ssh</h4><p>配置controller节点可以无秘钥访问compute节点。</p>
<h4 id="1-1-4-基础安装-4"><a href="#1-1-4-基础安装-4" class="headerlink" title="1.1.4   基础安装"></a>1.1.4   基础安装</h4><p>在控制节点和计算节点上分别安装openstack-iaas软件包。</p>
<h4 id="1-1-5-数据库安装与调优-4"><a href="#1-1-5-数据库安装与调优-4" class="headerlink" title="1.1.5   数据库安装与调优"></a>1.1.5   数据库安装与调优</h4><p>在控制节点上使用安装Mariadb、RabbitMQ等服务。并进行相关操作。</p>
<h4 id="1-1-6-Keystone服务安装与使用-4"><a href="#1-1-6-Keystone服务安装与使用-4" class="headerlink" title="1.1.6   Keystone服务安装与使用"></a>1.1.6   Keystone服务安装与使用</h4><p>在控制节点上安装Keystone服务并创建用户。</p>
<h4 id="1-1-7-Glance安装与使用-4"><a href="#1-1-7-Glance安装与使用-4" class="headerlink" title="1.1.7   Glance安装与使用"></a>1.1.7   Glance安装与使用</h4><p>在控制节点上安装Glance 服务。上传镜像至平台，并设置镜像启动的要求参数。</p>
<h4 id="1-1-8-Nova安装-4"><a href="#1-1-8-Nova安装-4" class="headerlink" title="1.1.8   Nova安装"></a>1.1.8   Nova安装</h4><p>在控制节点和计算节点上分别安装Nova服务。安装完成后，完成Nova相关配置。</p>
<h4 id="1-1-9-Neutron安装-4"><a href="#1-1-9-Neutron安装-4" class="headerlink" title="1.1.9   Neutron安装"></a>1.1.9   Neutron安装</h4><p>在控制和计算节点上正确安装Neutron服务。</p>
<h4 id="1-1-10-Dashboard安装-4"><a href="#1-1-10-Dashboard安装-4" class="headerlink" title="1.1.10  Dashboard安装"></a>1.1.10  Dashboard安装</h4><p>在控制节点上安装Dashboard服务。安装完成后，将Dashboard中的 Django数据修改为存储在文件中。</p>
<h4 id="1-1-11-Swift安装-4"><a href="#1-1-11-Swift安装-4" class="headerlink" title="1.1.11  Swift安装"></a>1.1.11  Swift安装</h4><p>在控制节点和计算节点上分别安装Swift服务。安装完成后，将cirros镜像进行分片存储。</p>
<h4 id="1-1-12-Cinder创建硬盘-4"><a href="#1-1-12-Cinder创建硬盘-4" class="headerlink" title="1.1.12  Cinder创建硬盘"></a>1.1.12  Cinder创建硬盘</h4><p>在控制节点和计算节点分别安装Cinder服务，请在计算节点，对块存储进行扩容操作。</p>
<h4 id="1-1-13-Manila服务安装与使用-1"><a href="#1-1-13-Manila服务安装与使用-1" class="headerlink" title="1.1.13  Manila服务安装与使用"></a>1.1.13  Manila服务安装与使用</h4><p>在控制和计算节点上分别在控制节点和计算节点安装Manila服务。</p>
<h3 id="任务2-私有云服务运维（15分）-4"><a href="#任务2-私有云服务运维（15分）-4" class="headerlink" title="任务2 私有云服务运维（15分）"></a>任务2 私有云服务运维（15分）</h3><h4 id="1-2-1-Keystone优化-优化token失效时间"><a href="#1-2-1-Keystone优化-优化token失效时间" class="headerlink" title="1.2.1   Keystone优化-优化token失效时间"></a>1.2.1   Keystone优化-优化token失效时间</h4><p>请修改相关配置，将Keystone的失效列表缓存时间增加到原来的两倍。</p>
<h4 id="1-2-2-OpenStack消息队列调优"><a href="#1-2-2-OpenStack消息队列调优" class="headerlink" title="1.2.2   OpenStack消息队列调优"></a>1.2.2   OpenStack消息队列调优</h4><p>在OpenStack私有云平台，分别通过用户级别、系统级别、配置文件来设置RabbitMQ服务的最大连接数为10240。</p>
<h4 id="1-2-3-Raid磁盘阵列管理-1"><a href="#1-2-3-Raid磁盘阵列管理-1" class="headerlink" title="1.2.3   Raid磁盘阵列管理"></a>1.2.3   Raid磁盘阵列管理</h4><p>在云主机上对云硬盘进行操作，先进行分区，然后创建名为&#x2F;dev&#x2F;md5、raid级别为5的磁盘阵列加一个热备盘。</p>
<h4 id="1-2-4-虚拟机调整flavor"><a href="#1-2-4-虚拟机调整flavor" class="headerlink" title="1.2.4   虚拟机调整flavor"></a>1.2.4   虚拟机调整flavor</h4><p>使用OpenStack私有云平台，请修改相应配置，实现云主机调整实例大小可以使用。</p>
<h4 id="1-2-5-OpenStack镜像压缩"><a href="#1-2-5-OpenStack镜像压缩" class="headerlink" title="1.2.5   OpenStack镜像压缩"></a>1.2.5   OpenStack镜像压缩</h4><p>在HTTP文件服务器中存在一个镜像为CentOS7.5-compress.qcow2的镜像，请对该镜像进行压缩操作。</p>
<h4 id="1-2-6-Ceph部署"><a href="#1-2-6-Ceph部署" class="headerlink" title="1.2.6   Ceph部署"></a>1.2.6   Ceph部署</h4><p>使用提供的ceph.tar.gz软件包，安装ceph服务并完成初始化操作。</p>
<h4 id="1-2-7-使用Heat模板创建网络-1"><a href="#1-2-7-使用Heat模板创建网络-1" class="headerlink" title="1.2.7   使用Heat模板创建网络"></a>1.2.7   使用Heat模板创建网络</h4><p>在自行搭建的OpenStack私有云平台上，编写Heat模板文件，完成网络的创建。</p>
<h4 id="1-2-8-Glance调优"><a href="#1-2-8-Glance调优" class="headerlink" title="1.2.8   Glance调优"></a>1.2.8   Glance调优</h4><p>在OpenStack平台中，修改相关配置文件，将子进程数量相应的配置修改成2。</p>
<h4 id="1-2-9-Nova资源优化"><a href="#1-2-9-Nova资源优化" class="headerlink" title="1.2.9   Nova资源优化"></a>1.2.9   Nova资源优化</h4><p>编辑nova.conf文件，将内存预留量配置为4GB。</p>
<h4 id="1-2-10-Nova安装与优化-优化数据库连接"><a href="#1-2-10-Nova安装与优化-优化数据库连接" class="headerlink" title="1.2.10  Nova安装与优化-优化数据库连接"></a>1.2.10  Nova安装与优化-优化数据库连接</h4><p>修改nova相关配置文件，修改连接池大小和最大允许超出的连接数为10。</p>
<h4 id="1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-2"><a href="#1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-2" class="headerlink" title="1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-私有云运维开发（10分）-4"><a href="#任务3-私有云运维开发（10分）-4" class="headerlink" title="任务3 私有云运维开发（10分）"></a>任务3 私有云运维开发（10分）</h3><h4 id="1-3-1-编写Shell一键部署脚本-3"><a href="#1-3-1-编写Shell一键部署脚本-3" class="headerlink" title="1.3.1   编写Shell一键部署脚本"></a>1.3.1   编写Shell一键部署脚本</h4><p>编写一键部署nfs云网盘应用系统。</p>
<h4 id="1-3-2-Ansible部署MariaDB服务-2"><a href="#1-3-2-Ansible部署MariaDB服务-2" class="headerlink" title="1.3.2   Ansible部署MariaDB服务"></a>1.3.2   Ansible部署MariaDB服务</h4><p>编写Ansible脚本，部署MariaDB服务。</p>
<h4 id="1-3-3-Ansible部署zabbix服务-2"><a href="#1-3-3-Ansible部署zabbix服务-2" class="headerlink" title="1.3.3   Ansible部署zabbix服务"></a>1.3.3   Ansible部署zabbix服务</h4><p>编写Ansible脚本，部署zabbix服务。</p>
<h4 id="1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-3"><a href="#1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-3" class="headerlink" title="1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块二-容器云（30分）-4"><a href="#模块二-容器云（30分）-4" class="headerlink" title="模块二 容器云（30分）"></a>模块二 容器云（30分）</h2><p>企业构建Kubernetes容器云集群，引入KubeVirt实现OpenStack到Kubernetes的全面转型，用Kubernetes来管一切虚拟化运行时，包含裸金属、VM、容器。同时研发团队决定搭建基于Kubernetes 的CI&#x2F;CD环境，基于这个平台来实现DevOps流程。引入服务网格Istio，实现业务系统的灰度发布，治理和优化公司各种微服务，并开发自动化运维程序。</p>
<h3 id="任务1-容器云服务搭建（5分）-4"><a href="#任务1-容器云服务搭建（5分）-4" class="headerlink" title="任务1 容器云服务搭建（5分）"></a>任务1 容器云服务搭建（5分）</h3><h4 id="2-1-1-部署容器云平台-4"><a href="#2-1-1-部署容器云平台-4" class="headerlink" title="2.1.1   部署容器云平台"></a>2.1.1   部署容器云平台</h4><p>使用OpenStack私有云平台创建两台云主机，分别作为Kubernetes集群的master节点和node节点，然后完成Kubernetes集群的部署，并完成Istio服务网格、KubeVirt虚拟化和Harbor镜像仓库的部署。</p>
<h3 id="任务2-容器云服务运维（15分）-4"><a href="#任务2-容器云服务运维（15分）-4" class="headerlink" title="任务2 容器云服务运维（15分）"></a>任务2 容器云服务运维（15分）</h3><h4 id="2-2-1-容器化部署MariaDB"><a href="#2-2-1-容器化部署MariaDB" class="headerlink" title="2.2.1   容器化部署MariaDB"></a>2.2.1   容器化部署MariaDB</h4><p>编写Dockerfile文件构建mysql镜像，要求基于centos完成MariaDB数据库的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-2-容器化部署Redis-1"><a href="#2-2-2-容器化部署Redis-1" class="headerlink" title="2.2.2   容器化部署Redis"></a>2.2.2   容器化部署Redis</h4><p>编写Dockerfile文件构建redis镜像，要求基于centos完成Redis服务的安装和配置，并设置服务开机自启。</p>
<h4 id="2-2-3-容器化部署Nginx"><a href="#2-2-3-容器化部署Nginx" class="headerlink" title="2.2.3   容器化部署Nginx"></a>2.2.3   容器化部署Nginx</h4><p>编写Dockerfile文件构建nginx镜像，要求基于centos完成Nginx服务的安装和配置，并设置服务开机自启。</p>
<h4 id="2-2-4-容器化部署Explorer"><a href="#2-2-4-容器化部署Explorer" class="headerlink" title="2.2.4   容器化部署Explorer"></a>2.2.4   容器化部署Explorer</h4><p>编写Dockerfile文件构建explorer镜像，要求基于centos完成PHP和HTTP环境的安装和配置，并设置服务开机自启。</p>
<h4 id="2-2-5-编排部署Explorer管理系统"><a href="#2-2-5-编排部署Explorer管理系统" class="headerlink" title="2.2.5   编排部署Explorer管理系统"></a>2.2.5   编排部署Explorer管理系统</h4><p>编写docker-compose.yaml文件，要求使用镜像mysql、redis、nginx和explorer完成Explorer管理系统的编排部署。</p>
<h4 id="2-2-6-安装GitLab环境-1"><a href="#2-2-6-安装GitLab环境-1" class="headerlink" title="2.2.6   安装GitLab环境"></a>2.2.6   安装GitLab环境</h4><p>新建命名空间kube-ops，将GitLab部署到该命名空间下，然后完成GitLab服务的配置。</p>
<h4 id="2-2-7-部署GitLab-Runner-1"><a href="#2-2-7-部署GitLab-Runner-1" class="headerlink" title="2.2.7   部署GitLab Runner"></a>2.2.7   部署GitLab Runner</h4><p>将GitLab Runner部署到kube-ops命名空间下，并完成GitLab Runner在GitLab中的注册。</p>
<h4 id="2-2-8-配置GitLab-1"><a href="#2-2-8-配置GitLab-1" class="headerlink" title="2.2.8   配置GitLab"></a>2.2.8   配置GitLab</h4><p>在GitLab中新建公开项目并导入离线项目包，然后将Kubernetes集群添加到GitLab中。</p>
<h4 id="2-2-9-构建CI-x2F-CD-4"><a href="#2-2-9-构建CI-x2F-CD-4" class="headerlink" title="2.2.9   构建CI&#x2F;CD"></a>2.2.9   构建CI&#x2F;CD</h4><p>在项目中编写流水线脚本，然后触发自动构建，要求完成构建代码、构建镜像、推送镜像Harbor、并发布服务到Kubernetes集群。</p>
<h4 id="2-2-10-服务网格：路由管理"><a href="#2-2-10-服务网格：路由管理" class="headerlink" title="2.2.10  服务网格：路由管理"></a>2.2.10  服务网格：路由管理</h4><p>将Bookinfo应用部署到default命名空间下，应用默认请求路由，将所有流量路由到各个微服务的v1版本。然后更改请求路由reviews，将指定比例的流量从reviews的v1转移到v3。</p>
<h4 id="2-2-11-KubeVirt运维：VMI管理"><a href="#2-2-11-KubeVirt运维：VMI管理" class="headerlink" title="2.2.11  KubeVirt运维：VMI管理"></a>2.2.11  KubeVirt运维：VMI管理</h4><p>将提供的镜像在default命名空间下创建一台VMI，名称为exam，使用Service对外暴露VMI。</p>
<h4 id="2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-2"><a href="#2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-2" class="headerlink" title="2.2.12  完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>2.2.12  完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-容器云运维开发（10分）-4"><a href="#任务3-容器云运维开发（10分）-4" class="headerlink" title="任务3 容器云运维开发（10分）"></a>任务3 容器云运维开发（10分）</h3><h4 id="2-3-2-管理Deployment资源"><a href="#2-3-2-管理Deployment资源" class="headerlink" title="2.3.2   管理Deployment资源"></a>2.3.2   管理Deployment资源</h4><p>Kubernetes Python运维脚本开发，使用SDK方式管理Deployment服务。</p>
<h4 id="2-3-1-管理service资源-1"><a href="#2-3-1-管理service资源-1" class="headerlink" title="2.3.1   管理service资源"></a>2.3.1   管理service资源</h4><p>Kubernetes Python运维脚本开发，使用Restful APIs方式管理service服务。</p>
<h4 id="2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-4"><a href="#2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-4" class="headerlink" title="2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块三-公有云（40分）-4"><a href="#模块三-公有云（40分）-4" class="headerlink" title="模块三 公有云（40分）"></a>模块三 公有云（40分）</h2><p>企业选择国内公有云提供商，选择云主机、云网络、云硬盘、云防火墙、负载均衡等服务，可创建Web服务，共享文件存储服务，数据库服务，数据库集群等服务。搭建基于云原生的DevOps相关服务，构建云、边、端一体化的边缘计算系统，并开发云应用程序。</p>
<p>根据上述公有云平台的特性，完成公有云中的各项运维工作。</p>
<h3 id="任务1-公有云服务搭建（5分）-4"><a href="#任务1-公有云服务搭建（5分）-4" class="headerlink" title="任务1 公有云服务搭建（5分）"></a>任务1 公有云服务搭建（5分）</h3><h4 id="3-1-1-私有网络管理-4"><a href="#3-1-1-私有网络管理-4" class="headerlink" title="3.1.1   私有网络管理"></a>3.1.1   私有网络管理</h4><p>在公有云中完成虚拟私有云网络的创建。</p>
<h4 id="3-1-2-云实例管理-4"><a href="#3-1-2-云实例管理-4" class="headerlink" title="3.1.2   云实例管理"></a>3.1.2   云实例管理</h4><p>登录公有云平台，创建两台云实例虚拟机。</p>
<h4 id="3-1-3-管理数据库-4"><a href="#3-1-3-管理数据库-4" class="headerlink" title="3.1.3   管理数据库"></a>3.1.3   管理数据库</h4><p>使用intnetX-mysql网络创建两台chinaskill-sql-1和chinaskill-sql-2云服务器，并完成MongoDB安装。</p>
<h4 id="3-1-4-主从数据库-4"><a href="#3-1-4-主从数据库-4" class="headerlink" title="3.1.4   主从数据库"></a>3.1.4   主从数据库</h4><p>在chinaskill-sql-1和chinaskill-sql-2云服务器中配置MongoDB主从数据库。</p>
<h4 id="3-1-5-node环境管理-4"><a href="#3-1-5-node环境管理-4" class="headerlink" title="3.1.5   node环境管理"></a>3.1.5   node环境管理</h4><p>使用提供的压缩文件，安装Node.js环境。</p>
<h4 id="3-1-6-安全组管理-4"><a href="#3-1-6-安全组管理-4" class="headerlink" title="3.1.6   安全组管理"></a>3.1.6   安全组管理</h4><p>根据要求，创建一个安全组。</p>
<h4 id="3-1-7-RocketChat上云-4"><a href="#3-1-7-RocketChat上云-4" class="headerlink" title="3.1.7   RocketChat上云"></a>3.1.7   RocketChat上云</h4><p>使用http服务器提供文件，将Rocket.Chat应用部署上云。</p>
<h4 id="3-1-8-NAT网关-4"><a href="#3-1-8-NAT网关-4" class="headerlink" title="3.1.8   NAT网关"></a>3.1.8   NAT网关</h4><p>根据要求创建一个公网NAT网关。</p>
<h4 id="3-1-9-云服务器备份-3"><a href="#3-1-9-云服务器备份-3" class="headerlink" title="3.1.9   云服务器备份"></a>3.1.9   云服务器备份</h4><p>创建一个云服务器备份存储库名为server_backup，容量为100G。将ChinaSkill-node-1云服务器制作镜像文件chinaskill-image。</p>
<h3 id="任务2-公有云服务运维（10分）-4"><a href="#任务2-公有云服务运维（10分）-4" class="headerlink" title="任务2 公有云服务运维（10分）"></a>任务2 公有云服务运维（10分）</h3><h4 id="3-2-1-云容器引擎-4"><a href="#3-2-1-云容器引擎-4" class="headerlink" title="3.2.1   云容器引擎"></a>3.2.1   云容器引擎</h4><p>在公有云上，按照要求创建一个x86架构的容器云集群。</p>
<h4 id="3-2-2-云容器管理-4"><a href="#3-2-2-云容器管理-4" class="headerlink" title="3.2.2   云容器管理"></a>3.2.2   云容器管理</h4><p>使用插件管理在kcloud容器集群中安装Dashboard可视化监控界面。</p>
<h4 id="3-2-3-使用kubectl操作集群-4"><a href="#3-2-3-使用kubectl操作集群-4" class="headerlink" title="3.2.3   使用kubectl操作集群"></a>3.2.3   使用kubectl操作集群</h4><p>在kcloud集群中安装kubectl命令，使用kubectl命令管理kcloud集群。</p>
<h4 id="3-2-4-安装Helm-2"><a href="#3-2-4-安装Helm-2" class="headerlink" title="3.2.4   安装Helm"></a>3.2.4   安装Helm</h4><p>使用提供的Helm软件包，在kcloud集群中安装Helm服务。</p>
<h4 id="3-2-5-WordPress应用部署"><a href="#3-2-5-WordPress应用部署" class="headerlink" title="3.2.5   WordPress应用部署"></a>3.2.5   WordPress应用部署</h4><p>根据提供的chart包wordpress-13.0.23.tgz部署WordPress服务。</p>
<h4 id="3-2-6-ChartMuseum仓库部署"><a href="#3-2-6-ChartMuseum仓库部署" class="headerlink" title="3.2.6   ChartMuseum仓库部署"></a>3.2.6   ChartMuseum仓库部署</h4><p>在k8s集群中创建chartmuseum命名空间，编写yaml文件在chartmuseum命名空间中使用chartmuseum:latest镜像创建本地私有chart仓库。</p>
<h3 id="任务3-公有云运维开发（10分）-4"><a href="#任务3-公有云运维开发（10分）-4" class="headerlink" title="任务3 公有云运维开发（10分）"></a>任务3 公有云运维开发（10分）</h3><h4 id="3-3-1-开发环境搭建-4"><a href="#3-3-1-开发环境搭建-4" class="headerlink" title="3.3.1   开发环境搭建"></a>3.3.1   开发环境搭建</h4><p>创建一台云主机，并登录此云服务器，安装Python3.68运行环境与SDK依赖库。</p>
<h4 id="3-3-2-云主机管理"><a href="#3-3-2-云主机管理" class="headerlink" title="3.3.2   云主机管理"></a>3.3.2   云主机管理</h4><p>调用SDK云主机管理的方法，实现云主机的的增删查改。</p>
<h4 id="3-3-3-云主机组管理"><a href="#3-3-3-云主机组管理" class="headerlink" title="3.3.3   云主机组管理"></a>3.3.3   云主机组管理</h4><p>调用SDK云主机组管理的方法，实现云主机组的的增删查改。</p>
<h4 id="3-3-4-弹性伸缩组管理-1"><a href="#3-3-4-弹性伸缩组管理-1" class="headerlink" title="3.3.4   弹性伸缩组管理"></a>3.3.4   弹性伸缩组管理</h4><p>编写Python代码，调用弹性伸缩组API，创建弹性伸缩组。</p>
<h4 id="3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-3"><a href="#3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-3" class="headerlink" title="3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）"></a>3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务4-边缘计算系统运维（10分）-4"><a href="#任务4-边缘计算系统运维（10分）-4" class="headerlink" title="任务4 边缘计算系统运维（10分）"></a>任务4 边缘计算系统运维（10分）</h3><h4 id="3-4-1-云端部署-4"><a href="#3-4-1-云端部署-4" class="headerlink" title="3.4.1   云端部署"></a>3.4.1   云端部署</h4><p>构建Kubernetes容器云平台，云端部署KubeEdge CloudCore云测模块，并启动cloudcore服务。</p>
<h4 id="3-4-2-边端部署-4"><a href="#3-4-2-边端部署-4" class="headerlink" title="3.4.2   边端部署"></a>3.4.2   边端部署</h4><p>在边侧部署KubeEdge EdgeCore边侧模块，并启动edgecore服务。</p>
<h4 id="3-4-3-边缘应用部署-4"><a href="#3-4-3-边缘应用部署-4" class="headerlink" title="3.4.3   边缘应用部署"></a>3.4.3   边缘应用部署</h4><p>通过边缘计算平台完成应用场景镜像部署与调试。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务5-边缘计算云应用开发（5分）-4"><a href="#任务5-边缘计算云应用开发（5分）-4" class="headerlink" title="任务5 边缘计算云应用开发（5分）"></a>任务5 边缘计算云应用开发（5分）</h3><h4 id="3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-4"><a href="#3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-4" class="headerlink" title="3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）"></a>3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）</h4><h1 id="“云计算应用”赛项赛卷6"><a href="#“云计算应用”赛项赛卷6" class="headerlink" title="“云计算应用”赛项赛卷6"></a>“云计算应用”赛项赛卷6</h1><h2 id="模块一-私有云（30分）-5"><a href="#模块一-私有云（30分）-5" class="headerlink" title="模块一 私有云（30分）"></a>模块一 私有云（30分）</h2><p>企业首先完成私有云平台搭建和运维，私有云平台提供云主机、云网络、云存储等基础架构云服务，并开发自动化运维程序。</p>
<h3 id="任务1-私有云服务搭建（5分）-5"><a href="#任务1-私有云服务搭建（5分）-5" class="headerlink" title="任务1 私有云服务搭建（5分）"></a>任务1 私有云服务搭建（5分）</h3><h4 id="1-1-1-基础环境配置-5"><a href="#1-1-1-基础环境配置-5" class="headerlink" title="1.1.1   基础环境配置"></a>1.1.1   基础环境配置</h4><p>1.控制节点主机名为controller，设置计算节点主机名为compute；</p>
<p>2.hosts文件将IP地址映射为主机名。</p>
<h4 id="1-1-2-yum源配置-4"><a href="#1-1-2-yum源配置-4" class="headerlink" title="1.1.2   yum源配置"></a>1.1.2   yum源配置</h4><p>使用提供的http服务地址，分别设置controller节点和compute节点的yum源文件http.repo。</p>
<h4 id="1-1-3-配置无秘钥ssh-5"><a href="#1-1-3-配置无秘钥ssh-5" class="headerlink" title="1.1.3   配置无秘钥ssh"></a>1.1.3   配置无秘钥ssh</h4><p>配置controller节点可以无秘钥访问compute节点。</p>
<h4 id="1-1-4-基础安装-5"><a href="#1-1-4-基础安装-5" class="headerlink" title="1.1.4   基础安装"></a>1.1.4   基础安装</h4><p>在控制节点和计算节点上分别安装openstack-iaas软件包。</p>
<h4 id="1-1-5-数据库安装与调优-5"><a href="#1-1-5-数据库安装与调优-5" class="headerlink" title="1.1.5   数据库安装与调优"></a>1.1.5   数据库安装与调优</h4><p>在控制节点上使用安装Mariadb、RabbitMQ等服务。并进行相关操作。</p>
<h4 id="1-1-6-Keystone服务安装与使用-5"><a href="#1-1-6-Keystone服务安装与使用-5" class="headerlink" title="1.1.6   Keystone服务安装与使用"></a>1.1.6   Keystone服务安装与使用</h4><p>在控制节点上安装Keystone服务并创建用户。</p>
<h4 id="1-1-7-Glance安装与使用-5"><a href="#1-1-7-Glance安装与使用-5" class="headerlink" title="1.1.7   Glance安装与使用"></a>1.1.7   Glance安装与使用</h4><p>在控制节点上安装Glance 服务。上传镜像至平台，并设置镜像启动的要求参数。</p>
<h4 id="1-1-8-Nova安装-5"><a href="#1-1-8-Nova安装-5" class="headerlink" title="1.1.8   Nova安装"></a>1.1.8   Nova安装</h4><p>在控制节点和计算节点上分别安装Nova服务。安装完成后，完成Nova相关配置。</p>
<h4 id="1-1-9-Neutron安装-5"><a href="#1-1-9-Neutron安装-5" class="headerlink" title="1.1.9   Neutron安装"></a>1.1.9   Neutron安装</h4><p>在控制和计算节点上正确安装Neutron服务。</p>
<h4 id="1-1-10-Dashboard安装-5"><a href="#1-1-10-Dashboard安装-5" class="headerlink" title="1.1.10  Dashboard安装"></a>1.1.10  Dashboard安装</h4><p>在控制节点上安装Dashboard服务。安装完成后，将Dashboard中的 Django数据修改为存储在文件中。</p>
<h4 id="1-1-11-Swift安装-5"><a href="#1-1-11-Swift安装-5" class="headerlink" title="1.1.11  Swift安装"></a>1.1.11  Swift安装</h4><p>在控制节点和计算节点上分别安装Swift服务。安装完成后，将cirros镜像进行分片存储。</p>
<h4 id="1-1-12-Cinder创建硬盘-5"><a href="#1-1-12-Cinder创建硬盘-5" class="headerlink" title="1.1.12  Cinder创建硬盘"></a>1.1.12  Cinder创建硬盘</h4><p>在控制节点和计算节点分别安装Cinder服务，请在计算节点，对块存储进行扩容操作。</p>
<h4 id="1-1-13-OpenStack平台内存优化"><a href="#1-1-13-OpenStack平台内存优化" class="headerlink" title="1.1.13  OpenStack平台内存优化"></a>1.1.13  OpenStack平台内存优化</h4><p>搭建完OpenStack平台后，关闭系统的内存共享，打开透明大页。</p>
<h3 id="任务2-私有云服务运维（15分）-5"><a href="#任务2-私有云服务运维（15分）-5" class="headerlink" title="任务2 私有云服务运维（15分）"></a>任务2 私有云服务运维（15分）</h3><h4 id="1-2-1-OpenStack开放镜像权限-1"><a href="#1-2-1-OpenStack开放镜像权限-1" class="headerlink" title="1.2.1   OpenStack开放镜像权限"></a>1.2.1   OpenStack开放镜像权限</h4><p>在admin项目中存在glance-cirros镜像文件，将glance-cirros镜像指定demo项目进行共享使用。</p>
<h4 id="1-2-2-OpenStack消息队列调优-1"><a href="#1-2-2-OpenStack消息队列调优-1" class="headerlink" title="1.2.2   OpenStack消息队列调优"></a>1.2.2   OpenStack消息队列调优</h4><p>在OpenStack私有云平台，分别通过用户级别、系统级别、配置文件来设置RabbitMQ服务的最大连接数为10240。</p>
<h4 id="1-2-3-OpenStack镜像压缩-1"><a href="#1-2-3-OpenStack镜像压缩-1" class="headerlink" title="1.2.3   OpenStack镜像压缩"></a>1.2.3   OpenStack镜像压缩</h4><p>在HTTP文件服务器中存在一个镜像为CentOS7.5-compress.qcow2的镜像，请对该镜像进行压缩操作。</p>
<h4 id="1-2-4-Glance对接Cinder存储-1"><a href="#1-2-4-Glance对接Cinder存储-1" class="headerlink" title="1.2.4   Glance对接Cinder存储"></a>1.2.4   Glance对接Cinder存储</h4><p>在自行搭建的OpenStack平台中修改相关参数，使Glance可以使用Cinder作为后端存储。</p>
<h4 id="1-2-5-使用Heat模板创建容器-1"><a href="#1-2-5-使用Heat模板创建容器-1" class="headerlink" title="1.2.5   使用Heat模板创建容器"></a>1.2.5   使用Heat模板创建容器</h4><p>在自行搭建的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板文件，要求执行yaml文件可以创建名为heat-swift的容器。</p>
<h4 id="1-2-6-Nova清除缓存-1"><a href="#1-2-6-Nova清除缓存-1" class="headerlink" title="1.2.6   Nova清除缓存"></a>1.2.6   Nova清除缓存</h4><p>在OpenStack平台上，修改相关配置，让长时间不用的镜像缓存在过一定的时间后会被自动删除。</p>
<h4 id="1-2-7-Redis集群部署。-1"><a href="#1-2-7-Redis集群部署。-1" class="headerlink" title="1.2.7   Redis集群部署。"></a>1.2.7   Redis集群部署。</h4><p>部署Redis集群，Redis的一主二从三哨兵架构。</p>
<h4 id="1-2-8-redis服务调优-内存大页"><a href="#1-2-8-redis服务调优-内存大页" class="headerlink" title="1.2.8   redis服务调优-内存大页"></a>1.2.8   redis服务调优-内存大页</h4><p>请修改Redis的内存大页机制，规避大量拷贝时的性能变慢问题。</p>
<h4 id="1-2-9-JumpServer堡垒机部署-2"><a href="#1-2-9-JumpServer堡垒机部署-2" class="headerlink" title="1.2.9   JumpServer堡垒机部署"></a>1.2.9   JumpServer堡垒机部署</h4><p>使用提供的软件包安装JumpServer堡垒机服务，并配置使用该堡垒机对接自己安装的控制和计算节点。</p>
<h4 id="1-2-10-MongoDB主从"><a href="#1-2-10-MongoDB主从" class="headerlink" title="1.2.10  MongoDB主从"></a>1.2.10  MongoDB主从</h4><p>使用提供的OpenStack云平台创建两台云主机，在两台云主机中部署MongoDB数据库服务并配置MongoDB主从数据库。</p>
<h4 id="1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-3"><a href="#1-2-11-完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-3" class="headerlink" title="1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>1.2.11  完成私有云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-私有云运维开发（10分）-5"><a href="#任务3-私有云运维开发（10分）-5" class="headerlink" title="任务3 私有云运维开发（10分）"></a>任务3 私有云运维开发（10分）</h3><h4 id="1-3-1-编写Shell一键部署脚本-4"><a href="#1-3-1-编写Shell一键部署脚本-4" class="headerlink" title="1.3.1   编写Shell一键部署脚本"></a>1.3.1   编写Shell一键部署脚本</h4><p>编写一键部署脚本，要求可以一键部署gpmall商城应用系统。</p>
<h4 id="1-3-2-Ansible部署FTP服务-1"><a href="#1-3-2-Ansible部署FTP服务-1" class="headerlink" title="1.3.2   Ansible部署FTP服务"></a>1.3.2   Ansible部署FTP服务</h4><p>编写Ansible脚本，部署FTP服务。</p>
<h4 id="1-3-3-Ansible部署Kafka服务-1"><a href="#1-3-3-Ansible部署Kafka服务-1" class="headerlink" title="1.3.3   Ansible部署Kafka服务"></a>1.3.3   Ansible部署Kafka服务</h4><p>编写Playbook，部署的ZooKeeper和Kafka。</p>
<h4 id="1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-4"><a href="#1-3-4-编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-4" class="headerlink" title="1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>1.3.4   编写OpenStack容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块二-容器云（30分）-5"><a href="#模块二-容器云（30分）-5" class="headerlink" title="模块二 容器云（30分）"></a>模块二 容器云（30分）</h2><p>企业构建Kubernetes容器云集群，引入KubeVirt实现OpenStack到Kubernetes的全面转型，用Kubernetes来管一切虚拟化运行时，包含裸金属、VM、容器。同时研发团队决定搭建基于Kubernetes 的CI&#x2F;CD环境，基于这个平台来实现DevOps流程。引入服务网格Istio，实现业务系统的灰度发布，治理和优化公司各种微服务，并开发自动化运维程序。</p>
<h3 id="任务1-容器云服务搭建（5分）-5"><a href="#任务1-容器云服务搭建（5分）-5" class="headerlink" title="任务1 容器云服务搭建（5分）"></a>任务1 容器云服务搭建（5分）</h3><h4 id="2-1-1-部署容器云平台-5"><a href="#2-1-1-部署容器云平台-5" class="headerlink" title="2.1.1   部署容器云平台"></a>2.1.1   部署容器云平台</h4><p>使用OpenStack私有云平台创建两台云主机，分别作为Kubernetes集群的master节点和node节点，然后完成Kubernetes集群的部署，并完成Istio服务网格、KubeVirt虚拟化和Harbor镜像仓库的部署。</p>
<h3 id="任务2-容器云服务运维（15分）-5"><a href="#任务2-容器云服务运维（15分）-5" class="headerlink" title="任务2 容器云服务运维（15分）"></a>任务2 容器云服务运维（15分）</h3><h4 id="2-2-1-容器化部署MariaDB-1"><a href="#2-2-1-容器化部署MariaDB-1" class="headerlink" title="2.2.1   容器化部署MariaDB"></a>2.2.1   容器化部署MariaDB</h4><p>编写Dockerfile文件构建mysql镜像，要求基于centos完成MariaDB数据库的安装和配置，并设置服务开机自启。</p>
<h4 id="2-2-2-容器化部署Redis-2"><a href="#2-2-2-容器化部署Redis-2" class="headerlink" title="2.2.2   容器化部署Redis"></a>2.2.2   容器化部署Redis</h4><p>编写Dockerfile文件构建redis镜像，要求基于centos完成Redis服务的安装和配置，并设置服务开机自启。</p>
<h4 id="2-2-3-容器化部署Nginx-1"><a href="#2-2-3-容器化部署Nginx-1" class="headerlink" title="2.2.3   容器化部署Nginx"></a>2.2.3   容器化部署Nginx</h4><p>编写Dockerfile文件构建nginx镜像，要求基于centos完成Nginx服务的安装和配置，并设置服务开机自启。</p>
<h4 id="2-2-4-容器化部署ERP"><a href="#2-2-4-容器化部署ERP" class="headerlink" title="2.2.4   容器化部署ERP"></a>2.2.4   容器化部署ERP</h4><p>编写Dockerfile文件构建erp镜像，要求基于centos完成JDK环境和ERP服务的安装与配置，并设置服务开机自启。</p>
<h4 id="2-2-5-编排部署ERP管理系统"><a href="#2-2-5-编排部署ERP管理系统" class="headerlink" title="2.2.5   编排部署ERP管理系统"></a>2.2.5   编排部署ERP管理系统</h4><p>编写docker-compose.yaml文件，要求使用镜像mysql、redis、nginx和erp完成ERP管理系统的编排部署。</p>
<h4 id="2-2-6-部署Jenkins"><a href="#2-2-6-部署Jenkins" class="headerlink" title="2.2.6   部署Jenkins"></a>2.2.6   部署Jenkins</h4><p>使用Deployment将Jenkins部署到devops命名空间下，使用提供的软件包完成离线插件的安装，并完成Jenkins的基础配置。</p>
<h4 id="2-2-7-部署GitLab"><a href="#2-2-7-部署GitLab" class="headerlink" title="2.2.7   部署GitLab"></a>2.2.7   部署GitLab</h4><p>使用Deployment将GitLab部署到devops命名空间下，设置GitLab登录信息，然后新建公开项目，并将提供的代码上传到该项目。</p>
<h4 id="2-2-8-配置Jenkins连接GitLab-1"><a href="#2-2-8-配置Jenkins连接GitLab-1" class="headerlink" title="2.2.8   配置Jenkins连接GitLab"></a>2.2.8   配置Jenkins连接GitLab</h4><p>在Jenkins中配置GitLab凭据，完成后测试其连通性。</p>
<h4 id="2-2-9-构建CI-x2F-CD-5"><a href="#2-2-9-构建CI-x2F-CD-5" class="headerlink" title="2.2.9   构建CI&#x2F;CD"></a>2.2.9   构建CI&#x2F;CD</h4><p>在Jenkins中新建流水线任务，然后在GitLab项目中编写声明式Pipeline，触发构建，要求完成构建项目，然后构建Docker镜像并推送到Harbor仓库，并基于新构建的镜像完成服务自动发布到Kubernetes集群。</p>
<h4 id="2-2-10-服务网格：流量镜像"><a href="#2-2-10-服务网格：流量镜像" class="headerlink" title="2.2.10  服务网格：流量镜像"></a>2.2.10  服务网格：流量镜像</h4><p>将Bookinfo应用部署到default命名空间下，创建请求路由reviews，将指定的流量路由到reviews微服务的v1版本，然后将相同流量镜像到reviews微服务的v2版本。</p>
<h4 id="2-2-11-KubeVirt运维：快照管理-1"><a href="#2-2-11-KubeVirt运维：快照管理-1" class="headerlink" title="2.2.11  KubeVirt运维：快照管理"></a>2.2.11  KubeVirt运维：快照管理</h4><p>使用提供的镜像在default命名空间下创建一台VM，名称为exam，指定VM的配置信息，并为VM创建名为exam的快照。</p>
<h4 id="2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-3"><a href="#2-2-12-完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）-3" class="headerlink" title="2.2.12  完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）"></a>2.2.12  完成容器云平台的调优或排错工作。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务3-容器云运维开发（10分）-5"><a href="#任务3-容器云运维开发（10分）-5" class="headerlink" title="任务3 容器云运维开发（10分）"></a>任务3 容器云运维开发（10分）</h3><h4 id="2-3-1-管理Deployment服务"><a href="#2-3-1-管理Deployment服务" class="headerlink" title="2.3.1   管理Deployment服务"></a>2.3.1   管理Deployment服务</h4><p>Kubernetes Python运维脚本开发-使用SDK方式管理Deployment服务。</p>
<h4 id="2-3-2-自定义调度器-2"><a href="#2-3-2-自定义调度器-2" class="headerlink" title="2.3.2   自定义调度器"></a>2.3.2   自定义调度器</h4><p>Kubernetes Python运维脚本开发-使用Restful API方式管理调度器。</p>
<h4 id="2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-5"><a href="#2-3-3-编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）-5" class="headerlink" title="2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）"></a>2.3.3   编写Kubernetes容器云平台自动化运维工具。（本任务只公布考试范围，不公布赛题）</h4><h2 id="模块三-公有云（40分）-5"><a href="#模块三-公有云（40分）-5" class="headerlink" title="模块三 公有云（40分）"></a>模块三 公有云（40分）</h2><p>企业选择国内公有云提供商，选择云主机、云网络、云硬盘、云防火墙、负载均衡等服务，可创建Web服务，共享文件存储服务，数据库服务，数据库集群等服务。搭建基于云原生的DevOps相关服务，构建云、边、端一体化的边缘计算系统，并开发云应用程序。</p>
<p>根据上述公有云平台的特性，完成公有云中的各项运维工作。</p>
<h3 id="任务1-公有云服务搭建（5分）-5"><a href="#任务1-公有云服务搭建（5分）-5" class="headerlink" title="任务1 公有云服务搭建（5分）"></a>任务1 公有云服务搭建（5分）</h3><h4 id="3-1-1-私有网络管理-5"><a href="#3-1-1-私有网络管理-5" class="headerlink" title="3.1.1   私有网络管理"></a>3.1.1   私有网络管理</h4><p>在公有云中完成虚拟私有云网络的创建。</p>
<h4 id="3-1-2-云实例管理-5"><a href="#3-1-2-云实例管理-5" class="headerlink" title="3.1.2   云实例管理"></a>3.1.2   云实例管理</h4><p>登录公有云平台，创建两台云实例虚拟机。</p>
<h4 id="3-1-3-管理数据库-5"><a href="#3-1-3-管理数据库-5" class="headerlink" title="3.1.3   管理数据库"></a>3.1.3   管理数据库</h4><p>使用intnetX-mysql网络创建两台chinaskill-sql-1和chinaskill-sql-2云服务器，并完成MongoDB安装。</p>
<h4 id="3-1-4-主从数据库-5"><a href="#3-1-4-主从数据库-5" class="headerlink" title="3.1.4   主从数据库"></a>3.1.4   主从数据库</h4><p>在chinaskill-sql-1和chinaskill-sql-2云服务器中配置MongoDB主从数据库。</p>
<h4 id="3-1-5-node环境管理-5"><a href="#3-1-5-node环境管理-5" class="headerlink" title="3.1.5   node环境管理"></a>3.1.5   node环境管理</h4><p>使用提供的压缩文件，安装Node.js环境。</p>
<h4 id="3-1-6-安全组管理-5"><a href="#3-1-6-安全组管理-5" class="headerlink" title="3.1.6   安全组管理"></a>3.1.6   安全组管理</h4><p>根据要求，创建一个安全组。</p>
<h4 id="3-1-7-RocketChat上云-5"><a href="#3-1-7-RocketChat上云-5" class="headerlink" title="3.1.7   RocketChat上云"></a>3.1.7   RocketChat上云</h4><p>使用http服务器提供文件，将Rocket.Chat应用部署上云。</p>
<h4 id="3-1-8-NAT网关-5"><a href="#3-1-8-NAT网关-5" class="headerlink" title="3.1.8   NAT网关"></a>3.1.8   NAT网关</h4><p>根据要求创建一个公网NAT网关。</p>
<h4 id="3-1-9-云服务器备份-4"><a href="#3-1-9-云服务器备份-4" class="headerlink" title="3.1.9   云服务器备份"></a>3.1.9   云服务器备份</h4><p>创建一个云服务器备份存储库名为server_backup，容量为100G。将ChinaSkill-node-1云服务器制作镜像文件chinaskill-image。</p>
<h4 id="3-1-10-负载均衡器-4"><a href="#3-1-10-负载均衡器-4" class="headerlink" title="3.1.10  负载均衡器"></a>3.1.10  负载均衡器</h4><p>根据要求创建一个负载均衡器chinaskill-elb。</p>
<h4 id="3-1-11-弹性伸缩管理-4"><a href="#3-1-11-弹性伸缩管理-4" class="headerlink" title="3.1.11  弹性伸缩管理"></a>3.1.11  弹性伸缩管理</h4><p>根据要求新建一个弹性伸缩启动配置。</p>
<h3 id="任务2-公有云服务运维（10分）-5"><a href="#任务2-公有云服务运维（10分）-5" class="headerlink" title="任务2 公有云服务运维（10分）"></a>任务2 公有云服务运维（10分）</h3><h4 id="3-2-1-云容器引擎-5"><a href="#3-2-1-云容器引擎-5" class="headerlink" title="3.2.1   云容器引擎"></a>3.2.1   云容器引擎</h4><p>在公有云上，按照要求创建一个x86架构的容器云集群。</p>
<h4 id="3-2-2-云容器管理-5"><a href="#3-2-2-云容器管理-5" class="headerlink" title="3.2.2   云容器管理"></a>3.2.2   云容器管理</h4><p>使用插件管理在kcloud容器集群中安装Dashboard可视化监控界面。</p>
<h4 id="3-2-3-使用kubectl操作集群-5"><a href="#3-2-3-使用kubectl操作集群-5" class="headerlink" title="3.2.3   使用kubectl操作集群"></a>3.2.3   使用kubectl操作集群</h4><p>在kcloud集群中安装kubectl命令，使用kubectl命令管理kcloud集群。</p>
<h4 id="3-2-4-安装Helm-3"><a href="#3-2-4-安装Helm-3" class="headerlink" title="3.2.4   安装Helm"></a>3.2.4   安装Helm</h4><p>使用提供的Helm软件包，在kcloud集群中安装Helm服务。</p>
<h4 id="3-2-5-云硬盘存储卷"><a href="#3-2-5-云硬盘存储卷" class="headerlink" title="3.2.5   云硬盘存储卷"></a>3.2.5   云硬盘存储卷</h4><p>根据要求购买云硬盘存储卷。</p>
<h4 id="3-2-6-Secrets管理–Opaque"><a href="#3-2-6-Secrets管理–Opaque" class="headerlink" title="3.2.6   Secrets管理–Opaque"></a>3.2.6   Secrets管理–Opaque</h4><p>在master节点&#x2F;root目录下编写YAML文件secret.yaml，要求执行文件创建密钥。</p>
<h3 id="任务3-公有云运维开发（10分）-5"><a href="#任务3-公有云运维开发（10分）-5" class="headerlink" title="任务3 公有云运维开发（10分）"></a>任务3 公有云运维开发（10分）</h3><h4 id="3-3-1-开发环境搭建-5"><a href="#3-3-1-开发环境搭建-5" class="headerlink" title="3.3.1   开发环境搭建"></a>3.3.1   开发环境搭建</h4><p>创建一台云主机，并登录此云服务器，安装Python3.68运行环境与SDK依赖库。</p>
<h4 id="3-3-2-安全组管理-1"><a href="#3-3-2-安全组管理-1" class="headerlink" title="3.3.2   安全组管理"></a>3.3.2   安全组管理</h4><p>调用api安全组的接口，实现安全组的增删查改。</p>
<h4 id="3-3-3-安全组规则管理-1"><a href="#3-3-3-安全组规则管理-1" class="headerlink" title="3.3.3   安全组规则管理"></a>3.3.3   安全组规则管理</h4><p>调用SDK安全组规则的方法，实现安全组规则的增删查改。</p>
<h4 id="3-3-4-云主机管理-2"><a href="#3-3-4-云主机管理-2" class="headerlink" title="3.3.4   云主机管理"></a>3.3.4   云主机管理</h4><p>调用SDK云主机管理的方法，实现云主机的的增删查改。</p>
<h4 id="3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-4"><a href="#3-3-5-完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）-4" class="headerlink" title="3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）"></a>3.3.5   完成公有云平台自动化运维程序开发。（本任务只公布考试范围，不公布赛题）</h4><h3 id="任务4-边缘计算系统运维（10分）-5"><a href="#任务4-边缘计算系统运维（10分）-5" class="headerlink" title="任务4 边缘计算系统运维（10分）"></a>任务4 边缘计算系统运维（10分）</h3><h4 id="3-4-1-云端部署-5"><a href="#3-4-1-云端部署-5" class="headerlink" title="3.4.1   云端部署"></a>3.4.1   云端部署</h4><p>构建Kubernetes容器云平台，云端部署KubeEdge CloudCore云测模块，并启动cloudcore服务。</p>
<h4 id="3-4-2-边端部署-5"><a href="#3-4-2-边端部署-5" class="headerlink" title="3.4.2   边端部署"></a>3.4.2   边端部署</h4><p>在边侧部署KubeEdge EdgeCore边侧模块，并启动edgecore服务。</p>
<h4 id="3-4-3-边缘应用部署-5"><a href="#3-4-3-边缘应用部署-5" class="headerlink" title="3.4.3   边缘应用部署"></a>3.4.3   边缘应用部署</h4><p>通过边缘计算平台完成应用场景镜像部署与调试。（本任务只公布考试范围，不公布赛题）</p>
<h3 id="任务5-边缘计算云应用开发（5分）-5"><a href="#任务5-边缘计算云应用开发（5分）-5" class="headerlink" title="任务5 边缘计算云应用开发（5分）"></a>任务5 边缘计算云应用开发（5分）</h3><h4 id="3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-5"><a href="#3-5-1-对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）-5" class="headerlink" title="3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）"></a>3.5.1   对接边缘计算系统，完成云应用微服务开发。（本任务只公布考试范围，不公布赛题）</h4>]]></content>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s学习日记day2</title>
    <url>/2022/05/19/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day2/</url>
    <content><![CDATA[<h1 id="kubernetes的资源管理"><a href="#kubernetes的资源管理" class="headerlink" title="kubernetes的资源管理"></a>kubernetes的资源管理<span id="more"></span></h1><h2 id="1-资源管理介绍"><a href="#1-资源管理介绍" class="headerlink" title="1 资源管理介绍"></a>1 资源管理介绍</h2><p>在Kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理Kubernetes。</p>
<p>Kubernetes的本质就是一个集群系统，用户可以在集群中部署各种服务。所谓的部署服务，其实就是在Kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。<br>Kubernetes的最小管理单元是Pod而不是容器，所以只能将容器放在Pod中，而Kubernetes一般也不会直接管理Pod，而是通过Pod控制器来管理Pod的。<br>Pod提供服务之后，就需要考虑如何访问Pod中的服务，Kubernetes提供了Service资源实现这个功能。<br>当然，如果Pod中程序的数据需要持久化，Kubernetes还提供了各种存储系统。</p>
<p><strong>学习kubernets的核心，就是学习如何对集群中的Pod、Pod控制器、Service、存储等各种资源进行操作。</strong></p>
<h2 id="2-YAML语法介绍"><a href="#2-YAML语法介绍" class="headerlink" title="2 YAML语法介绍"></a>2 YAML语法介绍</h2><h3 id="2-1-YAML语法介绍"><a href="#2-1-YAML语法介绍" class="headerlink" title="2.1 YAML语法介绍"></a>2.1 YAML语法介绍</h3><p>YAML是一个类似于XML、JSON的标记性语言。它强调的是以“数据”为中心，并不是以标记语言为重点。因而YAML本身的定义比较简单，号称是“一种人性化的数据格式语言”。</p>
<p>YAML的语法比较简单，主要有下面的几个：</p>
<p>○大小写敏感。</p>
<p>○使用缩进表示层级关系。</p>
<p>○缩进不允许使用tab，只允许空格（低版本限制）。</p>
<p>○缩进的空格数不重要，只要相同层级的元素左对齐即可。</p>
<p>○‘#’表示注释。</p>
<p>YAML支持以下几种数据类型：</p>
<p>○常量：单个的、不能再分的值。</p>
<p>○对象：键值对的集合，又称为映射&#x2F;哈希&#x2F;字典。</p>
<p>○数组：一组按次序排列的值，又称为序列&#x2F;列表。</p>
<h3 id="2-2-YAML语法示例"><a href="#2-2-YAML语法示例" class="headerlink" title="2.2 YAML语法示例"></a><strong>2.2 YAML语法示例</strong></h3><h4 id="2-2-1-YAML常量"><a href="#2-2-1-YAML常量" class="headerlink" title="2.2.1 YAML常量"></a><strong>2.2.1 YAML常量</strong></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#常量，就是指的是一个简单的值，字符串、布尔值、整数、浮点数、NUll、时间、日期</span><br><span class="line"># 布尔类型 c1: true </span><br><span class="line"># 整型 c2: 123456 </span><br><span class="line"># 浮点类型 c3: 3.14 # null类型 c4: ~ </span><br><span class="line"># 使用~表示null </span><br><span class="line"># 日期类型 c5: 2019-11-11 </span><br><span class="line"># 日期类型必须使用ISO 8601格式，即yyyy-MM-dd </span><br><span class="line"># 时间类型 c6: 2019-11-11T15:02:31+08.00 </span><br><span class="line"># 时间类型使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区 </span><br><span class="line"># 字符串类型 c7: haha </span><br><span class="line"># 简单写法，直接写值，如果字符串中间有特殊符号，必须使用双引号或单引号包裹 c8: line1    line2 </span><br><span class="line"># 字符串过多的情况可以折成多行，每一行都会转换成一个空格</span><br></pre></td></tr></table></figure>

<h4 id="2-2-2-对象"><a href="#2-2-2-对象" class="headerlink" title="2.2.2 对象"></a><strong>2.2.2 对象</strong></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\# 对象 </span><br><span class="line"># 形式一（推荐）： </span><br><span class="line">xudaxian:	 </span><br><span class="line">  name: wjy </span><br><span class="line">  age: 20</span><br><span class="line"># 形式二（了解）： </span><br><span class="line">xuxian: &#123; name: wjy, age: 20 &#125;</span><br></pre></td></tr></table></figure>

<h4 id="2-2-3-数组"><a href="#2-2-3-数组" class="headerlink" title="2.2.3 数组"></a><strong>2.2.3 数组</strong></h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">\# 数组 </span><br><span class="line"># 形式一（推荐）： </span><br><span class="line">address: </span><br><span class="line">  - 江苏 </span><br><span class="line">  - 北京 </span><br><span class="line"># 形式二（了解）： </span><br><span class="line">address: [江苏,上海]</span><br></pre></td></tr></table></figure>

<h2 id="3-资源管理方式"><a href="#3-资源管理方式" class="headerlink" title="3 资源管理方式"></a><strong>3 资源管理方式</strong></h2><h3 id="3-1-资源管理方式"><a href="#3-1-资源管理方式" class="headerlink" title="3.1 资源管理方式"></a>3.1 资源管理方式</h3><p>命令式对象管理：直接使用命令去操作kubernetes的资源。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl run nginx3 --image=192.168.20.119/library/nginx:latest</span><br></pre></td></tr></table></figure>

<p>命令式对象配置：通过命令配置和配置文件去操作kubernetes的资源。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create/patch -f nginx-pod.yaml</span><br></pre></td></tr></table></figure>

<p>声明式对象配置：通过apply命令和配置文件去操作kubernetes的资源。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f nginx-pod.yaml</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>类型</th>
<th>操作</th>
<th>适用场景</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>命令式对象管理</td>
<td>对象</td>
<td>测试</td>
<td>简单</td>
<td>只能操作活动对象，无法审计、跟踪</td>
</tr>
<tr>
<td>命令式对象配置</td>
<td>文件</td>
<td>开发</td>
<td>可以审计、跟踪</td>
<td>项目大的时候，配置文件多，操作麻烦</td>
</tr>
<tr>
<td>声明式对象配置</td>
<td>目录</td>
<td>开发</td>
<td>支持目录操作</td>
<td>意外情况下难以调试</td>
</tr>
</tbody></table>
<h3 id="3-2-命令式对象管理"><a href="#3-2-命令式对象管理" class="headerlink" title="3.2 命令式对象管理"></a><strong>3.2 命令式对象管理</strong></h3><h4 id="3-2-1-kubectl命令"><a href="#3-2-1-kubectl命令" class="headerlink" title="3.2.1 kubectl命令"></a><strong>3.2.1 kubectl命令</strong></h4><p>●kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署。<br>●kubectl命令的语法如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl [command] [type] [name] [flags]</span><br></pre></td></tr></table></figure>

<p>●command：指定要对资源执行的操作，比如create、get、delete。<br>●type：指定资源的类型，比如deployment、pod、service。<br>●name：指定资源的名称，名称大小写敏感。<br>●flags：指定额外的可选参数。</p>
<p><strong>示例：查看所有的pod</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS             RESTARTS   AGE</span><br><span class="line">nginx                    1/1     Running            0          15h</span><br><span class="line">nginx-674ff86d-6cjt9     0/1     ImagePullBackOff   0          15h</span><br><span class="line">nginx1-cc97d58b4-st5mx   1/1     Running            0          38h</span><br><span class="line">nginx3                   1/1     Running            0          5m15s</span><br></pre></td></tr></table></figure>

<p><strong>示例：查看某个pod</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods nginx #加上pod名称</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx   1/1     Running   0          15h</span><br></pre></td></tr></table></figure>

<p><strong>示例：查看某个pod，以yaml格式展示结果</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods nginx -o yaml</span><br></pre></td></tr></table></figure>

<h4 id="3-2-2-操作（command）"><a href="#3-2-2-操作（command）" class="headerlink" title="3.2.2 操作（command）"></a><strong>3.2.2 操作（command</strong>）</h4><p>kubernetes允许对资源进行多种操作，可以通过–help查看详细的操作命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl -h</span><br><span class="line">kubectl controls the Kubernetes cluster manager.</span><br><span class="line"> Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/</span><br><span class="line">Basic Commands (Beginner):</span><br><span class="line">  create        Create a resource from a file or from stdin.</span><br><span class="line">  expose        使用 replication controller, service, deployment 或者 pod 并暴露它作为一个 新的 Kubernetes</span><br><span class="line">Service</span><br><span class="line">  run           在集群中运行一个指定的镜像</span><br><span class="line">  set           为 objects 设置一个指定的特征</span><br><span class="line">Basic Commands (Intermediate):</span><br><span class="line">  explain       查看资源的文档</span><br><span class="line">  get           显示一个或更多 resources</span><br><span class="line">  edit          在服务器上编辑一个资源</span><br><span class="line">  delete        Delete resources by filenames, stdin, resources and names, or by resources and label selector</span><br><span class="line">Deploy Commands:</span><br><span class="line">  rollout       Manage the rollout of a resource</span><br><span class="line">  scale         Set a new size for a Deployment, ReplicaSet or Replication Controller</span><br><span class="line">  autoscale     自动调整一个 Deployment, ReplicaSet, 或者 ReplicationController 的副本数量</span><br><span class="line">Cluster Management Commands:</span><br><span class="line">  certificate   修改 certificate 资源.</span><br><span class="line">  cluster-info  显示集群信息</span><br><span class="line">  top           Display Resource (CPU/Memory/Storage) usage.</span><br><span class="line">  cordon        标记 node 为 unschedulable</span><br><span class="line">  uncordon      标记 node 为 schedulable</span><br><span class="line">  drain         Drain node in preparation for maintenance</span><br><span class="line">  taint         更新一个或者多个 node 上的 taints</span><br><span class="line">Troubleshooting and Debugging Commands:</span><br><span class="line">  describe      显示一个指定 resource 或者 group 的 resources 详情</span><br><span class="line">  logs          输出容器在 pod 中的日志</span><br><span class="line">  attach        Attach 到一个运行中的 container</span><br><span class="line">  exec          在一个 container 中执行一个命令</span><br><span class="line">  port-forward  Forward one or more local ports to a pod</span><br><span class="line">  proxy         运行一个 proxy 到 Kubernetes API server</span><br><span class="line">  cp            复制 files 和 directories 到 containers 和从容器中复制 files 和 directories.</span><br><span class="line">  auth          Inspect authorization</span><br><span class="line">Advanced Commands:</span><br><span class="line">  diff          Diff live version against would-be applied version</span><br><span class="line">  apply         通过文件名或标准输入流(stdin)对资源进行配置</span><br><span class="line">  patch         使用 strategic merge patch 更新一个资源的 field(s)</span><br><span class="line">  replace       通过 filename 或者 stdin替换一个资源</span><br><span class="line">  wait          Experimental: Wait for a specific condition on one or many resources.</span><br><span class="line">  convert       在不同的 API versions 转换配置文件</span><br><span class="line">  kustomize     Build a kustomization target from a directory or a remote url.</span><br><span class="line">Settings Commands:</span><br><span class="line">  label         更新在这个资源上的 labels</span><br><span class="line">  annotate      更新一个资源的注解</span><br><span class="line">  completion    Output shell completion code for the specified shell (bash or zsh)</span><br><span class="line">Other Commands:</span><br><span class="line">  alpha         Commands for features in alpha</span><br><span class="line">  api-resources Print the supported API resources on the server</span><br><span class="line">  api-versions  Print the supported API versions on the server, in the form of &quot;group/version&quot;</span><br><span class="line">  config        修改 kubeconfig 文件</span><br><span class="line">  plugin        Provides utilities for interacting with plugins.</span><br><span class="line">  version       输出 client 和 server 的版本信息</span><br><span class="line">Usage:</span><br><span class="line">  kubectl [flags] [options]</span><br><span class="line">Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.</span><br><span class="line">Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).</span><br></pre></td></tr></table></figure>


<p>经常使用的操作如下所示：</p>
<p>① 基本命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>翻译</th>
<th>命令作用</th>
</tr>
</thead>
<tbody><tr>
<td>create</td>
<td>创建</td>
<td>创建一个资源</td>
</tr>
<tr>
<td>edit</td>
<td>编辑</td>
<td>编辑一个资源</td>
</tr>
<tr>
<td>get</td>
<td>获取</td>
<td>获取一个资源</td>
</tr>
<tr>
<td>patch</td>
<td>更新</td>
<td>更新一个资源</td>
</tr>
<tr>
<td>delete</td>
<td>删除</td>
<td>删除一个资源</td>
</tr>
<tr>
<td>explain</td>
<td>解释</td>
<td>展示资源文档</td>
</tr>
</tbody></table>
<p>② 运行和调试：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>翻译</th>
<th>命令作用</th>
</tr>
</thead>
<tbody><tr>
<td>run</td>
<td>运行</td>
<td>在集群中运行一个指定的镜像</td>
</tr>
<tr>
<td>expose</td>
<td>暴露</td>
<td>暴露资源为Service</td>
</tr>
<tr>
<td>describe</td>
<td>描述</td>
<td>显示资源内部信息</td>
</tr>
<tr>
<td>logs</td>
<td>日志</td>
<td>输出容器在Pod中的日志</td>
</tr>
<tr>
<td>attach</td>
<td>缠绕</td>
<td>进入运行中的容器</td>
</tr>
<tr>
<td>exec</td>
<td>执行</td>
<td>执行容器中的一个命令</td>
</tr>
<tr>
<td>cp</td>
<td>复制</td>
<td>在Pod内外复制文件</td>
</tr>
<tr>
<td>rollout</td>
<td>首次展示</td>
<td>管理资源的发布</td>
</tr>
<tr>
<td>scale</td>
<td>规模</td>
<td>扩（缩）容Pod的数量</td>
</tr>
<tr>
<td>autoscale</td>
<td>自动调整</td>
<td>自动调整Pod的数量</td>
</tr>
</tbody></table>
<p>③ 高级命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>翻译</th>
<th>命令作用</th>
</tr>
</thead>
<tbody><tr>
<td>apply</td>
<td>应用</td>
<td>通过文件对资源进行配置</td>
</tr>
<tr>
<td>label</td>
<td>标签</td>
<td>更新资源上的标签</td>
</tr>
</tbody></table>
<p>④ 其他命令：</p>
<table>
<thead>
<tr>
<th>命令</th>
<th>翻译</th>
<th>命令作用</th>
</tr>
</thead>
<tbody><tr>
<td>cluster-info</td>
<td>集群信息</td>
<td>显示集群信息</td>
</tr>
<tr>
<td>version</td>
<td>版本</td>
<td>显示当前Client和Server的版本</td>
</tr>
</tbody></table>
<h4 id="3-2-3-资源类型（type）"><a href="#3-2-3-资源类型（type）" class="headerlink" title="3.2.3 资源类型（type）"></a><strong>3.2.3 资源类型（type）</strong></h4><p>kubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl api-resources</span><br><span class="line">NAME                              SHORTNAMES   APIGROUP                       NAMESPACED   KIND</span><br><span class="line">bindings                                                                      true         Binding</span><br><span class="line">componentstatuses                 cs                                          false        ComponentStatus</span><br><span class="line">configmaps                        cm                                          true         ConfigMap</span><br><span class="line">endpoints                         ep                                          true         Endpoints</span><br><span class="line">events                            ev                                          true         Event</span><br><span class="line">limitranges                       limits                                      true         LimitRange</span><br><span class="line">namespaces                        ns                                          false        Namespace</span><br><span class="line">nodes                             no                                          false        Node</span><br><span class="line">persistentvolumeclaims            pvc                                         true         PersistentVolumeClaim</span><br><span class="line">persistentvolumes                 pv                                          false        PersistentVolume</span><br><span class="line">pods                              po                                          true         Pod</span><br><span class="line">podtemplates                                                                  true         PodTemplate</span><br><span class="line">replicationcontrollers            rc                                          true         ReplicationController</span><br><span class="line">resourcequotas                    quota                                       true         ResourceQuota</span><br><span class="line">secrets                                                                       true         Secret</span><br><span class="line">serviceaccounts                   sa                                          true         ServiceAccount</span><br><span class="line">services                          svc                                         true         Service</span><br><span class="line">mutatingwebhookconfigurations                  admissionregistration.k8s.io   false        MutatingWebhookConfiguration</span><br><span class="line">validatingwebhookconfigurations                admissionregistration.k8s.io   false        ValidatingWebhookConfiguration</span><br><span class="line">customresourcedefinitions         crd,crds     apiextensions.k8s.io           false        CustomResourceDefinition</span><br><span class="line">apiservices                                    apiregistration.k8s.io         false        APIService</span><br><span class="line">controllerrevisions                            apps                           true         ControllerRevision</span><br><span class="line">daemonsets                        ds           apps                           true         DaemonSet</span><br><span class="line">deployments                       deploy       apps                           true         Deployment</span><br><span class="line">replicasets                       rs           apps                           true         ReplicaSet</span><br><span class="line">statefulsets                      sts          apps                           true         StatefulSet</span><br><span class="line">tokenreviews                                   authentication.k8s.io          false        TokenReview</span><br><span class="line">localsubjectaccessreviews                      authorization.k8s.io           true         LocalSubjectAccessReview</span><br><span class="line">selfsubjectaccessreviews                       authorization.k8s.io           false        SelfSubjectAccessReview</span><br><span class="line">selfsubjectrulesreviews                        authorization.k8s.io           false        SelfSubjectRulesReview</span><br><span class="line">subjectaccessreviews                           authorization.k8s.io           false        SubjectAccessReview</span><br><span class="line">horizontalpodautoscalers          hpa          autoscaling                    true         HorizontalPodAutoscaler</span><br><span class="line">cronjobs                          cj           batch                          true         CronJob</span><br><span class="line">jobs                                           batch                          true         Job</span><br><span class="line">certificatesigningrequests        csr          certificates.k8s.io            false        CertificateSigningRequest</span><br><span class="line">leases                                         coordination.k8s.io            true         Lease</span><br><span class="line">endpointslices                                 discovery.k8s.io               true         EndpointSlice</span><br><span class="line">events                            ev           events.k8s.io                  true         Event</span><br><span class="line">ingresses                         ing          extensions                     true         Ingress</span><br><span class="line">ingressclasses                                 networking.k8s.io              false        IngressClass</span><br><span class="line">ingresses                         ing          networking.k8s.io              true         Ingress</span><br><span class="line">networkpolicies                   netpol       networking.k8s.io              true         NetworkPolicy</span><br><span class="line">runtimeclasses                                 node.k8s.io                    false        RuntimeClass</span><br><span class="line">poddisruptionbudgets              pdb          policy                         true         PodDisruptionBudget</span><br><span class="line">podsecuritypolicies               psp          policy                         false        PodSecurityPolicy</span><br><span class="line">clusterrolebindings                            rbac.authorization.k8s.io      false        ClusterRoleBinding</span><br><span class="line">clusterroles                                   rbac.authorization.k8s.io      false        ClusterRole</span><br><span class="line">rolebindings                                   rbac.authorization.k8s.io      true         RoleBinding</span><br><span class="line">roles                                          rbac.authorization.k8s.io      true         Role</span><br><span class="line">priorityclasses                   pc           scheduling.k8s.io              false        PriorityClass</span><br><span class="line">csidrivers                                     storage.k8s.io                 false        CSIDriver</span><br><span class="line">csinodes                                       storage.k8s.io                 false        CSINode</span><br><span class="line">storageclasses                    sc           storage.k8s.io                 false        StorageClass</span><br><span class="line">volumeattachments                              storage.k8s.io                 false        VolumeAttachment</span><br></pre></td></tr></table></figure>

<p>经常使用的资源如下所示：</p>
<p>① 集群级别资源：</p>
<table>
<thead>
<tr>
<th>资源名称</th>
<th>缩写</th>
<th>资源作用</th>
</tr>
</thead>
<tbody><tr>
<td>nodes</td>
<td>no</td>
<td>集群组成部分</td>
</tr>
<tr>
<td>namespaces</td>
<td>ns</td>
<td>隔离Pod</td>
</tr>
</tbody></table>
<p>② Pod资源：</p>
<table>
<thead>
<tr>
<th>资源名称</th>
<th>缩写</th>
<th>资源作用</th>
</tr>
</thead>
<tbody><tr>
<td>Pods</td>
<td>po</td>
<td>装载容器</td>
</tr>
</tbody></table>
<p>③ Pod资源控制器：</p>
<table>
<thead>
<tr>
<th>资源名称</th>
<th>缩写</th>
<th>资源作用</th>
</tr>
</thead>
<tbody><tr>
<td>replicationcontrollers</td>
<td>rc</td>
<td>控制Pod资源</td>
</tr>
<tr>
<td>replicasets</td>
<td>rs</td>
<td>控制Pod资源</td>
</tr>
<tr>
<td>deployments</td>
<td>deploy</td>
<td>控制Pod资源</td>
</tr>
<tr>
<td>daemonsets</td>
<td>ds</td>
<td>控制Pod资源</td>
</tr>
<tr>
<td>jobs</td>
<td></td>
<td>控制Pod资源</td>
</tr>
<tr>
<td>cronjobs</td>
<td>cj</td>
<td>控制Pod资源</td>
</tr>
<tr>
<td>horizontalpodautoscalers</td>
<td>hpa</td>
<td>控制Pod资源</td>
</tr>
<tr>
<td>statefulsets</td>
<td>sts</td>
<td>控制Pod资源</td>
</tr>
</tbody></table>
<p>④ 服务发现资源：</p>
<table>
<thead>
<tr>
<th>资源名称</th>
<th>缩写</th>
<th>资源作用</th>
</tr>
</thead>
<tbody><tr>
<td>services</td>
<td>svc</td>
<td>统一Pod对外接口</td>
</tr>
<tr>
<td>ingress</td>
<td>ing</td>
<td>统一Pod对外接口</td>
</tr>
</tbody></table>
<p>⑤ 存储资源：</p>
<table>
<thead>
<tr>
<th>资源名称</th>
<th>缩写</th>
<th>资源作用</th>
</tr>
</thead>
<tbody><tr>
<td>volumeattachments</td>
<td></td>
<td>存储</td>
</tr>
<tr>
<td>persistentvolumes</td>
<td>pv</td>
<td>存储</td>
</tr>
<tr>
<td>persistentvolumeclaims</td>
<td>pvc</td>
<td>存储</td>
</tr>
</tbody></table>
<p>⑥ 配置资源：</p>
<table>
<thead>
<tr>
<th>资源名称</th>
<th>缩写</th>
<th>资源作用</th>
</tr>
</thead>
<tbody><tr>
<td>configmaps</td>
<td>cm</td>
<td>配置</td>
</tr>
<tr>
<td>secrets</td>
<td></td>
<td>配置</td>
</tr>
</tbody></table>
<h4 id="3-2-4-应用示例"><a href="#3-2-4-应用示例" class="headerlink" title="3.2.4 应用示例"></a><strong>3.2.4 应用示例</strong></h4><p>示例：创建一个namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create ns test #ns为namespace简写</span><br><span class="line">namespace/test created</span><br></pre></td></tr></table></figure>

<p>示例：获取namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   39h</span><br><span class="line">dev                    Active   15h</span><br><span class="line">kube-node-lease        Active   39h</span><br><span class="line">kube-public            Active   39h</span><br><span class="line">kube-system            Active   39h</span><br><span class="line">kubernetes-dashboard   Active   39h</span><br><span class="line">test                   Active   27s</span><br></pre></td></tr></table></figure>

<p>示例：在刚才创建的namespace下创建并运行一个Nginx的Pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl run nginx --image=192.168.20.119/library/nginx:latest -n test </span><br><span class="line">pod/nginx created</span><br></pre></td></tr></table></figure>

<p>示例：查看名为dev的namespace下的所有Pod，如果不加-n，默认就是default的namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n test</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx   1/1     Running   0          34s</span><br></pre></td></tr></table></figure>

<p>示例：删除指定namespace下的指定Pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl delete pod nginx -n test</span><br><span class="line">pod &quot;nginx&quot; deleted</span><br></pre></td></tr></table></figure>

<p>示例：删除指定的namespace</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl delete namespace test</span><br><span class="line">namespace &quot;test&quot; deleted</span><br></pre></td></tr></table></figure>

<h3 id="3-3-命令式对象配置"><a href="#3-3-命令式对象配置" class="headerlink" title="3.3 命令式对象配置"></a><strong>3.3 命令式对象配置</strong></h3><h4 id="3-3-1-概述"><a href="#3-3-1-概述" class="headerlink" title="3.3.1 概述"></a>3.3.1 概述</h4><p>命令式对象配置：通过命令配置和配置文件去操作kubernetes的资源。</p>
<h4 id="3-3-2-应用示例"><a href="#3-3-2-应用示例" class="headerlink" title="3.3.2 应用示例"></a>3.3.2 应用示例</h4><p>示例：<br>创建一个nginxpod.yaml，内容如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat nginxpod.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: test</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginxpod</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">   - name: nginx-containers</span><br><span class="line">     image: 192.168.20.119/library/nginx:latest</span><br></pre></td></tr></table></figure>

<p>执行create命令，创建资源</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create -f nginxpod.yaml </span><br><span class="line">namespace/test created</span><br><span class="line">pod/nginxpod created</span><br></pre></td></tr></table></figure>

<p> 执行get命令，查看资源：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get -f nginxpod.yaml </span><br><span class="line">NAME             STATUS   AGE</span><br><span class="line">namespace/test   Active   48s</span><br><span class="line"></span><br><span class="line">NAME           READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/nginxpod   1/1     Running   0          48s</span><br></pre></td></tr></table></figure>

<p>执行delete命令，删除资源：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl delete -f nginxpod.yaml </span><br><span class="line">namespace &quot;test&quot; deleted</span><br><span class="line">pod &quot;nginxpod&quot; deleted</span><br></pre></td></tr></table></figure>

<h4 id="3-3-3-总结"><a href="#3-3-3-总结" class="headerlink" title="3.3.3 总结"></a><strong>3.3.3 总结</strong></h4><p>命令式对象配置的方式操作资源，可以简单的认为：命令+yaml配置文件（里面是命令需要的各种参数）。</p>
<h3 id="3-4-声明式对象配置"><a href="#3-4-声明式对象配置" class="headerlink" title="3.4 声明式对象配置"></a><strong>3.4 声明式对象配置</strong></h3><h4 id="3-4-1-概述"><a href="#3-4-1-概述" class="headerlink" title="3.4.1 概述"></a><strong>3.4.1 概述</strong></h4><p>声明式对象配置：通过apply命令和配置文件去操作kubernetes的资源。<br>声明式对象配置和命令式对象配置类似，只不过它只有一个apply命令。<br>apply相当于create和patch。</p>
<h4 id="3-4-2-应用示例"><a href="#3-4-2-应用示例" class="headerlink" title="3.4.2 应用示例"></a><strong>3.4.2 应用示例</strong></h4><p>示例：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f nginxpod.yaml </span><br><span class="line">namespace/test created</span><br><span class="line">pod/nginxpod created</span><br></pre></td></tr></table></figure>

<h4 id="3-4-3-总结"><a href="#3-4-3-总结" class="headerlink" title="3.4.3 总结"></a>3.4.3 总结</h4><p>声明式对象配置就是使用apply描述一个资源的最终状态（在yaml中定义状态）。<br>使用apply操作资源：<br>如果资源不存在，就创建，相当于kubectl create。<br>如果资源存在，就更新，相当于kubectl patch。</p>
<h3 id="3-5-使用方式推荐"><a href="#3-5-使用方式推荐" class="headerlink" title="3.5 使用方式推荐"></a>3.5 使用方式推荐</h3><p>●创建和更新资源使用声明式对象配置：kubectl apply -f xxx.yaml。<br>●删除资源使用命令式对象配置：kubectl delete -f xxx.yaml。<br>●查询资源使用命令式对象管理：kubectl get(describe) 资源名称。</p>
<h3 id="3-6-扩展：kubectl可以在Node上运行"><a href="#3-6-扩展：kubectl可以在Node上运行" class="headerlink" title="3.6 扩展：kubectl可以在Node上运行"></a><strong>3.6 扩展：kubectl可以在Node上运行</strong></h3><p>kubectl的运行需要进行配置，它的配置文件是$HOME&#x2F;.kube，如果想要在Node节点上运行此命令，需要将Master节点的.kube文件夹复制到Node节点上，即在Master节点上执行下面的操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# scp -r /root/.kube/ root@node1:/root/</span><br></pre></td></tr></table></figure>

<h2 id="4-如何快速的编写yaml文件"><a href="#4-如何快速的编写yaml文件" class="headerlink" title="4 如何快速的编写yaml文件"></a><strong>4 如何快速的编写yaml文件</strong></h2><p>此种方式适用于没有真正部署资源。<br>使用kubectl create命令生成yaml文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create deployment nginx --image=192.168.20.119/library/nginx:latest --dry-run=client -n dev -o yaml &gt; test.yaml</span><br><span class="line">[root@master ~]# cat test.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: 192.168.20.119/library/nginx:latest</span><br><span class="line">        name: nginx</span><br><span class="line">        resources: &#123;&#125;</span><br><span class="line">status: &#123;&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s学习日记day3</title>
    <url>/2022/05/20/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day3/</url>
    <content><![CDATA[<h1 id="kubernetes的实战入门"><a href="#kubernetes的实战入门" class="headerlink" title="kubernetes的实战入门"></a>kubernetes的实战入门<span id="more"></span></h1><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><ul>
<li>介绍如何在kubernetes集群中部署一个Nginx服务，并且能够对其访问。</li>
</ul>
<h2 id="2-Namespace"><a href="#2-Namespace" class="headerlink" title="2 Namespace"></a>2 Namespace</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><ul>
<li><p>Namespace是kubernetes系统中一种非常重要的资源，它的主要作用是用来实现<code>多套系统的资源隔离</code>或者<code>多租户的资源隔离</code>。</p>
</li>
<li><p>默认情况下，kubernetes集群中的所有Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那么此时就可以将两个Pod划分到不同的Namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的“组”，以方便不同的组的资源进行隔离使用和管理。</p>
</li>
<li><p>可以通过kubernetes的授权机制，将不同的Namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。</p>
</li>
</ul>
<p><strong>kubernetes在集群启动之后，会默认创建几个namespace。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   2d14h #自动创建</span><br><span class="line">dev                    Active   39h</span><br><span class="line">kube-node-lease        Active   2d14h #自动创建</span><br><span class="line">kube-public            Active   2d14h #自动创建</span><br><span class="line">kube-system            Active   2d14h #自动创建</span><br><span class="line">kubernetes-dashboard   Active   2d14h</span><br><span class="line">test                   Active   23h</span><br></pre></td></tr></table></figure>

<ul>
<li><p>default：所有未指定的Namespace的对象都会被分配在default命名空间。</p>
</li>
<li><p>kube-node-lease：集群节点之间的心跳维护，v1.13开始引入。</p>
</li>
<li><p>kube-public：此命名空间的资源可以被所有人访问（包括未认证用户）。</p>
</li>
<li><p>kube-system：所有由kubernetes系统创建的资源都处于这个命名空间。</p>
</li>
</ul>
<h3 id="2-2-应用示例"><a href="#2-2-应用示例" class="headerlink" title="2.2 应用示例"></a>2.2 应用示例</h3><ul>
<li>示例：查看所有的命名空间</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   2d14h</span><br><span class="line">dev                    Active   39h</span><br><span class="line">kube-node-lease        Active   2d14h</span><br><span class="line">kube-public            Active   2d14h</span><br><span class="line">kube-system            Active   2d14h</span><br><span class="line">kubernetes-dashboard   Active   2d14h</span><br><span class="line">test                   Active   23h</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：查看指定的命名空间</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns default</span><br><span class="line">NAME      STATUS   AGE</span><br><span class="line">default   Active   2d14h</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：指定命名空间的输出格式</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns default -o wide</span><br><span class="line">NAME      STATUS   AGE</span><br><span class="line">default   Active   2d14h</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns default -o json</span><br><span class="line">&#123;</span><br><span class="line">    &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">    &quot;kind&quot;: &quot;Namespace&quot;,</span><br><span class="line">    &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;creationTimestamp&quot;: &quot;2022-05-18T05:17:51Z&quot;,</span><br><span class="line">        &quot;managedFields&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">                &quot;fieldsType&quot;: &quot;FieldsV1&quot;,</span><br><span class="line">                &quot;fieldsV1&quot;: &#123;</span><br><span class="line">                    &quot;f:status&quot;: &#123;</span><br><span class="line">                        &quot;f:phase&quot;: &#123;&#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;,</span><br><span class="line">                &quot;manager&quot;: &quot;kube-apiserver&quot;,</span><br><span class="line">                &quot;operation&quot;: &quot;Update&quot;,</span><br><span class="line">                &quot;time&quot;: &quot;2022-05-18T05:17:51Z&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;name&quot;: &quot;default&quot;,</span><br><span class="line">        &quot;resourceVersion&quot;: &quot;152&quot;,</span><br><span class="line">        &quot;selfLink&quot;: &quot;/api/v1/namespaces/default&quot;,</span><br><span class="line">        &quot;uid&quot;: &quot;de6edb5b-6b5d-4f5a-a901-f840831bc796&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;spec&quot;: &#123;</span><br><span class="line">        &quot;finalizers&quot;: [</span><br><span class="line">            &quot;kubernetes&quot;</span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;status&quot;: &#123;</span><br><span class="line">        &quot;phase&quot;: &quot;Active&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns default -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2022-05-18T05:17:51Z&quot;</span><br><span class="line">  managedFields:</span><br><span class="line"></span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:status:</span><br><span class="line">        f:phase: &#123;&#125;</span><br><span class="line">    manager: kube-apiserver</span><br><span class="line">    operation: Update</span><br><span class="line">    time: &quot;2022-05-18T05:17:51Z&quot;</span><br><span class="line">      name: default</span><br><span class="line">      resourceVersion: &quot;152&quot;</span><br><span class="line">      selfLink: /api/v1/namespaces/default</span><br><span class="line">      uid: de6edb5b-6b5d-4f5a-a901-f840831bc796</span><br><span class="line">    spec:</span><br><span class="line">      finalizers:</span><br><span class="line">  - kubernetes</span><br><span class="line">    status:</span><br><span class="line">      phase: Active</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：查看命名空间的详情</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl describe ns default</span><br><span class="line">Name:         default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Active #Active表示命名空间正在使用中，Terminating表示正在删除</span><br><span class="line"></span><br><span class="line">No resource quota.</span><br><span class="line"></span><br><span class="line">No LimitRange resource.</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：创建命名空间</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create ns dev</span><br><span class="line">namespace/dev created</span><br><span class="line">[root@master ~]# kubectl get ns </span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   2d14h</span><br><span class="line">dev                    Active   45s</span><br><span class="line">kube-node-lease        Active   2d14h</span><br><span class="line">kube-public            Active   2d14h</span><br><span class="line">kube-system            Active   2d14h</span><br><span class="line">kubernetes-dashboard   Active   2d14h</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：删除命名空间</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get ns </span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   2d14h</span><br><span class="line">dev                    Active   45s</span><br><span class="line">kube-node-lease        Active   2d14h</span><br><span class="line">kube-public            Active   2d14h</span><br><span class="line">kube-system            Active   2d14h</span><br><span class="line">kubernetes-dashboard   Active   2d14h</span><br><span class="line">[root@master ~]# kubectl delete ns dev</span><br><span class="line">namespace &quot;dev&quot; deleted</span><br><span class="line">[root@master ~]# kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   2d15h</span><br><span class="line">kube-node-lease        Active   2d15h</span><br><span class="line">kube-public            Active   2d15h</span><br><span class="line">kube-system            Active   2d15h</span><br><span class="line">kubernetes-dashboard   Active   2d14h</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：命令式对象配置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat ns-dev.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: dev</span><br><span class="line">[root@master ~]# kubectl create -f ns-dev.yaml </span><br><span class="line">namespace/dev created</span><br><span class="line">[root@master ~]# kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   2d15h</span><br><span class="line">dev                    Active   4s</span><br><span class="line">kube-node-lease        Active   2d15h</span><br><span class="line">kube-public            Active   2d15h</span><br><span class="line">kube-system            Active   2d15h</span><br><span class="line">kubernetes-dashboard   Active   2d14h</span><br><span class="line">[root@master ~]# kubectl delete -f ns-dev.yaml </span><br><span class="line">namespace &quot;dev&quot; deleted</span><br><span class="line">[root@master ~]# kubectl get ns</span><br><span class="line">NAME                   STATUS   AGE</span><br><span class="line">default                Active   2d15h</span><br><span class="line">kube-node-lease        Active   2d15h</span><br><span class="line">kube-public            Active   2d15h</span><br><span class="line">kube-system            Active   2d15h</span><br><span class="line">kubernetes-dashboard   Active   2d14h</span><br></pre></td></tr></table></figure>

<h2 id="3-Pod"><a href="#3-Pod" class="headerlink" title="3 Pod"></a>3 Pod</h2><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><ul>
<li><p>Pod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。</p>
</li>
<li><p>Pod可以认为是容器的封装，一个Pod中可以存在一个或多个容器。</p>
</li>
</ul>
<img src="/2022/05/20/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day3/1609137223448-715c6ece-0158-4ee2-9efa-fcff15e143ed.png" class="" title="img">

<ul>
<li>kubernetes在集群启动之后，集群中的各个组件也是以Pod方式运行的，可以通过下面的命令查看：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n kube-system</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">coredns-6fcfc67db4-6tdxr         1/1     Running   0          2d15h</span><br><span class="line">coredns-6fcfc67db4-b6m9j         1/1     Running   0          2d15h</span><br><span class="line">etcd-master                      1/1     Running   0          2d15h</span><br><span class="line">kube-apiserver-master            1/1     Running   0          2d15h</span><br><span class="line">kube-controller-manager-master   1/1     Running   0          2d15h</span><br><span class="line">kube-flannel-ds-l4trs            1/1     Running   0          2d14h</span><br><span class="line">kube-flannel-ds-lbrnt            1/1     Running   1          2d14h</span><br><span class="line">kube-flannel-ds-xpdl2            1/1     Running   0          2d14h</span><br><span class="line">kube-proxy-d7vxn                 1/1     Running   0          2d15h</span><br><span class="line">kube-proxy-gfnq5                 1/1     Running   0          2d14h</span><br><span class="line">kube-proxy-x8qgm                 1/1     Running   0          2d14h</span><br><span class="line">kube-scheduler-master            1/1     Running   0          2d15h</span><br></pre></td></tr></table></figure>

<h3 id="3-2-语法及应用示例"><a href="#3-2-语法及应用示例" class="headerlink" title="3.2 语法及应用示例"></a>3.2 语法及应用示例</h3><ul>
<li>语法：创建并运行Pod</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kubectl run (Pod的名称) [参数]</span><br><span class="line"># --image 指定Pod的镜像</span><br><span class="line"># --port 指定端口</span><br><span class="line"># --namespace 指定namespace</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：在名称为dev的namespace下创建一个Nginx的Pod</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl run nginx --image=192.168.20.119/library/nginx:latest --port 80 -n dev</span><br><span class="line">pod/nginx created</span><br></pre></td></tr></table></figure>

<ul>
<li>语法： 查询所有Pod的基本信息</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx   1/1     Running   0          6s</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：查看Pod的详细信息</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl describe pods -n dev</span><br><span class="line">Name:         nginx</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node2/192.168.20.124</span><br><span class="line">Start Time:   Fri, 20 May 2022 20:29:32 +0000</span><br><span class="line">Labels:       run=nginx</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           10.244.2.13</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.2.13</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://a524a5ec4c57ed61996ef2e06580cafe19254aa2ed2ac079a07e5e0e1bfda589</span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83</span><br><span class="line">    Port:           80/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Fri, 20 May 2022 20:29:33 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-chm54 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             True </span><br><span class="line">  ContainersReady   True </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-chm54:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-chm54</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From               Message</span><br><span class="line"></span><br><span class="line">----    ------     ----  ----               -------</span><br><span class="line"></span><br><span class="line">  Normal  Scheduled  2m3s  default-scheduler  Successfully assigned dev/nginx to node2</span><br><span class="line">  Normal  Pulling    2m2s  kubelet, node2     Pulling image &quot;192.168.20.119/library/nginx:latest&quot;</span><br><span class="line">  Normal  Pulled     2m2s  kubelet, node2     Successfully pulled image &quot;192.168.20.119/library/nginx:latest&quot;</span><br><span class="line">  Normal  Created    2m2s  kubelet, node2     Created container nginx</span><br><span class="line">  Normal  Started    2m2s  kubelet, node2     Started container nginx</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：Pod的访问</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -o wide -n dev</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE     IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">nginx   1/1     Running   0          3m18s   10.244.2.13   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@master ~]# curl 10.244.2.13:80</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：删除指定的Pod</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl delete pods nginx -n dev</span><br><span class="line">pod &quot;nginx&quot; deleted</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">No resources found in dev namespace.</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：命令式对象配置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-nginx.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line"></span><br><span class="line">  - image: 192.168.20.119/library/nginx:latest</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: pod</span><br><span class="line">    ports: </span><br><span class="line">    - name: nginx-port</span><br><span class="line">      containerPort: 80</span><br><span class="line">      protocol: TCP</span><br><span class="line">[root@master ~]# kubectl create -f pod-nginx.yaml </span><br><span class="line">pod/nginx created</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx   1/1     Running   0          6s</span><br><span class="line">[root@master ~]# kubectl delete -f pod-nginx.yaml </span><br><span class="line">pod &quot;nginx&quot; deleted</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">No resources found in dev namespace.</span><br></pre></td></tr></table></figure>

<h2 id="4-Label"><a href="#4-Label" class="headerlink" title="4 Label"></a>4 Label</h2><h3 id="4-1-概述"><a href="#4-1-概述" class="headerlink" title="4.1 概述"></a>4.1 概述</h3><ul>
<li><p>Label是kubernetes的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。</p>
</li>
<li><p>Label的特点：</p>
</li>
<li><ul>
<li>一个Label会以key&#x2F;value键值对的形式附加到各种对象上，如Node、Pod、Service等。</li>
</ul>
</li>
<li><ul>
<li>一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上去。</li>
</ul>
</li>
<li><ul>
<li>Label通常在资源对象定义时确定，当然也可以在对象创建后动态的添加或删除。</li>
</ul>
</li>
<li><p>可以通过Label实现资源的多纬度分组，以便灵活、方便地进行资源分配、调度、配置和部署等管理工作。</p>
</li>
</ul>
<p><strong>一些常用的Label标签示例如下：</strong></p>
<blockquote>
<ul>
<li><p>版本标签：“version”:”release”,”version”:”stable”。。。</p>
</li>
<li><p>环境标签：“environment”:”dev”,“environment”:”test”,“environment”:”pro”。。。</p>
</li>
<li><p>架构标签：“tier”:”frontend”,”tier”:”backend”。。。</p>
</li>
</ul>
</blockquote>
<ul>
<li><p>标签定义完毕之后，还要考虑到标签的选择，这就要用到Label Selector，即：</p>
</li>
<li><ul>
<li>Label用于给某个资源对象定义标识。</li>
</ul>
</li>
<li><ul>
<li>Label Selector用于查询和筛选拥有某些标签的资源对象。</li>
</ul>
</li>
<li><p>当前有两种Label Selector：</p>
</li>
<li><ul>
<li>基于等式的Label Selector。</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>name&#x3D;slave：选择所有包含Label中的key&#x3D;“name”并且value&#x3D;“slave”的对象。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>env!&#x3D;production：选择所有包含Label中的key&#x3D;“env”并且value!&#x3D;“production”的对象。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>基于集合的Label Selector。</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>name in (master,slave)：选择所有包含Label中的key&#x3D;“name”并且value&#x3D;“master”或value&#x3D;“slave”的对象。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>name not in (master,slave)：选择所有包含Label中的key&#x3D;“name”并且value!&#x3D;“master”和value!&#x3D;“slave”的对象。</li>
</ul>
</li>
</ul>
</li>
<li><p>标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号（,）进行分隔即可。</p>
</li>
<li><ul>
<li>name&#x3D;salve,env!&#x3D;production。</li>
</ul>
</li>
<li><ul>
<li>name not in (master,slave),env!&#x3D;production。</li>
</ul>
</li>
</ul>
<h3 id="4-2-语法及应用示例"><a href="#4-2-语法及应用示例" class="headerlink" title="4.2 语法及应用示例"></a>4.2 语法及应用示例</h3><ul>
<li>语法：为资源打标签</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl label pod nginx version=2.0 -n dev</span><br><span class="line">pod/nginx labeled</span><br><span class="line">[root@master ~]# kubectl get pods -n dev --show-labels</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE   LABELS</span><br><span class="line">nginx   1/1     Running   0          13m   version=2.0</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：更新资源的标签</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl label pod nginx version=3.0 -n dev --overwrite</span><br><span class="line">pod/nginx labeled</span><br><span class="line">[root@master ~]# kubectl get pods -n dev --show-labels</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE   LABELS</span><br><span class="line">nginx   1/1     Running   0          15m   version=3.0</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：显示Nginx的Pod的标签</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n dev --show-labels</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE   LABELS</span><br><span class="line">nginx   1/1     Running   0          15m   version=3.0</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：筛选标签</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -l version=3.0 -n dev --show-labels</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE   LABELS</span><br><span class="line">nginx   1/1     Running   0          16m   version=3.0</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：删除标签</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl label pods nginx -n dev version-</span><br><span class="line">pod/nginx labeled</span><br><span class="line">[root@master ~]# kubectl get pods -n dev --show-labels</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE   LABELS</span><br><span class="line">nginx   1/1     Running   0          18m   &lt;none&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：命令式对象配置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-nginx.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    version: &quot;3.0&quot;</span><br><span class="line">    env: &quot;test&quot;</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: 192.168.20.119/library/nginx:latest</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: pod</span><br><span class="line">    ports: </span><br><span class="line">    - name: nginx-port</span><br><span class="line">      containerPort: 80</span><br><span class="line">      protocol: TCP</span><br><span class="line">[root@master ~]# kubectl apply -f pod-nginx.yaml </span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">pod/nginx configured</span><br><span class="line">[root@master ~]# kubectl get pods -n dev --show-labels</span><br><span class="line">NAME    READY   STATUS    RESTARTS   AGE   LABELS</span><br><span class="line">nginx   1/1     Running   0          20m   env=test,version=3.0</span><br></pre></td></tr></table></figure>

<h2 id="5-Deployment"><a href="#5-Deployment" class="headerlink" title="5 Deployment"></a>5 Deployment</h2><h3 id="5-1-概述"><a href="#5-1-概述" class="headerlink" title="5.1 概述"></a>5.1 概述</h3><ul>
<li><p>在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。</p>
</li>
<li><p>Pod控制器用于Pod的管理，确保Pod资源符合预期的状态，当Pod的资源出现故障的时候，会尝试进行重启或重建Pod。</p>
</li>
<li><p>在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。</p>
</li>
</ul>
<img src="/2022/05/20/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day3/1609137432883-b2ec0213-7c10-4efa-9689-066a4e239a74.png" class="" title="img">

<h3 id="5-2-语法及应用示例"><a href="#5-2-语法及应用示例" class="headerlink" title="5.2 语法及应用示例"></a>5.2 语法及应用示例</h3><blockquote>
<p>特别注意：在v1.18版之后，kubectl run nginx –image&#x3D;nginx –replicas&#x3D;2 –port&#x3D;80，会反馈Flag –replicas has been deprecated, has no effect and will be removed in the future，并且只会创建一个Nginx容器实例。</p>
</blockquote>
<ul>
<li>示例：在名称为test的命名空间下创建名为nginx的deployment</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create ns test</span><br><span class="line">namespace/test created</span><br><span class="line">[root@master ~]# kubectl create deploy nginx --image=192.168.20.119/library/nginx:latest -n test</span><br><span class="line">deployment.apps/nginx created</span><br><span class="line">[root@master ~]# kubectl get deploy -n test</span><br><span class="line">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx   1/1     1            1           18s</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：在名称为test的命名空间下根据名为nginx的deployment创建3个Pod</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl scale deploy nginx --replicas=3 -n test</span><br><span class="line">deployment.apps/nginx scaled</span><br><span class="line">[root@master ~]# kubectl get pods -n test</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-56669589d9-h67ng   1/1     Running   0          3s</span><br><span class="line">nginx-56669589d9-l2vb5   1/1     Running   0          3s</span><br><span class="line">nginx-56669589d9-lmp9k   1/1     Running   0          2m55s</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：命令式对象配置</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat deploy-nginx.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: 192.168.20.119/library/nginx:latest</span><br><span class="line">        name: nginx</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">[root@master ~]# kubectl create -f deploy-nginx.yaml </span><br><span class="line">deployment.apps/nginx created</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME                   READY   STATUS    RESTARTS   AGE</span><br><span class="line">nginx-85dbb646-fqft8   1/1     Running   0          3s</span><br><span class="line">nginx-85dbb646-lnhjp   1/1     Running   0          3s</span><br><span class="line">nginx-85dbb646-w4pjb   1/1     Running   0          3s</span><br><span class="line">[root@master ~]# kubectl get deploy -n dev</span><br><span class="line">NAME    READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">nginx   3/3     3            3           46s</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：查看名为dev的namespace下的名为nginx的deployment的详细信息</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl describe deploy nginx -n dev</span><br><span class="line">Name:                   nginx</span><br><span class="line">Namespace:              dev</span><br><span class="line">CreationTimestamp:      Fri, 20 May 2022 22:59:19 +0000</span><br><span class="line">Labels:                 &lt;none&gt;</span><br><span class="line">Annotations:            deployment.kubernetes.io/revision: 1</span><br><span class="line">Selector:               run=nginx</span><br><span class="line">Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable</span><br><span class="line">StrategyType:           RollingUpdate</span><br><span class="line">MinReadySeconds:        0</span><br><span class="line">RollingUpdateStrategy:  25% max unavailable, 25% max surge</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  run=nginx</span><br><span class="line">  Containers:</span><br><span class="line">   nginx:</span><br><span class="line">    Image:        192.168.20.119/library/nginx:latest</span><br><span class="line">    Port:         80/TCP</span><br><span class="line">    Host Port:    0/TCP</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:       &lt;none&gt;</span><br><span class="line">  Volumes:        &lt;none&gt;</span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status  Reason</span><br><span class="line"></span><br><span class="line">----           ------  ------</span><br><span class="line"></span><br><span class="line">  Available      True    MinimumReplicasAvailable</span><br><span class="line">  Progressing    True    NewReplicaSetAvailable</span><br><span class="line">OldReplicaSets:  &lt;none&gt;</span><br><span class="line">NewReplicaSet:   nginx-85dbb646 (3/3 replicas created)</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age    From                   Message</span><br><span class="line"></span><br><span class="line">----    ------             ----   ----                   -------</span><br><span class="line"></span><br><span class="line">  Normal  ScalingReplicaSet  2m17s  deployment-controller  Scaled up replica set nginx-85dbb646 to 3</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：删除名为dev的namespace下的名为nginx的deployment</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl delete deploy nginx -n dev</span><br><span class="line">deployment.apps &quot;nginx&quot; deleted</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">No resources found in dev namespace.</span><br></pre></td></tr></table></figure>

<h2 id="6-Service"><a href="#6-Service" class="headerlink" title="6 Service"></a>6 Service</h2><h3 id="6-1-概述"><a href="#6-1-概述" class="headerlink" title="6.1 概述"></a>6.1 概述</h3><ul>
<li><p>我们已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务，虽然每个Pod都会分配一个单独的Pod的IP地址，但是却存在如下的问题：</p>
</li>
<li><ul>
<li>Pod的IP会随着Pod的重建产生变化。</li>
</ul>
</li>
<li><ul>
<li>Pod的IP仅仅是集群内部可见的虚拟的IP，外部无法访问。</li>
</ul>
</li>
<li><p>这样对于访问这个服务带来了难度，因此，kubernetes设计了Service来解决这个问题。</p>
</li>
<li><p>Service可以看做是一组同类的Pod对外的访问接口，借助Service，应用可以方便的实现服务发现和负载均衡。</p>
</li>
</ul>
<img src="/2022/05/20/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day3/1609137571725-a4754fdb-d0a1-4a49-a7a4-169fc6cc9703.png" class="" title="Service概述.png">

<h3 id="6-2-语法及应用示例"><a href="#6-2-语法及应用示例" class="headerlink" title="6.2 语法及应用示例"></a>6.2 语法及应用示例</h3><h4 id="6-2-1-创建集群内部可访问的Service"><a href="#6-2-1-创建集群内部可访问的Service" class="headerlink" title="6.2.1 创建集群内部可访问的Service"></a>6.2.1 创建集群内部可访问的Service</h4><ul>
<li>示例：暴露名为test的namespace下的名为nginx的deployment，并设置服务名为svc-nginx1</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev</span><br><span class="line">service/svc-nginx1 exposed</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：查看名为test的命名空间的所有Service</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get svc -n dev -o wide</span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE   SELECTOR</span><br><span class="line">svc-nginx1   ClusterIP   10.103.119.55   &lt;none&gt;        80/TCP    20s   run=nginx</span><br></pre></td></tr></table></figure>

<h4 id="6-2-2-创建集群外部可访问的Service"><a href="#6-2-2-创建集群外部可访问的Service" class="headerlink" title="6.2.2 创建集群外部可访问的Service"></a>6.2.2 创建集群外部可访问的Service</h4><ul>
<li>示例：暴露名为test的namespace下的名为nginx的deployment，并设置服务名为svc-nginx2</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev</span><br><span class="line">service/svc-nginx2 exposed</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：查看名为test的命名空间的所有Service</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get svc -n dev</span><br><span class="line">NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">svc-nginx1   ClusterIP   10.103.119.55    &lt;none&gt;        80/TCP         37m</span><br><span class="line">svc-nginx2   NodePort    10.105.235.109   &lt;none&gt;        80:30763/TCP   12s</span><br></pre></td></tr></table></figure>

<h4 id="6-2-3-删除服务"><a href="#6-2-3-删除服务" class="headerlink" title="6.2.3 删除服务"></a>6.2.3 删除服务</h4><ul>
<li>示例：删除服务</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl delete svc svc-nginx1 -n dev</span><br><span class="line">service &quot;svc-nginx1&quot; deleted</span><br><span class="line">[root@master ~]# kubectl get svc -n dev</span><br><span class="line">NAME         TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">svc-nginx2   NodePort   10.105.235.109   &lt;none&gt;        80:30763/TCP   3m35s</span><br></pre></td></tr></table></figure>

<h4 id="6-2-4-对象配置方式"><a href="#6-2-4-对象配置方式" class="headerlink" title="6.2.4 对象配置方式"></a>6.2.4 对象配置方式</h4><ul>
<li>示例：对象配置方式</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat svc-nginx.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: svc-nginx</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  clusterIP: 10.109.179.231</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    run: nginx</span><br><span class="line">  type: ClusterIP</span><br><span class="line">[root@master ~]# kubectl create -f svc-nginx.yaml </span><br><span class="line">service/svc-nginx created</span><br><span class="line">[root@master ~]# kubectl get svc -n dev</span><br><span class="line">    NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">    svc-nginx    ClusterIP   10.109.179.231   &lt;none&gt;        80/TCP         12s</span><br><span class="line">    svc-nginx2   NodePort    10.105.235.109   &lt;none&gt;        80:30763/TCP   7m20s</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s学习日记day4</title>
    <url>/2022/05/23/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day4/</url>
    <content><![CDATA[<h1 id="kubernetes的Pod详解一"><a href="#kubernetes的Pod详解一" class="headerlink" title="kubernetes的Pod详解一"></a><strong>kubernetes的Pod详解</strong>一<span id="more"></span></h1><h2 id="1-Pod的介绍"><a href="#1-Pod的介绍" class="headerlink" title="1 Pod的介绍"></a>1 Pod的介绍</h2><h3 id="1-1-Pod的结构"><a href="#1-1-Pod的结构" class="headerlink" title="1.1 Pod的结构"></a>1.1 Pod的结构</h3><ul>
<li><p>每个Pod中都包含一个或者多个容器，这些容器可以分为两类：</p>
</li>
<li><p>① 用户程序所在的容器，数量可多可少。</p>
</li>
<li><p>② Pause容器，这是每个Pod都会有的一个根容器，它的作用有两个：</p>
</li>
<li><ul>
<li>可以以它为依据，评估整个Pod的健康状况。</li>
</ul>
</li>
<li><ul>
<li>可以在根容器上设置IP地址，其它容器都共享此IP（Pod的IP），以实现Pod内部的网络通信（这里是Pod内部的通讯，Pod之间的通讯采用虚拟二层网络技术来实现，我们当前环境使用的是Flannel）。</li>
</ul>
</li>
</ul>
<h3 id="1-2-Pod定义"><a href="#1-2-Pod定义" class="headerlink" title="1.2 Pod定义"></a>1.2 Pod定义</h3><ul>
<li>下面是Pod的资源清单：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">apiVersion: v1     #必选，版本号，例如v1</span><br><span class="line">kind: Pod       　 #必选，资源类型，例如 Pod</span><br><span class="line">metadata:       　 #必选，元数据</span><br><span class="line">  name: string     #必选，Pod名称</span><br><span class="line">  namespace: string  #Pod所属的命名空间,默认为&quot;default&quot;</span><br><span class="line">  labels:       　　  #自定义标签列表</span><br><span class="line">    - name: string      　          </span><br><span class="line">spec:  #必选，Pod中容器的详细定义</span><br><span class="line">  containers:  #必选，Pod中容器列表</span><br><span class="line">  - name: string   #必选，容器名称</span><br><span class="line">    image: string  #必选，容器的镜像名称</span><br><span class="line">    imagePullPolicy: [ Always|Never|IfNotPresent ]  #获取镜像的策略 </span><br><span class="line">    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令</span><br><span class="line">    args: [string]      #容器的启动命令参数列表</span><br><span class="line">    workingDir: string  #容器的工作目录</span><br><span class="line">    volumeMounts:       #挂载到容器内部的存储卷配置</span><br><span class="line">    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名</span><br><span class="line">      mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符</span><br><span class="line">      readOnly: boolean #是否为只读模式</span><br><span class="line">      ports: #需要暴露的端口库号列表</span><br><span class="line">    - name: string        #端口的名称</span><br><span class="line">      containerPort: int  #容器需要监听的端口号</span><br><span class="line">      hostPort: int       #容器所在主机需要监听的端口号，默认与Container相同</span><br><span class="line">      protocol: string    #端口协议，支持TCP和UDP，默认TCP</span><br><span class="line">      env:   #容器运行前需设置的环境变量列表</span><br><span class="line">    - name: string  #环境变量名称</span><br><span class="line">      value: string #环境变量的值</span><br><span class="line">      resources: #资源限制和请求的设置</span><br><span class="line">      limits:  #资源限制的设置</span><br><span class="line">        cpu: string     #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数</span><br><span class="line">        memory: string  #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数</span><br><span class="line">      requests: #资源请求的设置</span><br><span class="line">        cpu: string    #Cpu请求，容器启动的初始可用数量</span><br><span class="line">        memory: string #内存请求,容器启动的初始可用数量</span><br><span class="line">      lifecycle: #生命周期钩子</span><br><span class="line">      postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启</span><br><span class="line">      preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止</span><br><span class="line">      livenessProbe:  #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器</span><br><span class="line">      exec:       　 #对Pod容器内检查方式设置为exec方式</span><br><span class="line">        command: [string]  #exec方式需要制定的命令或脚本</span><br><span class="line">      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port</span><br><span class="line">        path: string</span><br><span class="line">        port: number</span><br><span class="line">        host: string</span><br><span class="line">        scheme: string</span><br><span class="line">        HttpHeaders:</span><br><span class="line">        - name: string</span><br><span class="line">          value: string</span><br><span class="line">          tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式</span><br><span class="line">             port: number</span><br><span class="line">           initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒</span><br><span class="line">           timeoutSeconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒</span><br><span class="line">           periodSeconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次</span><br><span class="line">           successThreshold: 0</span><br><span class="line">           failureThreshold: 0</span><br><span class="line">           securityContext:</span><br><span class="line">             privileged: false</span><br><span class="line">          restartPolicy: [Always | Never | OnFailure]  #Pod的重启策略</span><br><span class="line">          nodeName: &lt;string&gt; #设置NodeName表示将该Pod调度到指定到名称的node节点上</span><br><span class="line">          nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上</span><br><span class="line">          imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定</span><br><span class="line">  - name: string</span><br><span class="line">    hostNetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络</span><br><span class="line">      volumes:   #在该pod上定义共享存储卷列表</span><br><span class="line">  - name: string    #共享存储卷名称 （volumes类型有很多种）</span><br><span class="line">    emptyDir: &#123;&#125;       #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值</span><br><span class="line">    hostPath: string   #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录</span><br><span class="line">      path: string      　　        #Pod所在宿主机的目录，将被用于同期中mount的目录</span><br><span class="line">    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部</span><br><span class="line">      scretname: string  </span><br><span class="line">      items:     </span><br><span class="line">      - key: string</span><br><span class="line">        path: string</span><br><span class="line">        configMap:         #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部</span><br><span class="line">          name: string</span><br><span class="line">          items:</span><br><span class="line">      - key: string</span><br><span class="line">        path: string</span><br></pre></td></tr></table></figure>

<ul>
<li>语法：查看每种资源的可配置项</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 查看某种资源可以配置的一级配置</span><br><span class="line"></span><br><span class="line">kubectl explain 资源类型 </span><br><span class="line"></span><br><span class="line"># 查看属性的子属性</span><br><span class="line"></span><br><span class="line">kubectl explain 资源类型.属性</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：查看资源类型为pod的可配置项</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl explain pod</span><br><span class="line">KIND:     Pod</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">     Pod is a collection of containers that can run on a host. This resource is</span><br><span class="line">     created by clients and scheduled onto hosts.</span><br><span class="line"></span><br><span class="line">FIELDS:</span><br><span class="line">   apiVersion   &lt;string&gt;</span><br><span class="line">     APIVersion defines the versioned schema of this representation of an</span><br><span class="line">     object. Servers should convert recognized schemas to the latest internal</span><br><span class="line">     value, and may reject unrecognized values. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</span><br><span class="line"></span><br><span class="line">   kind &lt;string&gt;</span><br><span class="line">     Kind is a string value representing the REST resource this object</span><br><span class="line">     represents. Servers may infer this from the endpoint the client submits</span><br><span class="line">     requests to. Cannot be updated. In CamelCase. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</span><br><span class="line"></span><br><span class="line">   metadata     &lt;Object&gt;</span><br><span class="line">     Standard object&#x27;s metadata. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</span><br><span class="line"></span><br><span class="line">   spec &lt;Object&gt;</span><br><span class="line">     Specification of the desired behavior of the pod. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</span><br><span class="line"></span><br><span class="line">   status       &lt;Object&gt;</span><br><span class="line">     Most recently observed status of the pod. This data may not be up to date.</span><br><span class="line">     Populated by the system. Read-only. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conv</span><br></pre></td></tr></table></figure>

<ul>
<li>示例：查看资源类型为Pod的metadata的属性的可配置项</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl explain pod.metadata</span><br><span class="line">KIND:     Pod</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">RESOURCE: metadata &lt;Object&gt;</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">     Standard object&#x27;s metadata. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</span><br><span class="line"></span><br><span class="line">     ObjectMeta is metadata that all persisted resources must have, which</span><br><span class="line">     includes all objects users must create.</span><br><span class="line"></span><br><span class="line">FIELDS:</span><br><span class="line">   annotations  &lt;map[string]string&gt;</span><br><span class="line">     Annotations is an unstructured key value map stored with a resource that</span><br><span class="line">     may be set by external tools to store and retrieve arbitrary metadata. They</span><br><span class="line">     are not queryable and should be preserved when modifying objects. More</span><br><span class="line">     info: http://kubernetes.io/docs/user-guide/annotations</span><br><span class="line"></span><br><span class="line">   clusterName  &lt;string&gt;</span><br><span class="line">     The name of the cluster which the object belongs to. This is used to</span><br><span class="line">     distinguish resources with same name and namespace in different clusters.</span><br><span class="line">     This field is not set anywhere right now and apiserver is going to ignore</span><br><span class="line">     it if set in create or update request.</span><br><span class="line"></span><br><span class="line">   creationTimestamp    &lt;string&gt;</span><br><span class="line">     CreationTimestamp is a timestamp representing the server time when this</span><br><span class="line">     object was created. It is not guaranteed to be set in happens-before order</span><br><span class="line">     across separate operations. Clients may not set this value. It is</span><br><span class="line">     represented in RFC3339 form and is in UTC. Populated by the system.</span><br><span class="line">     Read-only. Null for lists. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</span><br><span class="line"></span><br><span class="line">   deletionGracePeriodSeconds   &lt;integer&gt;</span><br><span class="line">     Number of seconds allowed for this object to gracefully terminate before it</span><br><span class="line">     will be removed from the system. Only set when deletionTimestamp is also</span><br><span class="line">     set. May only be shortened. Read-only.</span><br><span class="line"></span><br><span class="line">   deletionTimestamp    &lt;string&gt;</span><br><span class="line">     DeletionTimestamp is RFC 3339 date and time at which this resource will be</span><br><span class="line">     deleted. This field is set by the server when a graceful deletion is</span><br><span class="line">     requested by the user, and is not directly settable by a client. The</span><br><span class="line">     resource is expected to be deleted (no longer visible from resource lists,</span><br><span class="line">     and not reachable by name) after the time in this field, once the</span><br><span class="line">     finalizers list is empty. As long as the finalizers list contains items,</span><br><span class="line">     deletion is blocked. Once the deletionTimestamp is set, this value may not</span><br><span class="line">     be unset or be set further into the future, although it may be shortened or</span><br><span class="line">     the resource may be deleted prior to this time. For example, a user may</span><br><span class="line">     request that a pod is deleted in 30 seconds. The Kubelet will react by</span><br><span class="line">     sending a graceful termination signal to the containers in the pod. After</span><br><span class="line">     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)</span><br><span class="line">     to the container and after cleanup, remove the pod from the API. In the</span><br><span class="line">     presence of network partitions, this object may still exist after this</span><br><span class="line">     timestamp, until an administrator or automated process can determine the</span><br><span class="line">     resource is fully terminated. If not set, graceful deletion of the object</span><br><span class="line">     has not been requested. Populated by the system when a graceful deletion is</span><br><span class="line">     requested. Read-only. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</span><br><span class="line"></span><br><span class="line">   finalizers   &lt;[]string&gt;</span><br><span class="line">     Must be empty before the object is deleted from the registry. Each entry is</span><br><span class="line">     an identifier for the responsible component that will remove the entry from</span><br><span class="line">     the list. If the deletionTimestamp of the object is non-nil, entries in</span><br><span class="line">     this list can only be removed. Finalizers may be processed and removed in</span><br><span class="line">     any order. Order is NOT enforced because it introduces significant risk of</span><br><span class="line">     stuck finalizers. finalizers is a shared field, any actor with permission</span><br><span class="line">     can reorder it. If the finalizer list is processed in order, then this can</span><br><span class="line">     lead to a situation in which the component responsible for the first</span><br><span class="line">     finalizer in the list is waiting for a signal (field value, external</span><br><span class="line">     system, or other) produced by a component responsible for a finalizer later</span><br><span class="line">     in the list, resulting in a deadlock. Without enforced ordering finalizers</span><br><span class="line">     are free to order amongst themselves and are not vulnerable to ordering</span><br><span class="line">     changes in the list.</span><br><span class="line"></span><br><span class="line">   generateName &lt;string&gt;</span><br><span class="line">     GenerateName is an optional prefix, used by the server, to generate a</span><br><span class="line">     unique name ONLY IF the Name field has not been provided. If this field is</span><br><span class="line">     used, the name returned to the client will be different than the name</span><br><span class="line">     passed. This value will also be combined with a unique suffix. The provided</span><br><span class="line">     value has the same validation rules as the Name field, and may be truncated</span><br><span class="line">     by the length of the suffix required to make the value unique on the</span><br><span class="line">     server. If this field is specified and the generated name exists, the</span><br><span class="line">     server will NOT return a 409 - instead, it will either return 201 Created</span><br><span class="line">     or 500 with Reason ServerTimeout indicating a unique name could not be</span><br><span class="line">     found in the time allotted, and the client should retry (optionally after</span><br><span class="line">     the time indicated in the Retry-After header). Applied only if Name is not</span><br><span class="line">     specified. More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency</span><br><span class="line"></span><br><span class="line">   generation   &lt;integer&gt;</span><br><span class="line">     A sequence number representing a specific generation of the desired state.</span><br><span class="line">     Populated by the system. Read-only.</span><br><span class="line"></span><br><span class="line">   labels       &lt;map[string]string&gt;</span><br><span class="line">     Map of string keys and values that can be used to organize and categorize</span><br><span class="line">     (scope and select) objects. May match selectors of replication controllers</span><br><span class="line">     and services. More info: http://kubernetes.io/docs/user-guide/labels</span><br><span class="line"></span><br><span class="line">   managedFields        &lt;[]Object&gt;</span><br><span class="line">     ManagedFields maps workflow-id and version to the set of fields that are</span><br><span class="line">     managed by that workflow. This is mostly for internal housekeeping, and</span><br><span class="line">     users typically shouldn&#x27;t need to set or understand this field. A workflow</span><br><span class="line">     can be the user&#x27;s name, a controller&#x27;s name, or the name of a specific</span><br><span class="line">     apply path like &quot;ci-cd&quot;. The set of fields is always in the version that</span><br><span class="line">     the workflow used when modifying the object.</span><br><span class="line"></span><br><span class="line">   name &lt;string&gt;</span><br><span class="line">     Name must be unique within a namespace. Is required when creating</span><br><span class="line">     resources, although some resources may allow a client to request the</span><br><span class="line">     generation of an appropriate name automatically. Name is primarily intended</span><br><span class="line">     for creation idempotence and configuration definition. Cannot be updated.</span><br><span class="line">     More info: http://kubernetes.io/docs/user-guide/identifiers#names</span><br><span class="line"></span><br><span class="line">   namespace    &lt;string&gt;</span><br><span class="line">     Namespace defines the space within each name must be unique. An empty</span><br><span class="line">     namespace is equivalent to the &quot;default&quot; namespace, but &quot;default&quot; is the</span><br><span class="line">     canonical representation. Not all objects are required to be scoped to a</span><br><span class="line">     namespace - the value of this field for those objects will be empty. Must</span><br><span class="line">     be a DNS_LABEL. Cannot be updated. More info:</span><br><span class="line">     http://kubernetes.io/docs/user-guide/namespaces</span><br><span class="line"></span><br><span class="line">   ownerReferences      &lt;[]Object&gt;</span><br><span class="line">     List of objects depended by this object. If ALL objects in the list have</span><br><span class="line">     been deleted, this object will be garbage collected. If this object is</span><br><span class="line">     managed by a controller, then an entry in this list will point to this</span><br><span class="line">     controller, with the controller field set to true. There cannot be more</span><br><span class="line">     than one managing controller.</span><br><span class="line"></span><br><span class="line">   resourceVersion      &lt;string&gt;</span><br><span class="line">     An opaque value that represents the internal version of this object that</span><br><span class="line">     can be used by clients to determine when objects have changed. May be used</span><br><span class="line">     for optimistic concurrency, change detection, and the watch operation on a</span><br><span class="line">     resource or set of resources. Clients must treat these values as opaque and</span><br><span class="line">     passed unmodified back to the server. They may only be valid for a</span><br><span class="line">     particular resource or set of resources. Populated by the system.</span><br><span class="line">     Read-only. Value must be treated as opaque by clients and . More info:</span><br><span class="line">     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency</span><br><span class="line"></span><br><span class="line">   selfLink     &lt;string&gt;</span><br><span class="line">     SelfLink is a URL representing this object. Populated by the system.</span><br><span class="line">     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20</span><br><span class="line">     release and the field is planned to be removed in 1.21 release.</span><br><span class="line"></span><br><span class="line">   uid  &lt;string&gt;</span><br><span class="line">     UID is the unique in time and space value for this object. It is typically</span><br><span class="line">     generated by the server on successful creation of a resource and is not</span><br><span class="line">     allowed to change on PUT operations. Populated by the system. Read-only.</span><br><span class="line">     More info: http://kubernetes.io/docs/user-guide/identifiers#uids</span><br></pre></td></tr></table></figure>



<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在kubernetes中基本所有资源的一级属性都是一样的，主要包含5个部分：</span><br><span class="line"></span><br><span class="line">apiVersion  &lt;string&gt;：版本，有kubernetes内部定义，版本号必须用kubectl api-versions查询。</span><br><span class="line"></span><br><span class="line">kind &lt;string&gt;：类型，有kubernetes内部定义，类型必须用kubectl api-resources查询。</span><br><span class="line"></span><br><span class="line">metadata  &lt;Object&gt;：元数据，主要是资源标识和说明，常用的有name、namespace、labels等。</span><br><span class="line"></span><br><span class="line">spec &lt;Object&gt;：描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述。</span><br><span class="line"></span><br><span class="line">status  &lt;Object&gt;：状态信息，里面的内容不需要定义，由kubernetes自动生成。</span><br><span class="line"></span><br><span class="line">**在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性：**</span><br><span class="line"></span><br><span class="line">containers  &lt;[]Object&gt;：容器列表，用于定义容器的详细信息。</span><br><span class="line"></span><br><span class="line">nodeName &lt;String&gt;：根据nodeName的值将Pod调度到指定的Node节点上。</span><br><span class="line"></span><br><span class="line">nodeSelector  &lt;map[]&gt; ：根据NodeSelector中定义的信息选择该Pod调度到包含这些Label的Node上。</span><br><span class="line"></span><br><span class="line">hostNetwork  &lt;boolean&gt;：是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络。</span><br><span class="line"></span><br><span class="line">volumes    &lt;[]Object&gt; ：存储卷，用于定义Pod上面挂载的存储信息。</span><br><span class="line"></span><br><span class="line">restartPolicy	&lt;string&gt;：重启策略，表示Pod在遇到故障的时候的处理策略。</span><br></pre></td></tr></table></figure>



<h2 id="2-Pod的配置"><a href="#2-Pod的配置" class="headerlink" title="2 Pod的配置"></a>2 Pod的配置</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 概述"></a>2.1 概述</h3><ul>
<li><p>本小节主要来研究pod.spec.containers属性，这也是Pod配置中最为关键的一项配置。</p>
</li>
<li><p>示例：查看pod.spec.containers的可选配置项</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl explain pod.spec.containers</span><br><span class="line"># 返回的重要属性</span><br><span class="line">KIND:     Pod</span><br><span class="line">VERSION:  v1</span><br><span class="line">RESOURCE: containers &lt;[]Object&gt;   # 数组，代表可以有多个容器FIELDS:</span><br><span class="line">  name  &lt;string&gt;     # 容器名称</span><br><span class="line">  image &lt;string&gt;     # 容器需要的镜像地址</span><br><span class="line">  imagePullPolicy  &lt;string&gt; # 镜像拉取策略 </span><br><span class="line">  command  &lt;[]string&gt; # 容器的启动命令列表，如不指定，使用打包时使用的启动命令</span><br><span class="line">  args   &lt;[]string&gt; # 容器的启动命令需要的参数列表 </span><br><span class="line">  env    &lt;[]Object&gt; # 容器环境变量的配置</span><br><span class="line">  ports  &lt;[]Object&gt;  # 容器需要暴露的端口号列表</span><br><span class="line">  resources &lt;Object&gt; # 资源限制和资源请求的设置</span><br></pre></td></tr></table></figure>

<h3 id="2-2-基本配置"><a href="#2-2-基本配置" class="headerlink" title="2.2 基本配置"></a>2.2 基本配置</h3><ul>
<li>创建pod-base.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-base.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-base</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">    - name: busybox # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/busybox:latest # 容器需要的镜像地址</span><br><span class="line">[root@master ~]# kubectl apply -f pod-base.yaml </span><br><span class="line">pod/pod-base created</span><br></pre></td></tr></table></figure>

<ul>
<li><p>上面定义了一个比较简单的Pod的配置，里面有两个容器：</p>
</li>
<li><ul>
<li>nginx：用的是1.17.1版本的nginx镜像创建（nginx是一个轻量级的web容器）。</li>
</ul>
</li>
<li><ul>
<li>busybox：用的是1.30版本的busybox镜像创建（busybox是一个小巧的linux命令集合）。</li>
</ul>
</li>
<li><p>查看Pod状况：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME       READY   STATUS             RESTARTS   AGE</span><br><span class="line">pod-base   1/2     CrashLoopBackOff   3          62s</span><br></pre></td></tr></table></figure>

<p><strong>表示当前pod里有2个容器其中一个容器故障了 并且pod一直在重启试图恢复它</strong></p>
<ul>
<li>通过describe查看内部的详情：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl describe pod pod-base -n dev</span><br><span class="line">Name:         pod-base</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node1/192.168.20.115</span><br><span class="line">Start Time:   Mon, 23 May 2022 20:18:00 +0000</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  Status:  Running</span><br><span class="line">IP:           10.244.1.27</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.1.27</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://69a12ef22071c018ad434d1a0ce507a9c6a29e036f4d3aa1051fceef35945235</span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83</span><br><span class="line">    Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Mon, 23 May 2022 20:18:01 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gr9wn (ro)</span><br><span class="line">  busybox:</span><br><span class="line">    Container ID:   docker://d5844970e09d9eb8d924562468cec75d4cf69090ed9db0d769759ac858e31497</span><br><span class="line">    Image:          192.168.20.119/library/busybox:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/busybox@sha256:2ca5e69e244d2da7368f7088ea3ad0653c3ce7aaccd0b8823d11b0d5de956002</span><br><span class="line">    Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       CrashLoopBackOff</span><br><span class="line">    Last State:     Terminated</span><br><span class="line">      Reason:       Completed</span><br><span class="line">      Exit Code:    0</span><br><span class="line">      Started:      Mon, 23 May 2022 20:19:37 +0000</span><br><span class="line">      Finished:     Mon, 23 May 2022 20:19:37 +0000</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  4</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gr9wn (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             False </span><br><span class="line">  ContainersReady   False </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-gr9wn:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-gr9wn</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                   From               Message</span><br><span class="line"></span><br><span class="line">----     ------     ----                  ----               -------</span><br><span class="line"></span><br><span class="line">  Normal   Scheduled  &lt;unknown&gt;             default-scheduler  Successfully assigned dev/pod-base to node1</span><br><span class="line">  Normal   Pulling    2m53s                 kubelet, node1     Pulling image &quot;192.168.20.119/library/nginx:latest&quot;</span><br><span class="line">  Normal   Pulled     2m53s                 kubelet, node1     Successfully pulled image &quot;192.168.20.119/library/nginx:latest&quot;</span><br><span class="line">  Normal   Created    2m53s                 kubelet, node1     Created container nginx</span><br><span class="line">  Normal   Started    2m52s                 kubelet, node1     Started container nginx</span><br><span class="line">  Normal   Pulling    2m7s (x4 over 2m52s)  kubelet, node1     Pulling image &quot;192.168.20.119/library/busybox:latest&quot;</span><br><span class="line">  Normal   Pulled     2m6s (x4 over 2m52s)  kubelet, node1     Successfully pulled image &quot;192.168.20.119/library/busybox:latest&quot;</span><br><span class="line">  Normal   Created    2m6s (x4 over 2m52s)  kubelet, node1     Created container busybox</span><br><span class="line">  Normal   Started    2m6s (x4 over 2m52s)  kubelet, node1     Started container busybox</span><br><span class="line">  Warning  BackOff    2m5s (x5 over 2m50s)  kubelet, node1     Back-off restarting failed container</span><br></pre></td></tr></table></figure>

<p><strong>此时已经运行起来了一个基本的Pod，虽然它暂时有问题</strong></p>
<h3 id="2-3-镜像拉取策略"><a href="#2-3-镜像拉取策略" class="headerlink" title="2.3 镜像拉取策略"></a>2.3 镜像拉取策略</h3><ul>
<li>创建pod-imagepullpolicy.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-imagepullpolicy.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-imagepullpolicy</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">      imagePullPolicy: Always # 用于设置镜像的拉取策略</span><br><span class="line">    - name: busybox # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/busybox:latest # 容器需要的镜像地址</span><br><span class="line">[root@master ~]# kubectl apply -f pod-imagepullpolicy.yaml </span><br><span class="line">pod/pod-imagepullpolicy created</span><br></pre></td></tr></table></figure>

<ul>
<li><p>imagePullPolicy：用于设置镜像拉取的策略，kubernetes支持配置三种拉取策略：</p>
</li>
<li><ul>
<li>Always：总是从远程仓库拉取镜像（一直远程下载）。</li>
</ul>
</li>
<li><ul>
<li>IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就用本地，本地没有就使用远程下载）。</li>
</ul>
</li>
<li><ul>
<li>Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错（一直使用本地，没有就报错）。</li>
</ul>
</li>
</ul>
<blockquote>
<p>默认值说明：</p>
<ul>
<li><p>如果镜像tag为具体的版本号，默认策略是IfNotPresent。</p>
</li>
<li><p>如果镜像tag为latest（最终版本），默认策略是Always。</p>
</li>
</ul>
</blockquote>
<ul>
<li>查看Pod详情：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME                  READY   STATUS             RESTARTS   AGE</span><br><span class="line">pod-base              1/2     CrashLoopBackOff   6          9m50s</span><br><span class="line">pod-imagepullpolicy   1/2     CrashLoopBackOff   3          79s</span><br><span class="line">[root@master ~]# kubectl describe pod pod-imagepullpolicy -n dev</span><br><span class="line">Name:         pod-imagepullpolicy</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node2/192.168.20.124</span><br><span class="line">Start Time:   Mon, 23 May 2022 20:26:32 +0000</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  Status:  Running</span><br><span class="line">IP:           10.244.2.39</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.2.39</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://9ec1e08eb621e7bd27b58ef48837ea81df6a5841530f410196638ae0ad19c685</span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83</span><br><span class="line">    Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Mon, 23 May 2022 20:26:34 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gr9wn (ro)</span><br><span class="line">  busybox:</span><br><span class="line">    Container ID:   docker://1df3c0eec902d8d7e0965b4025665c1f34d1d4652929782670108ee75e42197b</span><br><span class="line">    Image:          192.168.20.119/library/busybox:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/busybox@sha256:2ca5e69e244d2da7368f7088ea3ad0653c3ce7aaccd0b8823d11b0d5de956002</span><br><span class="line">    Port:           &lt;none&gt;</span><br><span class="line">    Host Port:      &lt;none&gt;</span><br><span class="line">    State:          Terminated</span><br><span class="line">      Reason:       Completed</span><br><span class="line">      Exit Code:    0</span><br><span class="line">      Started:      Mon, 23 May 2022 20:28:01 +0000</span><br><span class="line">      Finished:     Mon, 23 May 2022 20:28:01 +0000</span><br><span class="line">    Last State:     Terminated</span><br><span class="line">      Reason:       Completed</span><br><span class="line">      Exit Code:    0</span><br><span class="line">      Started:      Mon, 23 May 2022 20:27:17 +0000</span><br><span class="line">      Finished:     Mon, 23 May 2022 20:27:17 +0000</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  4</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gr9wn (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             False </span><br><span class="line">  ContainersReady   False </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-gr9wn:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-gr9wn</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                From               Message</span><br><span class="line"></span><br><span class="line">----     ------     ----               ----               -------</span><br><span class="line"></span><br><span class="line">  Normal   Scheduled  &lt;unknown&gt;          default-scheduler  Successfully assigned dev/pod-imagepullpolicy to node2</span><br><span class="line">  Normal   Pulling    93s                kubelet, node2     Pulling image &quot;192.168.20.119/library/nginx:latest&quot;</span><br><span class="line">  Normal   Pulled     93s                kubelet, node2     Successfully pulled image &quot;192.168.20.119/library/nginx:latest&quot;</span><br><span class="line">  Normal   Created    92s                kubelet, node2     Created container nginx</span><br><span class="line">  Normal   Started    92s                kubelet, node2     Started container nginx</span><br><span class="line">  Normal   Pulling    50s (x4 over 92s)  kubelet, node2     Pulling image &quot;192.168.20.119/library/busybox:latest&quot;</span><br><span class="line">  Normal   Pulled     49s (x4 over 91s)  kubelet, node2     Successfully pulled image &quot;192.168.20.119/library/busybox:latest&quot;</span><br><span class="line">  Normal   Created    49s (x4 over 91s)  kubelet, node2     Created container busybox</span><br><span class="line">  Normal   Started    49s (x4 over 91s)  kubelet, node2     Started container busybox</span><br><span class="line">  Warning  BackOff    48s (x5 over 89s)  kubelet, node2     Back-off restarting failed container</span><br></pre></td></tr></table></figure>

<h3 id="2-4-启动命令"><a href="#2-4-启动命令" class="headerlink" title="2.4 启动命令"></a>2.4 启动命令</h3><ul>
<li><p>在前面的案例中，一直有一个问题没有解决，就是busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障的呢？</p>
</li>
<li><p>原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command的配置。</p>
</li>
<li><p>创建pod-command.yaml文件，内容如下：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-command.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-command</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">      imagePullPolicy: IfNotPresent # 设置镜像拉取策略</span><br><span class="line">    - name: busybox # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/busybox:latest # 容器需要的镜像地址</span><br><span class="line">      command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt;sleep 3;done;&quot;]</span><br><span class="line">[root@master ~]# kubectl apply -f pod-command.yaml </span><br><span class="line">pod/pod-command unchanged</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME                  READY   STATUS             RESTARTS   AGE</span><br><span class="line">pod-base              1/2     CrashLoopBackOff   7          16m</span><br><span class="line">pod-command           2/2     Running            0          91s</span><br><span class="line">pod-imagepullpolicy   1/2     CrashLoopBackOff   6          7m34s</span><br></pre></td></tr></table></figure>

<blockquote>
<p>command：用于在Pod中的容器初始化完毕之后执行一个命令。</p>
<p>这里稍微解释下command中的命令的意思：</p>
<ul>
<li><p>“&#x2F;bin&#x2F;sh”,”-c”：使用sh执行命令。</p>
</li>
<li><p>touch &#x2F;tmp&#x2F;hello.txt：创建一个&#x2F;tmp&#x2F;hello.txt的文件。</p>
</li>
<li><p>while true;do &#x2F;bin&#x2F;echo $(date +%T) &gt;&gt; &#x2F;tmp&#x2F;hello.txt;sleep 3;done：每隔3秒，向文件写入当前时间</p>
</li>
</ul>
</blockquote>
<ul>
<li>进入Pod中的busybox容器，查看文件内容：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl exec -it pod-command -n dev -c busybox /bin/sh</span><br><span class="line">kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.</span><br><span class="line">/ # tail -f /tmp/hello.txt </span><br><span class="line">20:36:09</span><br><span class="line">20:36:12</span><br><span class="line">20:36:15</span><br><span class="line">20:36:18</span><br><span class="line">20:36:21</span><br><span class="line">20:36:24</span><br><span class="line">20:36:27</span><br><span class="line">20:36:30</span><br><span class="line">20:36:33</span><br><span class="line">20:36:36</span><br><span class="line">20:36:39</span><br><span class="line">20:36:42</span><br><span class="line">20:36:45</span><br><span class="line">20:36:48</span><br><span class="line">20:36:51</span><br><span class="line">20:36:54</span><br><span class="line">20:36:57</span><br><span class="line">20:37:00</span><br></pre></td></tr></table></figure>

<blockquote>
<p>特别说明：通过上面发现command已经可以完成启动命令和传递参数的功能，为什么还要提供一个args选项，用于传递参数？其实和Docker有点关系，kubernetes中的command和args两个参数其实是为了实现覆盖Dockerfile中的ENTRYPOINT的功能：</p>
<ul>
<li><p>如果command和args均没有写，那么用Dockerfile的配置。</p>
</li>
<li><p>如果command写了，但是args没有写，那么Dockerfile默认的配置会被忽略，执行注入的command。</p>
</li>
<li><p>如果command没有写，但是args写了，那么Dockerfile中配置的ENTRYPOINT命令会被执行，使用当前args的参数。</p>
</li>
<li><p>如果command和args都写了，那么Dockerfile中的配置会被忽略，执行command并追加上args参数。</p>
</li>
</ul>
</blockquote>
<h3 id="2-5-环境变量（不推荐）"><a href="#2-5-环境变量（不推荐）" class="headerlink" title="2.5 环境变量（不推荐）"></a>2.5 环境变量（不推荐）</h3><ul>
<li>创建pod-evn.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-env.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-env</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">      imagePullPolicy: IfNotPresent # 设置镜像拉取策略</span><br><span class="line">    - name: busybox # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/busybox:latest # 容器需要的镜像地址</span><br><span class="line">      command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt;sleep 3;done;&quot;]</span><br><span class="line">      env:</span><br><span class="line">        - name: &quot;username&quot;</span><br><span class="line">          value: &quot;admin&quot;</span><br><span class="line">        - name: &quot;password&quot;</span><br><span class="line">          value: &quot;123456&quot;</span><br><span class="line">[root@master ~]# kubectl apply -f pod-env.yaml </span><br><span class="line">pod/pod-env created</span><br></pre></td></tr></table></figure>

<ul>
<li>进入容器，输出环境变量：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-env.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-env</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">      imagePullPolicy: IfNotPresent # 设置镜像拉取策略</span><br><span class="line">    - name: busybox # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/busybox:latest # 容器需要的镜像地址</span><br><span class="line">      command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt;sleep 3;done;&quot;]</span><br><span class="line">      env:</span><br><span class="line">        - name: &quot;username&quot;</span><br><span class="line">          value: &quot;admin&quot;</span><br><span class="line">        - name: &quot;password&quot;</span><br><span class="line">          value: &quot;123456&quot;</span><br><span class="line">[root@master ~]# kubectl apply -f pod-env.yaml </span><br><span class="line">pod/pod-env created</span><br><span class="line">[root@master ~]# kubectl exec -it pod-env -n dev -c busybox /bin/sh</span><br><span class="line">kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.</span><br><span class="line">/ # echo $username</span><br><span class="line">admin</span><br><span class="line">/ # echo $password</span><br><span class="line">123456</span><br></pre></td></tr></table></figure>

<h3 id="2-6-端口设置"><a href="#2-6-端口设置" class="headerlink" title="2.6 端口设置"></a>2.6 端口设置</h3><ul>
<li>查看ports支持的子选项：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl explain pod.spec.containers.ports</span><br><span class="line">KIND:     Pod</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">RESOURCE: ports &lt;[]Object&gt;</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">     List of ports to expose from the container. Exposing a port here gives the</span><br><span class="line">     system additional information about the network connections a container</span><br><span class="line">     uses, but is primarily informational. Not specifying a port here DOES NOT</span><br><span class="line">     prevent that port from being exposed. Any port which is listening on the</span><br><span class="line">     default &quot;0.0.0.0&quot; address inside a container will be accessible from the</span><br><span class="line">     network. Cannot be updated.</span><br><span class="line"></span><br><span class="line">     ContainerPort represents a network port in a single container.</span><br><span class="line"></span><br><span class="line">FIELDS:</span><br><span class="line">   containerPort        &lt;integer&gt; -required-</span><br><span class="line">     Number of port to expose on the pod&#x27;s IP address. This must be a valid port</span><br><span class="line">     number, 0 &lt; x &lt; 65536.</span><br><span class="line"></span><br><span class="line">   hostIP       &lt;string&gt;</span><br><span class="line">     What host IP to bind the external port to.</span><br><span class="line"></span><br><span class="line">   hostPort     &lt;integer&gt;</span><br><span class="line">     Number of port to expose on the host. If specified, this must be a valid</span><br><span class="line">     port number, 0 &lt; x &lt; 65536. If HostNetwork is specified, this must match</span><br><span class="line">     ContainerPort. Most containers do not need this.</span><br><span class="line"></span><br><span class="line">   name &lt;string&gt;</span><br><span class="line">     If specified, this must be an IANA_SVC_NAME and unique within the pod. Each</span><br><span class="line">     named port in a pod must have a unique name. Name for the port that can be</span><br><span class="line">     referred to by services.</span><br><span class="line"></span><br><span class="line">   protocol     &lt;string&gt;</span><br><span class="line">     Protocol for port. Must be UDP, TCP, or SCTP. Defaults to &quot;TCP&quot;.</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">KIND:     Pod</span><br><span class="line">VERSION:  v1</span><br><span class="line">RESOURCE: ports &lt;[]Object&gt;</span><br><span class="line">FIELDS:</span><br><span class="line">  name &lt;string&gt; # 端口名称，如果指定，必须保证name在pod中是唯一的</span><br><span class="line">  containerPort &lt;integer&gt; # 容器要监听的端口(0&lt;x&lt;65536)</span><br><span class="line">  hostPort &lt;integer&gt; # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略）</span><br><span class="line">  hostIP &lt;string&gt;  # 要将外部端口绑定到的主机IP(一般省略)</span><br><span class="line">  protocol &lt;string&gt;  # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”</span><br></pre></td></tr></table></figure>

<ul>
<li>创建pod-ports.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-ports.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-ports</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">      imagePullPolicy: IfNotPresent # 设置镜像拉取策略</span><br><span class="line">      ports:</span><br><span class="line">        - name: nginx-port # 端口名称，如果执行，必须保证name在Pod中是唯一的</span><br><span class="line">          containerPort: 80 # 容器要监听的端口 （0~65536）</span><br><span class="line">          protocol: TCP # 端口协议</span><br><span class="line">[root@master ~]# kubectl apply -f pod-ports.yaml </span><br><span class="line">pod/pod-ports created</span><br><span class="line">[root@master ~]# kubectl get pod -n dev -o wide</span><br><span class="line">NAME                  READY   STATUS             RESTARTS   AGE     IP            NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-base              1/2     CrashLoopBackOff   11         36m     10.244.1.27   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-command           2/2     Running            0          21m     10.244.2.40   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-env               2/2     Running            0          7m33s   10.244.1.28   node1   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-imagepullpolicy   1/2     CrashLoopBackOff   10         27m     10.244.2.39   node2   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-ports             1/1     Running            0          40s     10.244.2.41   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>访问Pod中的容器中的程序使用的是PodIp:containerPort。</p>
</blockquote>
<h3 id="2-7-资源配额"><a href="#2-7-资源配额" class="headerlink" title="2.7 资源配额"></a>2.7 资源配额</h3><ul>
<li><p>容器中的程序要运行，肯定会占用一定的资源，比如CPU和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量的资源，导致其他的容器无法运行。针对这种情况，kubernetes提供了对内存和CPU的资源进行配额的机制，这种机制主要通过resources选项实现，它有两个子选项：</p>
</li>
<li><ul>
<li>limits：用于限制运行的容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启。</li>
</ul>
</li>
<li><ul>
<li>requests：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动。</li>
</ul>
</li>
<li><p>可以通过上面的两个选项设置资源的上下限。</p>
</li>
<li><p>创建pod-resoures.yaml文件，内容如下：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-resources.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-resoures</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">      imagePullPolicy: IfNotPresent # 设置镜像拉取策略</span><br><span class="line">      ports: # 端口设置</span><br><span class="line">        - name: nginx-port # 端口名称，如果执行，必须保证name在Pod中是唯一的</span><br><span class="line">          containerPort: 80 # 容器要监听的端口 （0~65536）</span><br><span class="line">          protocol: TCP # 端口协议</span><br><span class="line">      resources: # 资源配额</span><br><span class="line">        limits: # 限制资源的上限</span><br><span class="line">          cpu: &quot;2&quot; # CPU限制，单位是core数</span><br><span class="line">          memory: &quot;10Gi&quot; # 内存限制</span><br><span class="line">        requests: # 限制资源的下限</span><br><span class="line">          cpu: &quot;1&quot; # CPU限制，单位是core数 </span><br><span class="line">          memory: &quot;10Mi&quot; # 内存限制</span><br><span class="line">[root@master ~]# kubectl apply -f pod-resources.yaml </span><br><span class="line">pod/pod-resoures created</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME                  READY   STATUS             RESTARTS   AGE</span><br><span class="line">pod-base              1/2     NotReady           13         41m</span><br><span class="line">pod-command           2/2     Running            0          27m</span><br><span class="line">pod-env               2/2     Running            0          13m</span><br><span class="line">pod-imagepullpolicy   1/2     CrashLoopBackOff   11         33m</span><br><span class="line">pod-ports             1/1     Running            0          6m23s</span><br><span class="line">pod-resoures          1/1     Running            0          92s</span><br></pre></td></tr></table></figure>

<blockquote>
<p>cpu：core数，可以为整数或小数。</p>
<p>memory：内存大小，可以使用Gi、Mi、G、M等形式。</p>
</blockquote>
<p><strong>此时pod运行正常</strong></p>
<ul>
<li>编辑Pod，修改resources.requests.memory的值为10Gi：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-resources.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-resoures</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx # 容器名称</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest # 容器需要的镜像地址</span><br><span class="line">      imagePullPolicy: IfNotPresent # 设置镜像拉取策略</span><br><span class="line">      ports: # 端口设置</span><br><span class="line">        - name: nginx-port # 端口名称，如果执行，必须保证name在Pod中是唯一的</span><br><span class="line">          containerPort: 80 # 容器要监听的端口 （0~65536）</span><br><span class="line">          protocol: TCP # 端口协议</span><br><span class="line">      resources: # 资源配额</span><br><span class="line">        limits: # 限制资源的上限</span><br><span class="line">          cpu: &quot;2&quot; # CPU限制，单位是core数</span><br><span class="line">          memory: &quot;10Gi&quot; # 内存限制</span><br><span class="line">        requests: # 限制资源的下限</span><br><span class="line">          cpu: &quot;1&quot; # CPU限制，单位是core数 </span><br><span class="line">          memory: &quot;10Gi&quot; # 内存限制</span><br><span class="line">[root@master ~]# kubectl delete pods pod-resoures -n dev</span><br><span class="line">pod &quot;pod-resoures&quot; deleted</span><br><span class="line">[root@master ~]# kubectl create -f pod-resources.yaml </span><br><span class="line">pod/pod-resoures created</span><br><span class="line">[root@master ~]# kubectl get pods -n dev</span><br><span class="line">NAME                  READY   STATUS             RESTARTS   AGE</span><br><span class="line">pod-base              1/2     CrashLoopBackOff   14         47m</span><br><span class="line">pod-command           2/2     Running            0          32m</span><br><span class="line">pod-env               2/2     Running            0          18m</span><br><span class="line">pod-imagepullpolicy   1/2     CrashLoopBackOff   12         38m</span><br><span class="line">pod-ports             1/1     Running            0          11m</span><br><span class="line">pod-resoures          0/1     Pending            0          7s</span><br><span class="line">[root@master ~]# kubectl describe pods pod-resoures -n dev</span><br><span class="line">Name:         pod-resoures</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         &lt;none&gt;</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Pending</span><br><span class="line">IP:           </span><br><span class="line">IPs:          &lt;none&gt;</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Image:      192.168.20.119/library/nginx:latest</span><br><span class="line">    Port:       80/TCP</span><br><span class="line">    Host Port:  0/TCP</span><br><span class="line">    Limits:</span><br><span class="line">      cpu:     2</span><br><span class="line">      memory:  10Gi</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        1</span><br><span class="line">      memory:     10Gi</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gr9wn (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status</span><br><span class="line">  PodScheduled   False </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-gr9wn:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-gr9wn</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       Burstable</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age        From               Message</span><br><span class="line"></span><br><span class="line">----     ------            ----       ----               -------</span><br><span class="line"></span><br><span class="line">  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#x27;t tolerate, 2 Insufficient memory.</span><br><span class="line">  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#x27;t tolerate, 2 Insufficient memory.</span><br></pre></td></tr></table></figure>

<blockquote>
<p>提示 Insufficient memory 内存不足</p>
</blockquote>
]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>k8s学习日记day5</title>
    <url>/2022/05/25/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day5/</url>
    <content><![CDATA[<h1 id="kubernetes的Pod详解二"><a href="#kubernetes的Pod详解二" class="headerlink" title="kubernetes的Pod详解二"></a>kubernetes的Pod详解二<span id="more"></span></h1><h2 id="3-Pod的生命周期"><a href="#3-Pod的生命周期" class="headerlink" title="3 Pod的生命周期"></a>3 Pod的生命周期</h2><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><ul>
<li><p>我们一般将Pod对象从创建到终止的这段时间范围称为Pod的生命周期，它主要包含下面的过程：</p>
</li>
<li><ul>
<li>Pod创建过程。</li>
</ul>
</li>
<li><ul>
<li>运行初始化容器（init container）过程。</li>
</ul>
</li>
<li><ul>
<li>运行主容器（main container）：</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>容器启动后钩子（post start）、容器终止前钩子（pre stop）。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li><ul>
<li>容器的存活性探测（liveness probe）、就绪性探测（readiness probe）。</li>
</ul>
</li>
</ul>
</li>
<li><ul>
<li>Pod终止过程。</li>
</ul>
</li>
</ul>
<img src="/2022/05/25/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day5/1609399647590-472c8628-8b69-42ab-8a50-929c27737926.png" class="" title="img">



<ul>
<li><p>在整个生命周期中，Pod会出现5种状态（相位），分别如下：</p>
</li>
<li><ul>
<li>挂起（Pending）：API Server已经创建了Pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中。</li>
</ul>
</li>
<li><ul>
<li>运行中（Running）：Pod已经被调度到某节点，并且所有容器都已经被kubelet创建完成。</li>
</ul>
</li>
<li><ul>
<li>成功（Succeeded）：Pod中的所有容器都已经成功终止并且不会被重启。</li>
</ul>
</li>
<li><ul>
<li>失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态。</li>
</ul>
</li>
<li><ul>
<li>未知（Unknown）：API Server无法正常获取到Pod对象的状态信息，通常由于网络通信失败所导致。</li>
</ul>
</li>
</ul>
<h3 id="3-2-创建和终止"><a href="#3-2-创建和终止" class="headerlink" title="3.2 创建和终止"></a>3.2 创建和终止</h3><h4 id="3-2-1-Pod的创建过程"><a href="#3-2-1-Pod的创建过程" class="headerlink" title="3.2.1 Pod的创建过程"></a>3.2.1 Pod的创建过程</h4><img src="/2022/05/25/k8s%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0day5/1609399660203-ab0d9834-3b35-4119-b304-4394b00f0b9d.jpeg" class="" title="img">



<ul>
<li><p>① 用户通过kubectl或其他的api客户端提交需要创建的Pod信息给API Server。</p>
</li>
<li><p>② API Server开始生成Pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端。</p>
</li>
<li><p>③ API Server开始反映etcd中的Pod对象的变化，其它组件使用watch机制来跟踪检查API Server上的变动。</p>
</li>
<li><p>④ Scheduler发现有新的Pod对象要创建，开始为Pod分配主机并将结果信息更新至API Server。</p>
</li>
<li><p>⑤ Node节点上的kubelet发现有Pod调度过来，尝试调度Docker启动容器，并将结果回送至API Server。</p>
</li>
<li><p>⑥ API Server将接收到的Pod状态信息存入到etcd中。</p>
</li>
</ul>
<h4 id="3-2-2-Pod的终止过程"><a href="#3-2-2-Pod的终止过程" class="headerlink" title="3.2.2 Pod的终止过程"></a>3.2.2 Pod的终止过程</h4><ul>
<li><p>① 用户向API Server发送删除Pod对象的命令。</p>
</li>
<li><p>② API Server中的Pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），Pod被视为dead。</p>
</li>
<li><p>③ 将Pod标记为terminating状态。</p>
</li>
<li><p>④ kubelete在监控到Pod对象转为terminating状态的同时启动Pod关闭过程。</p>
</li>
<li><p>⑤ 端点控制器监控到Pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除。</p>
</li>
<li><p>⑥ 如果当前Pod对象定义了preStop钩子处理器，则在其标记为terminating后会以同步的方式启动执行。</p>
</li>
<li><p>⑦ Pod对象中的容器进程收到停止信号。</p>
</li>
<li><p>⑧ 宽限期结束后，如果Pod中还存在运行的进程，那么Pod对象会收到立即终止的信号。</p>
</li>
<li><p>⑨ kubectl请求API Server将此Pod资源的宽限期设置为0从而完成删除操作，此时Pod对于用户已经不可用了。</p>
</li>
</ul>
<h3 id="3-3-初始化容器"><a href="#3-3-初始化容器" class="headerlink" title="3.3 初始化容器"></a>3.3 初始化容器</h3><ul>
<li><p>初始化容器是在Pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：</p>
</li>
<li><ul>
<li>① 初始化容器必须运行完成直至结束，如果某个初始化容器运行失败，那么kubernetes需要重启它直至成功完成。</li>
</ul>
</li>
<li><ul>
<li>② 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行。</li>
</ul>
</li>
<li><p>初始化容器有很多的应用场景，下面列出的是最常见的几个：</p>
</li>
<li><ul>
<li>提供主容器镜像中不具备的工具程序或自定义代码。</li>
</ul>
</li>
<li><ul>
<li>初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足。</li>
</ul>
</li>
<li><p>接下来做一个案例，模拟下面这个需求：</p>
</li>
<li><ul>
<li>假设要以主容器来运行Nginx，但是要求在运行Nginx之前要能够连接上MySQL和Redis所在的服务器。</li>
</ul>
</li>
<li><ul>
<li>为了简化测试，事先规定好MySQL和Redis所在的IP地址分别为192.168.20.201和192.168.18.202（注意，这两个IP都不能ping通，因为环境中没有这两个IP）。</li>
</ul>
</li>
<li><p>创建pod-initcontainer.yaml文件，内容如下：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-initcontainer.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-initcontainer</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers: # 容器配置</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - name: nginx-port</span><br><span class="line">          containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpu: &quot;2&quot;</span><br><span class="line">          memory: &quot;10Gi&quot;</span><br><span class="line">        requests:</span><br><span class="line">          cpu: &quot;1&quot;</span><br><span class="line">          memory: &quot;10Mi&quot;</span><br><span class="line">  initContainers: # 初始化容器配置</span><br><span class="line">    - name: test-mysql</span><br><span class="line">      image: 192.168.20.119/library/busybox:latest</span><br><span class="line">      command: [&quot;sh&quot;,&quot;-c&quot;,&quot;until ping 192.168.20.201 -c 1;do echo waiting for mysql ...;sleep 2;done;&quot;]</span><br><span class="line">      securityContext:</span><br><span class="line">        privileged: true # 使用特权模式运行容器</span><br><span class="line">    - name: test-redis</span><br><span class="line">      image: 192.168.20.119/library/busybox:latest</span><br><span class="line">      command: [&quot;sh&quot;,&quot;-c&quot;,&quot;until ping 192.168.20.202 -c 1;do echo waiting for redis ...;sleep 2;done;&quot;]</span><br></pre></td></tr></table></figure>

<ul>
<li>创建Pod：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create -f pod-initcontainer.yaml </span><br><span class="line">pod/pod-initcontainer created</span><br></pre></td></tr></table></figure>

<ul>
<li>查看Pod状态：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl describe pods pod-initcontainer -n dev</span><br><span class="line">Name:         pod-initcontainer</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         master/192.168.20.119</span><br><span class="line">Start Time:   Thu, 26 May 2022 04:58:23 +0000</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Pending</span><br><span class="line">IP:           10.244.0.9</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.0.9</span><br><span class="line">Init Containers:</span><br><span class="line">  test-mysql:</span><br><span class="line">    Container ID:  docker://17eb2b7998b26557b75100f8ac6dc613b69a1e3113f9deaab18d7914b2b8881f</span><br><span class="line">    Image:         192.168.20.119/library/busybox:latest</span><br><span class="line">    Image ID:      docker-pullable://192.168.20.119/library/busybox@sha256:2ca5e69e244d2da7368f7088ea3ad0653c3ce7aaccd0b8823d11b0d5de956002</span><br><span class="line">    Port:          &lt;none&gt;</span><br><span class="line">    Host Port:     &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      sh</span><br><span class="line">      -c</span><br><span class="line">      until ping 192.168.20.201 -c 1;do echo waiting for mysql ...;sleep 2;done;</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Thu, 26 May 2022 04:58:24 +0000</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sxn4 (ro)</span><br><span class="line">  test-redis:</span><br><span class="line">    Container ID:  </span><br><span class="line">    Image:         192.168.20.119/library/busybox:latest</span><br><span class="line">    Image ID:      </span><br><span class="line">    Port:          &lt;none&gt;</span><br><span class="line">    Host Port:     &lt;none&gt;</span><br><span class="line">    Command:</span><br><span class="line">      sh</span><br><span class="line">      -c</span><br><span class="line">      until ping 192.168.20.202 -c 1;do echo waiting for redis ...;sleep 2;done;</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       PodInitializing</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sxn4 (ro)</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   </span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       </span><br><span class="line">    Port:           80/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       PodInitializing</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Limits:</span><br><span class="line">      cpu:     2</span><br><span class="line">      memory:  10Gi</span><br><span class="line">    Requests:</span><br><span class="line">      cpu:        1</span><br><span class="line">      memory:     10Mi</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sxn4 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       False </span><br><span class="line">  Ready             False </span><br><span class="line">  ContainersReady   False </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-7sxn4:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-7sxn4</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       Burstable</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason     Age   From               Message</span><br><span class="line"></span><br><span class="line">----    ------     ----  ----               -------</span><br><span class="line"></span><br><span class="line">  Normal  Scheduled  30s   default-scheduler  Successfully assigned dev/pod-initcontainer to master</span><br><span class="line">  Normal  Pulling    29s   kubelet, master    Pulling image &quot;192.168.20.119/library/busybox:latest&quot;</span><br><span class="line">  Normal  Pulled     29s   kubelet, master    Successfully pulled image &quot;192.168.20.119/library/busybox:latest&quot;</span><br><span class="line">  Normal  Created    29s   kubelet, master    Created container test-mysql</span><br><span class="line">  Normal  Started    29s   kubelet, master    Started container test-mysql</span><br></pre></td></tr></table></figure>

<p><strong>发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行</strong></p>
<ul>
<li>动态查看Pod：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods pod-initcontainer -o wide  -n dev -w</span><br><span class="line">NAME                READY   STATUS     RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-initcontainer   0/1     Init:0/2   0          4m48s   10.244.0.9   master   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>接下来，新开一个shell，为当前服务器（192.168.20.119）新增两个IP，观察Pod的变化：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# ifconfig eth0:1 192.168.20.201 netmask 255.255.255.0 up </span><br><span class="line">[root@master ~]# ifconfig eth0:2 192.168.20.202 netmask 255.255.255.0 up </span><br></pre></td></tr></table></figure>

<p>发现pod已经正确运行</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods pod-initcontainer -o wide  -n dev -w</span><br><span class="line">NAME                READY   STATUS     RESTARTS   AGE     IP           NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-initcontainer   0/1     Init:0/2   0          4m48s   10.244.0.9   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-initcontainer   0/1     Init:1/2   0          7m1s    10.244.0.9   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-initcontainer   0/1     PodInitializing   0          7m3s    10.244.0.9   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">pod-initcontainer   1/1     Running           0          7m4s    10.244.0.9   master   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h3 id="3-4-钩子函数"><a href="#3-4-钩子函数" class="headerlink" title="3.4 钩子函数"></a>3.4 钩子函数</h3><ul>
<li><p>钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。</p>
</li>
<li><p>kubernetes在主容器启动之后和停止之前提供了两个钩子函数：</p>
</li>
<li><ul>
<li>post start：容器创建之后执行，如果失败会重启容器。</li>
</ul>
</li>
<li><ul>
<li>pre stop：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作。</li>
</ul>
</li>
<li><p>钩子处理器支持使用下面的三种方式定义动作：</p>
</li>
<li><ul>
<li>① exec命令：在容器内执行一次命令。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">……</span><br><span class="line">  lifecycle:</span><br><span class="line">     postStart: </span><br><span class="line">        exec:</span><br><span class="line">           command:</span><br><span class="line">             - cat</span><br><span class="line">             - /tmp/healthy</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<ul>
<li><ul>
<li>② tcpSocket：在当前容器尝试访问指定的socket。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">…… </span><br><span class="line">   lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">         tcpSocket:</span><br><span class="line">            port: 8080</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<ul>
<li><ul>
<li>③ httpGet：在当前容器中向某url发起HTTP请求。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">…… </span><br><span class="line">   lifecycle:</span><br><span class="line">      postStart:</span><br><span class="line">         httpGet:</span><br><span class="line">            path: / #URI地址</span><br><span class="line">            port: 80 #端口号</span><br><span class="line">            host: 192.168.109.100 #主机地址  </span><br><span class="line">            scheme: HTTP #支持的协议，http或者https</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<ul>
<li>接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-hook-exec.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-hook-exec</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers: # 容器配置</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - name: nginx-port</span><br><span class="line">          containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">      resources:</span><br><span class="line">        limits:</span><br><span class="line">          cpu: &quot;2&quot;</span><br><span class="line">          memory: &quot;10Gi&quot;</span><br><span class="line">        requests:</span><br><span class="line">          cpu: &quot;1&quot;</span><br><span class="line">          memory: &quot;10Mi&quot;</span><br><span class="line">      lifecycle: # 生命周期配置</span><br><span class="line">        postStart: # 容器创建之后执行，如果失败会重启容器</span><br><span class="line">          exec: # 在容器启动的时候，执行一条命令，修改掉Nginx的首页内容</span><br><span class="line">            command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;echo postStart ... &gt; /usr/share/nginx/html/index.html&quot;]</span><br><span class="line">        preStop: # 容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作</span><br><span class="line">          exec: # 在容器停止之前停止Nginx的服务</span><br><span class="line">            command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]</span><br></pre></td></tr></table></figure>

<ul>
<li>创建Pod：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create -f pod-hook-exec.yaml </span><br><span class="line">pod/pod-hook-exec created</span><br><span class="line">[root@master ~]# kubectl get pods pod-hook-exec -n dev -o wide</span><br><span class="line">NAME            READY   STATUS    RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-hook-exec   1/1     Running   0          24s   10.244.2.7   node1   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>访问Pod：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# curl 10.244.2.7</span><br><span class="line">postStart ...</span><br></pre></td></tr></table></figure>

<h3 id="3-5-容器探测"><a href="#3-5-容器探测" class="headerlink" title="3.5 容器探测"></a>3.5 容器探测</h3><h4 id="3-5-1-概述"><a href="#3-5-1-概述" class="headerlink" title="3.5.1 概述"></a>3.5.1 概述</h4><ul>
<li><p>容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例“摘除”，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是：</p>
</li>
<li><ul>
<li>liveness probes：存活性探测，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器。</li>
</ul>
</li>
<li><ul>
<li>readiness probes：就绪性探测，用于检测应用实例是否可以接受请求，如果不能，k8s不会转发流量。</li>
</ul>
</li>
</ul>
<blockquote>
<p>livenessProbe：存活性探测，决定是否重启容器。</p>
<p>readinessProbe：就绪性探测，决定是否将请求转发给容器。</p>
</blockquote>
<blockquote>
<p>k8s在1.16版本之后新增了startupProbe探针，用于判断容器内应用程序是否已经启动。如果配置了startupProbe探针，就会先禁止其他的探针，直到startupProbe探针成功为止，一旦成功将不再进行探测。</p>
</blockquote>
<ul>
<li><p>上面两种探针目前均支持三种探测方式：</p>
</li>
<li><ul>
<li>① exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">……</span><br><span class="line">  livenessProbe:</span><br><span class="line">     exec:</span><br><span class="line">        command:</span><br><span class="line">          -	cat</span><br><span class="line">          -	/tmp/healthy</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<ul>
<li><ul>
<li>② tcpSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">……</span><br><span class="line">   livenessProbe:</span><br><span class="line">      tcpSocket:</span><br><span class="line">         port: 8080</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<ul>
<li><ul>
<li>③ httpGet：调用容器内web应用的URL，如果返回的状态码在200和399之前，则认为程序正常，否则不正常。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">……</span><br><span class="line">   livenessProbe:</span><br><span class="line">      httpGet:</span><br><span class="line">         path: / #URI地址</span><br><span class="line">         port: 80 #端口号</span><br><span class="line">         host: 127.0.0.1 #主机地址</span><br><span class="line">         scheme: HTTP #支持的协议，http或者https</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<h4 id="3-5-2-exec方式"><a href="#3-5-2-exec方式" class="headerlink" title="3.5.2 exec方式"></a>3.5.2 exec方式</h4><ul>
<li>创建pod-liveness-exec.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-liveness-exec.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-liveness-exec</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers: # 容器配置</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - name: nginx-port</span><br><span class="line">          containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">      livenessProbe: # 存活性探针</span><br><span class="line">        exec:</span><br><span class="line">          command: [&quot;/bin/cat&quot;,&quot;/tmp/hello.txt&quot;] # 执行一个查看文件的命令，必须失败，因为根本没有这个文件</span><br></pre></td></tr></table></figure>

<ul>
<li>查看Pod详情：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create -f pod-liveness-exec.yaml </span><br><span class="line">pod/pod-liveness-exec created</span><br><span class="line">[root@master ~]# kubectl describe pod pod-liveness-exec -n dev</span><br><span class="line">Name:         pod-liveness-exec</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node2/192.168.20.124</span><br><span class="line">Start Time:   Thu, 26 May 2022 05:22:10 +0000</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           10.244.1.6</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.1.6</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://b9a138a73f0e5137efdee4497ee710e9fd7f2ed25608577d0470c4b5a16c90fb</span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83</span><br><span class="line">    Port:           80/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Waiting</span><br><span class="line">      Reason:       CrashLoopBackOff</span><br><span class="line">    Last State:     Terminated</span><br><span class="line">      Reason:       Completed</span><br><span class="line">      Exit Code:    0</span><br><span class="line">      Started:      Thu, 26 May 2022 05:25:17 +0000</span><br><span class="line">      Finished:     Thu, 26 May 2022 05:25:47 +0000</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  5</span><br><span class="line">    Liveness:       exec [/bin/cat /tmp/hello.txt] delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sxn4 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             False </span><br><span class="line">  ContainersReady   False </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-7sxn4:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-7sxn4</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                     From               Message</span><br><span class="line">  ----     ------     ----                    ----               -------</span><br><span class="line">  Normal   Scheduled  3m56s                   default-scheduler  Successfully assigned dev/pod-liveness-exec to node2</span><br><span class="line">  Normal   Pulled     2m29s (x4 over 3m55s)   kubelet, node2     Container image &quot;192.168.20.119/library/nginx:latest&quot; already present on machine</span><br><span class="line">  Normal   Created    2m29s (x4 over 3m55s)   kubelet, node2     Created container nginx</span><br><span class="line">  Normal   Started    2m29s (x4 over 3m55s)   kubelet, node2     Started container nginx</span><br><span class="line">  Normal   Killing    2m29s (x3 over 3m29s)   kubelet, node2     Container nginx failed liveness probe, will be restarted #容器 nginx 探测失败，将被重启</span><br><span class="line">  Warning  Unhealthy  2m19s (x10 over 3m49s)  kubelet, node2     Liveness probe failed: /bin/cat: /tmp/hello.txt: No such file or directory #找不到这个文件</span><br></pre></td></tr></table></figure>

<blockquote>
<ul>
<li><p>观察上面的信息就会发现nginx容器启动之后就进行了健康检查。</p>
</li>
<li><p>检查失败之后，容器被kill掉，然后尝试进行重启，这是重启策略的作用。</p>
</li>
<li><p>稍等一会之后，再观察Pod的信息，就会看到RESTARTS不再是0，而是一直增长。</p>
</li>
</ul>
</blockquote>
<ul>
<li>查看Pod信息：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods pod-liveness-exec -n dev -o wide</span><br><span class="line">NAME                READY   STATUS             RESTARTS   AGE     IP           NODE    NOMINATED NODE   READINESS GATES</span><br><span class="line">pod-liveness-exec   0/1     CrashLoopBackOff   6          7m23s   10.244.1.6   node2   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<h4 id="3-5-3-tcpSocket方式"><a href="#3-5-3-tcpSocket方式" class="headerlink" title="3.5.3 tcpSocket方式"></a>3.5.3 tcpSocket方式</h4><ul>
<li>创建pod-liveness-tcpsocket.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-liveness-tcpsocket.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-liveness-tcpsocket</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers: # 容器配置</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - name: nginx-port</span><br><span class="line">          containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">      livenessProbe: # 存活性探针</span><br><span class="line">        tcpSocket:</span><br><span class="line">          port: 8080 # 尝试访问8080端口，必须失败，因为Pod内部只有一个Nginx容器，而且只是监听了80端口</span><br></pre></td></tr></table></figure>

<ul>
<li>查看Pod详情：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create -f pod-liveness-tcpsocket.yaml </span><br><span class="line">pod/pod-liveness-tcpsocket created</span><br><span class="line">[root@master ~]# kubectl describe pod pod-liveness-tcpsocket -n dev</span><br><span class="line">Name:         pod-liveness-tcpsocket</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node2/192.168.20.124</span><br><span class="line">Start Time:   Thu, 26 May 2022 05:32:55 +0000</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           10.244.1.7</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.1.7</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://e5dfd067ca2dec264240e256726ed74eacf86ddbb97fa8a4a69ec6420b9d5525</span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83</span><br><span class="line">    Port:           80/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Thu, 26 May 2022 05:33:19 +0000</span><br><span class="line">    Last State:     Terminated</span><br><span class="line">      Reason:       Completed</span><br><span class="line">      Exit Code:    0</span><br><span class="line">      Started:      Thu, 26 May 2022 05:32:56 +0000</span><br><span class="line">      Finished:     Thu, 26 May 2022 05:33:18 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  1</span><br><span class="line">    Liveness:       tcp-socket :8080 delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sxn4 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             True </span><br><span class="line">  ContainersReady   True </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-7sxn4:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-7sxn4</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age               From               Message</span><br><span class="line">  ----     ------     ----              ----               -------</span><br><span class="line">  Normal   Scheduled  31s               default-scheduler  Successfully assigned dev/pod-liveness-tcpsocket to node2</span><br><span class="line">  Normal   Pulled     8s (x2 over 30s)  kubelet, node2     Container image &quot;192.168.20.119/library/nginx:latest&quot; already present on machine</span><br><span class="line">  Normal   Created    8s (x2 over 30s)  kubelet, node2     Created container nginx</span><br><span class="line">  Warning  Unhealthy  8s (x3 over 28s)  kubelet, node2     Liveness probe failed: dial tcp 10.244.1.7:8080: connect: connection refused</span><br><span class="line">  Normal   Killing    8s                kubelet, node2     Container nginx failed liveness probe, will be restarted</span><br><span class="line">  Normal   Started    7s (x2 over 30s)  kubelet, node2     Started container nginx</span><br></pre></td></tr></table></figure>

<blockquote>
<p>观察上面的信息，发现尝试访问8080端口，但是失败了</p>
<p>稍等一会之后，再观察Pod的信息，就会看到RESTARTS不再是0，而是一直增长。</p>
</blockquote>
<ul>
<li>查看Pod信息：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods pod-liveness-tcpsocket -n dev -w</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod-liveness-tcpsocket   1/1     Running   3          97s</span><br><span class="line">pod-liveness-tcpsocket   1/1     Running   4          114s</span><br><span class="line">pod-liveness-tcpsocket   0/1     CrashLoopBackOff   4          2m24s</span><br></pre></td></tr></table></figure>

<h4 id="3-5-4-httpGet方式"><a href="#3-5-4-httpGet方式" class="headerlink" title="3.5.4 httpGet方式"></a>3.5.4 httpGet方式</h4><ul>
<li>创建pod-liveness-httpget.yaml文件，内容如下：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-liveness-httpget.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-liveness-httpget</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers: # 容器配置</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - name: nginx-port</span><br><span class="line">          containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">      livenessProbe: # 存活性探针</span><br><span class="line">        httpGet: # 其实就是访问http://127.0.0.1:80/hello</span><br><span class="line">          port: 80 # 端口号</span><br><span class="line">          scheme: HTTP # 支持的协议，HTTP或HTTPS</span><br><span class="line">          path: /hello # URI地址</span><br><span class="line">          host: 127.0.0.1 # 主机地址</span><br></pre></td></tr></table></figure>

<ul>
<li>查看Pod详情：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create -f pod-liveness-httpget.yaml </span><br><span class="line">pod/pod-liveness-httpget created</span><br><span class="line">[root@master ~]# kubectl describe pod pod-liveness-httpget -n dev</span><br><span class="line">Name:         pod-liveness-httpget</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node2/192.168.20.124</span><br><span class="line">Start Time:   Thu, 26 May 2022 05:38:43 +0000</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           10.244.1.8</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.1.8</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://477faf82ca2bffc2914f103b1c1d11c733c41c55c3ec0978c0b726a51fbf68bc</span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83</span><br><span class="line">    Port:           80/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Thu, 26 May 2022 05:38:44 +0000</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Liveness:       http-get http://127.0.0.1:80/hello delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sxn4 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             True </span><br><span class="line">  ContainersReady   True </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-7sxn4:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-7sxn4</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age   From               Message</span><br><span class="line"></span><br><span class="line">----     ------     ----  ----               -------</span><br><span class="line"></span><br><span class="line">  Normal   Scheduled  15s   default-scheduler  Successfully assigned dev/pod-liveness-httpget to node2</span><br><span class="line">  Normal   Pulled     14s   kubelet, node2     Container image &quot;192.168.20.119/library/nginx:latest&quot; already present on machine</span><br><span class="line">  Normal   Created    14s   kubelet, node2     Created container nginx</span><br><span class="line">  Normal   Started    14s   kubelet, node2     Started container nginx</span><br><span class="line">  Warning  Unhealthy  6s    kubelet, node2     Liveness probe failed: Get http://127.0.0.1:80/hello: dial tcp 127.0.0.1:80: connect: connection refused</span><br></pre></td></tr></table></figure>

<ul>
<li>查看Pod信息：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pods pod-liveness-httpget -n dev</span><br><span class="line">NAME                   READY   STATUS             RESTARTS   AGE</span><br><span class="line">pod-liveness-httpget   0/1     CrashLoopBackOff   6          7m32s</span><br></pre></td></tr></table></figure>

<h4 id="3-5-5-容器探测的补充"><a href="#3-5-5-容器探测的补充" class="headerlink" title="3.5.5 容器探测的补充"></a>3.5.5 容器探测的补充</h4><ul>
<li>上面已经使用了livenessProbe演示了三种探测方式，但是查看livenessProbe的子属性，会发现除了这三种方式，还有一些其他的配置。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl explain pod.spec.containers.livenessProbe</span><br><span class="line">KIND:     Pod</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">RESOURCE: livenessProbe &lt;Object&gt;</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">     Periodic probe of container liveness. Container will be restarted if the</span><br><span class="line">     probe fails. Cannot be updated. More info:</span><br><span class="line">     https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes</span><br><span class="line"></span><br><span class="line">     Probe describes a health check to be performed against a container to</span><br><span class="line">     determine whether it is alive or ready to receive traffic.</span><br><span class="line"></span><br><span class="line">FIELDS:</span><br><span class="line">   exec &lt;Object&gt;</span><br><span class="line">     One and only one of the following should be specified. Exec specifies the</span><br><span class="line">     action to take.</span><br><span class="line"></span><br><span class="line">   failureThreshold     &lt;integer&gt;</span><br><span class="line">     Minimum consecutive failures for the probe to be considered failed after</span><br><span class="line">     having succeeded. Defaults to 3. Minimum value is 1.</span><br><span class="line"></span><br><span class="line">   httpGet      &lt;Object&gt;</span><br><span class="line">     HTTPGet specifies the http request to perform.</span><br><span class="line"></span><br><span class="line">   initialDelaySeconds  &lt;integer&gt;</span><br><span class="line">     Number of seconds after the container has started before liveness probes</span><br><span class="line">     are initiated. More info:</span><br><span class="line">     https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes</span><br><span class="line"></span><br><span class="line">   periodSeconds        &lt;integer&gt;</span><br><span class="line">     How often (in seconds) to perform the probe. Default to 10 seconds. Minimum</span><br><span class="line">     value is 1.</span><br><span class="line"></span><br><span class="line">   successThreshold     &lt;integer&gt;</span><br><span class="line">     Minimum consecutive successes for the probe to be considered successful</span><br><span class="line">     after having failed. Defaults to 1. Must be 1 for liveness and startup.</span><br><span class="line">     Minimum value is 1.</span><br><span class="line"></span><br><span class="line">   tcpSocket    &lt;Object&gt;</span><br><span class="line">     TCPSocket specifies an action involving a TCP port. TCP hooks not yet</span><br><span class="line">     supported</span><br><span class="line"></span><br><span class="line">   timeoutSeconds       &lt;integer&gt;</span><br><span class="line">     Number of seconds after which the probe times out. Defaults to 1 second.</span><br><span class="line">     Minimum value is 1. More info:</span><br><span class="line">     https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes</span><br></pre></td></tr></table></figure>

<blockquote>
<p>FIELDS:</p>
<p>exec </p>
<p>tcpSocket  </p>
<p>httpGet   </p>
<p>initialDelaySeconds   # 容器启动后等待多少秒执行第一次探测</p>
<p>timeoutSeconds    # 探测超时时间。默认1秒，最小1秒</p>
<p>periodSeconds    # 执行探测的频率。默认是10秒，最小1秒</p>
<p>failureThreshold   # 连续探测失败多少次才被认定为失败。默认是3。最小值是1</p>
<p>successThreshold   # 连续探测成功多少次才被认定为成功。默认是1</p>
</blockquote>
<h3 id="3-6-重启策略"><a href="#3-6-重启策略" class="headerlink" title="3.6 重启策略"></a>3.6 重启策略</h3><ul>
<li><p>在容器探测中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由Pod的重启策略决定的，Pod的重启策略有3种，分别如下：</p>
</li>
<li><ul>
<li>Always：容器失效时，自动重启该容器，默认值。</li>
</ul>
</li>
<li><ul>
<li>OnFailure：容器终止运行且退出码不为0时重启。</li>
</ul>
</li>
<li><ul>
<li>Never：不论状态如何，都不重启该容器。</li>
</ul>
</li>
<li><p>重启策略适用于Pod对象中的所有容器，首次需要重启的容器，将在其需要的时候立即进行重启，随后再次重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大的延迟时长。</p>
</li>
<li><p>创建pod-restart-policy.yaml文件，内容如下：</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pod-restart-policy.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-restart-policy</span><br><span class="line">  namespace: dev</span><br><span class="line">  labels:</span><br><span class="line">    user: xudaxian</span><br><span class="line">spec:</span><br><span class="line">  containers: # 容器配置</span><br><span class="line">    - name: nginx</span><br><span class="line">      image: 192.168.20.119/library/nginx:latest</span><br><span class="line">      imagePullPolicy: IfNotPresent</span><br><span class="line">      ports:</span><br><span class="line">        - name: nginx-port</span><br><span class="line">          containerPort: 80</span><br><span class="line">          protocol: TCP</span><br><span class="line">      livenessProbe: # 存活性探测</span><br><span class="line">        httpGet:</span><br><span class="line">          port: 80</span><br><span class="line">          path: /hello</span><br><span class="line">          host: 127.0.0.1</span><br><span class="line">          scheme: HTTP</span><br><span class="line">  restartPolicy: Never # 重启策略</span><br></pre></td></tr></table></figure>

<ul>
<li>查看Pod详情，发现nginx容器启动失败：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl create -f pod-restart-policy.yaml </span><br><span class="line">pod/pod-restart-policy created</span><br><span class="line">[root@master ~]# kubectl describe pod pod-restart-policy -n dev</span><br><span class="line">Name:         pod-restart-policy</span><br><span class="line">Namespace:    dev</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         node2/192.168.20.124</span><br><span class="line">Start Time:   Thu, 26 May 2022 05:50:27 +0000</span><br><span class="line">Labels:       user=xudaxian</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line">Status:       Succeeded</span><br><span class="line">IP:           10.244.1.9</span><br><span class="line">IPs:</span><br><span class="line">  IP:  10.244.1.9</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://b0adcc8f6fccad320ba37dd747b3c41e41a4f34b86a0a57612c5da45020b562c</span><br><span class="line">    Image:          192.168.20.119/library/nginx:latest</span><br><span class="line">    Image ID:       docker-pullable://192.168.20.119/library/nginx@sha256:416d511ffa63777489af47f250b70d1570e428b67666567085f2bece3571ad83</span><br><span class="line">    Port:           80/TCP</span><br><span class="line">    Host Port:      0/TCP</span><br><span class="line">    State:          Terminated</span><br><span class="line">      Reason:       Completed</span><br><span class="line">      Exit Code:    0</span><br><span class="line">      Started:      Thu, 26 May 2022 05:50:29 +0000</span><br><span class="line">      Finished:     Thu, 26 May 2022 05:50:51 +0000</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Liveness:       http-get http://127.0.0.1:80/hello delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sxn4 (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             False </span><br><span class="line">  ContainersReady   False </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-7sxn4:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-7sxn4</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line">Node-Selectors:  &lt;none&gt;</span><br><span class="line">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason     Age                From               Message</span><br><span class="line">  ----     ------     ----               ----               -------</span><br><span class="line">  Normal   Scheduled  38s                default-scheduler  Successfully assigned dev/pod-restart-policy to node2</span><br><span class="line">  Normal   Pulled     36s                kubelet, node2     Container image &quot;192.168.20.119/library/nginx:latest&quot; already present on machine</span><br><span class="line">  Normal   Created    36s                kubelet, node2     Created container nginx</span><br><span class="line">  Normal   Started    36s                kubelet, node2     Started container nginx</span><br><span class="line">  Warning  Unhealthy  14s (x3 over 34s)  kubelet, node2     Liveness probe failed: Get http://127.0.0.1:80/hello: dial tcp 127.0.0.1:80: connect: connection refused</span><br><span class="line">  Normal   Killing    14s                kubelet, node2     Stopping container nginx</span><br></pre></td></tr></table></figure>

<ul>
<li>查看Pod：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pod pod-restart-policy -n dev</span><br><span class="line">NAME                 READY   STATUS      RESTARTS   AGE</span><br><span class="line">pod-restart-policy   0/1     Completed   0          103s</span><br></pre></td></tr></table></figure>

<p><strong>多等一会，观察Pod的重试次数，发现一直是0，并未重启。</strong></p>
]]></content>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2023/04/19/openstacksdk/</url>
    <content><![CDATA[<h1 id="openstacksdk编写"><a href="#openstacksdk编写" class="headerlink" title="openstacksdk编写"></a>openstacksdk编写</h1><p>sdk_server_manager.py：<span id="more"></span></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">===========================================</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Copyright Jiangsu One Cloud Technology Development Co. LTD. All Rights Reserved.</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">===========================================</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">encoding:utf-8</span></span><br><span class="line">import json,logging</span><br><span class="line"></span><br><span class="line">import openstack</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">文档地址</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">https://docs.openstack.org/openstacksdk/latest/user/index.html</span></span><br><span class="line"></span><br><span class="line">def create_connection(auth_url, user_domain_name, username, password):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    建立连接</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return openstack.connect(</span><br><span class="line">        auth_url=auth_url,</span><br><span class="line">        user_domain_name=user_domain_name,</span><br><span class="line">        username=username,</span><br><span class="line">        password=password,</span><br><span class="line">    )</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">user Manager</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">参见文档</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">https://docs.openstack.org/openstacksdk/latest/user/guides/identity.html</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">openstack.connection.Connection</span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">云主机管理</span></span><br><span class="line">class server_manager:</span><br><span class="line"></span><br><span class="line">    def __init__(self, connect):</span><br><span class="line">        self.connect = connect</span><br><span class="line"></span><br><span class="line">    def list_servers(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        查询所有云主机.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        #to json</span><br><span class="line">        items = self.connect.compute.servers()</span><br><span class="line">        server_jsons = &#123;&#125;</span><br><span class="line">        for server in items:</span><br><span class="line">            server_jsons[server[&#x27;name&#x27;]] = server</span><br><span class="line">        # return &quot;&quot;</span><br><span class="line">        return items# json.dumps(server_jsons,indent=2,skipkeys=True)</span><br><span class="line"></span><br><span class="line">    def create_server(self, server_name, image_name, flavor_name,networ_name):</span><br><span class="line">        image = self.connect.compute.find_image(image_name)</span><br><span class="line">        flavor = self.connect.compute.find_flavor(flavor_name)</span><br><span class="line">        network = self.connect.network.find_network(networ_name)</span><br><span class="line">        server = self.connect.compute.create_server(</span><br><span class="line">            name=server_name, image_id=image.id, flavor_id=flavor.id,</span><br><span class="line">            networks=[&#123;&quot;uuid&quot;: network.id&#125;])</span><br><span class="line">        result = self.connect.compute.wait_for_server(server)</span><br><span class="line">        return result#json.dumps(result,indent=2,skipkeys=True)</span><br><span class="line"></span><br><span class="line">    def delete_server(self, server_name):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        删除云主机</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        server = self.connect.compute.find_server(server_name)</span><br><span class="line">        result = self.connect.compute.delete_server(server)</span><br><span class="line">        return json.dumps(result, indent=2, skipkeys=True)</span><br><span class="line"></span><br><span class="line">    def get_server(self, server_name):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        获取云主机</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        server = self.connect.compute.find_server(server_name)</span><br><span class="line">        if server:</span><br><span class="line">            return json.dumps(server, indent=2, skipkeys=True)</span><br><span class="line">        else:</span><br><span class="line">            return None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class image_manager:</span><br><span class="line"></span><br><span class="line">    def __init__(self, connect):</span><br><span class="line">        self.connect = connect</span><br><span class="line"></span><br><span class="line">    def list_images(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        查询所有镜像</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        #to json</span><br><span class="line">        items = self.connect.compute.images()</span><br><span class="line">        images_jsons = &#123;&#125;</span><br><span class="line">        for image in items:</span><br><span class="line">            images_jsons[image[&#x27;name&#x27;]] = image</span><br><span class="line">        return json.dumps(images_jsons,indent=2)</span><br><span class="line"></span><br><span class="line">    def get_image(self, image_name:str):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        查询镜像</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        #to json</span><br><span class="line">        image = self.connect.compute.find_image(image_name)</span><br><span class="line"></span><br><span class="line">        return json.dumps(image,indent=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class flavor_manager:</span><br><span class="line"></span><br><span class="line">    def __init__(self, connect):</span><br><span class="line">        self.connect = connect</span><br><span class="line"></span><br><span class="line">    def list_flavors(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        查询所有云主机类型</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        #to json</span><br><span class="line">        items = self.connect.compute.flavors()</span><br><span class="line">        flavors_jsons = &#123;&#125;</span><br><span class="line">        for flavor in items:</span><br><span class="line">            flavors_jsons[flavor[&#x27;name&#x27;]] = flavor</span><br><span class="line">        return json.dumps(flavors_jsons,indent=2)</span><br><span class="line"></span><br><span class="line">    def get_flavor(self, flavor_name:str):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        根据名称获取云主机类.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        #to json</span><br><span class="line">        flavor = self.connect.compute.find_flavor(flavor_name)</span><br><span class="line">        return json.dumps(flavor,indent=2)</span><br><span class="line"></span><br><span class="line">class network_manager:</span><br><span class="line"></span><br><span class="line">    def __init__(self, connect):</span><br><span class="line">        self.connect = connect</span><br><span class="line"></span><br><span class="line">    def list_networks(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        查询所有网络.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        #to json</span><br><span class="line">        items = self.connect.network.networks()</span><br><span class="line">        items_jsons = &#123;&#125;</span><br><span class="line">        for network in items:</span><br><span class="line">            items_jsons[network[&#x27;name&#x27;]] = network</span><br><span class="line">        return json.dumps(items_jsons,indent=2)</span><br><span class="line"></span><br><span class="line">    def get_network(self, network_name:str):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        跟名称查询网络.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        #to json</span><br><span class="line">        flavor = self.connect.compute.find_network(network_name)</span><br><span class="line">        return json.dumps(flavor,indent=2)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line"></span><br><span class="line">    # Initialize connection(通过配置文件）</span><br><span class="line">    # controller_ip = &quot;10.24.2.22&quot;</span><br><span class="line">    controller_ip = &quot;controller&quot;</span><br><span class="line">    auth_url = &quot;http://controller:5000/v3/&quot;</span><br><span class="line">    username = &quot;admin&quot;</span><br><span class="line">    password = &quot;000000&quot;</span><br><span class="line">    user_domain_name = &#x27;demo&#x27;</span><br><span class="line"></span><br><span class="line">    conn = create_connection(auth_url, user_domain_name, username, password)</span><br><span class="line"></span><br><span class="line">    sdk_m = server_manager(conn)</span><br><span class="line">    server = sdk_m.get_server(&quot;server001&quot;)</span><br><span class="line">    if server:</span><br><span class="line">        result = sdk_m.delete_server(&quot;server001&quot;)</span><br><span class="line">        print(&quot;servers:&quot;, result)</span><br><span class="line"></span><br><span class="line">    #2 创建云主机</span><br><span class="line">    print(&quot;creat server--------&quot;)</span><br><span class="line">    servers = sdk_m.create_server(&quot;server001&quot;,&quot;cirros001&quot;,&quot;m1.tiny&quot;,&quot;net&quot;)</span><br><span class="line">    print(&quot;servers:&quot;, servers)</span><br><span class="line"></span><br><span class="line">    #6 查询云主机</span><br><span class="line">    server_info = sdk_m.get_server(&quot;server001&quot;)</span><br><span class="line">print(server_info)</span><br></pre></td></tr></table></figure>

]]></content>
  </entry>
  <entry>
    <title>pig-dockerfile</title>
    <url>/2022/06/30/pig-dockerfile/</url>
    <content><![CDATA[<h1 id="Pig平台docker-compose部署"><a href="#Pig平台docker-compose部署" class="headerlink" title="Pig平台docker-compose部署"></a>Pig平台docker-compose部署</h1><h3 id="容器化部署MariaDB"><a href="#容器化部署MariaDB" class="headerlink" title="容器化部署MariaDB "></a>容器化部署MariaDB <span id="more"></span></h3><p><code>curl -O http://172.19.25.11/Pig.tar.gz</code></p>
<p><code>tar -xf Pig.tar.gz</code></p>
<p><code>cd Pig</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat Dockerfile-mariadb </span><br><span class="line">FROM centos:centos7.9.2009</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">copy yum /root/yum</span><br><span class="line">ENV LC_ALL en_US.UTF-8</span><br><span class="line">RUN yum install -y mariadb-server</span><br><span class="line">ADD mysql /opt/</span><br><span class="line">copy mysql_init.sh /opt/</span><br><span class="line">RUN bash /opt/mysql_init.sh</span><br><span class="line">EXPOSE 3306</span><br><span class="line">CMD [&quot;mysqld_safe&quot;,&quot;--user=root&quot;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat mysql_init.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">mysql_install_db --user=root</span><br><span class="line">mysqld_safe --user=root &amp;</span><br><span class="line">sleep 8</span><br><span class="line">mysqladmin -u root password &#x27;root&#x27;</span><br><span class="line">mysql -uroot -proot -e &quot;grant all on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;root&#x27;;flush privileges;&quot;</span><br><span class="line">mysql -uroot -proot -e &quot;source /opt/pig.sql;source /opt/pig_codegen.sql;source /opt/pig_config.sql;source /opt/pig_job.sql;&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat local.repo </span><br><span class="line">[pig]</span><br><span class="line">name=pig</span><br><span class="line">baseurl=file:///root/yum</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<h3 id="容器化部署Redis"><a href="#容器化部署Redis" class="headerlink" title="容器化部署Redis"></a>容器化部署Redis</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat Dockerfile-redis </span><br><span class="line">FROM centos:centos7.9.2009</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">copy yum /root/yum</span><br><span class="line">RUN yum install -y redis</span><br><span class="line">RUN sed -i &#x27;s/bind 127.0.0.1/bind 0.0.0.0/g&#x27; /etc/redis.conf</span><br><span class="line">RUN sed -i &#x27;s/protected-mode yes/protected-mode no/g&#x27; /etc/redis.conf</span><br><span class="line">EXPOSE 6379</span><br><span class="line">CMD [&quot;/usr/bin/redis-server&quot;,&quot;/etc/redis.conf&quot;]</span><br></pre></td></tr></table></figure>

<h3 id="容器化部署Pig"><a href="#容器化部署Pig" class="headerlink" title="容器化部署Pig"></a>容器化部署Pig</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat Dockerfile-pig </span><br><span class="line">FROM centos:centos7.9.2009</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">copy yum /root/yum</span><br><span class="line">RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel</span><br><span class="line">ADD pig_init.sh /root/</span><br><span class="line">RUN chmod +x  /root/pig_init.sh</span><br><span class="line">EXPOSE 8848 9999 3000 4000</span><br><span class="line">CMD [&quot;/bin/bash&quot;,&quot;/root/pig_init.sh&quot;]</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat pig_init.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">sleep 20</span><br><span class="line">nohup java -jar /root/pig-register.jar  $JAVA_OPTS  &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">sleep 20</span><br><span class="line">nohup java -jar /root/pig-gateway.jar  $JAVA_OPTS &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">sleep 20</span><br><span class="line">nohup java -jar /root/pig-auth.jar  $JAVA_OPTS &gt;/dev/null 2&gt;&amp;1 &amp;</span><br><span class="line">sleep 20</span><br><span class="line">nohup java -jar /root/pig-upms-biz.jar  $JAVA_OPTS &gt;/dev/null 2&gt;&amp;1</span><br></pre></td></tr></table></figure>

<h3 id="容器化部署前端服务"><a href="#容器化部署前端服务" class="headerlink" title="容器化部署前端服务"></a>容器化部署前端服务</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat Dockerfile-nginx </span><br><span class="line">FROM centos:centos7.9.2009</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">copy yum /root/yum</span><br><span class="line">RUN yum install -y nginx</span><br><span class="line">COPY nginx/dist /data</span><br><span class="line">ADD nginx/pig-ui.conf /etc/nginx/conf.d/</span><br><span class="line">RUN /bin/bash -c &#x27;echo init ok&#x27;</span><br><span class="line">EXPOSE 80</span><br><span class="line">CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]</span><br></pre></td></tr></table></figure>

<h3 id="编排部署Pig快速开发平台"><a href="#编排部署Pig快速开发平台" class="headerlink" title="编排部署Pig快速开发平台"></a>编排部署Pig快速开发平台</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node-szzjs002-1958801-g3yaw Pig]# cat docker-compose.yaml </span><br><span class="line">version: &#x27;2&#x27;</span><br><span class="line">services:</span><br><span class="line">  pig-mysql:</span><br><span class="line">    environment:</span><br><span class="line">      MYSQL_ROOT_PASSWORD: root</span><br><span class="line">    restart: always</span><br><span class="line">    container_name: pig-mysql</span><br><span class="line">    image: pig-mysql:v1.0</span><br><span class="line">    ports:</span><br><span class="line">      - 3306:3306</span><br><span class="line">    links:</span><br><span class="line">      - pig-service:pig-register</span><br><span class="line">  pig-redis:</span><br><span class="line">    image: pig-redis:v1.0</span><br><span class="line">    ports:</span><br><span class="line">      - 6379:6379</span><br><span class="line">    restart: always</span><br><span class="line">    container_name: pig-redis</span><br><span class="line">    hostname: pig-redis</span><br><span class="line">    links:</span><br><span class="line">      - pig-service:pig-register</span><br><span class="line">  pig-service:</span><br><span class="line">    ports:</span><br><span class="line">      - 8848:8848</span><br><span class="line">      - 9999:9999</span><br><span class="line">    restart: always</span><br><span class="line">    container_name: pig-service</span><br><span class="line">    hostname: pig-service</span><br><span class="line">    image: pig-service:v1.0</span><br><span class="line">    extra_hosts:</span><br><span class="line">      - pig-register:127.0.0.1</span><br><span class="line">      - pig-upms:127.0.0.1</span><br><span class="line">      - pig-gateway:127.0.0.1</span><br><span class="line">      - pig-auth:127.0.0.1</span><br><span class="line">      - pig-hou:127.0.0.1</span><br><span class="line">    stdin_open: true</span><br><span class="line">    tty: true</span><br><span class="line">    privileged: true</span><br><span class="line">  pig-ui:</span><br><span class="line">    restart: always</span><br><span class="line">    container_name: pig-ui</span><br><span class="line">    image: pig-ui:v1.0</span><br><span class="line">    ports:</span><br><span class="line">      - 8888:80</span><br><span class="line">    links:</span><br><span class="line">      - pig-service:pig-gateway</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>容器云</tag>
      </tags>
  </entry>
  <entry>
    <title>python-api</title>
    <url>/2022/06/06/python-api/</url>
    <content><![CDATA[<h1 id="OpenStack-API-运维任务-10-分"><a href="#OpenStack-API-运维任务-10-分" class="headerlink" title="OpenStack API 运维任务[10 分]"></a>OpenStack API 运维任务[10 分]<span id="more"></span></h1><p>【题目 1】使用 python 调用 api 实现创建 user[2 分]<br>在自行搭建的 OpenStack 私有云平台或提供的 all-in-one 平台上，根据 http 服务中提供的 Python-api.tar.gz 软件包，完成 python3.6 软件和依赖库的安装。在 controller 节点的&#x2F;root2021 年职业院校技能大赛“云计算”赛项 赛卷目录下创建 create_user.py 文件，编写 python 代码对接 OpenStack API，完成用户的创建。要求在 OpenStack 私有云平台中创建用户 chinaskill，描述为“API create user!”。执行完代码要求输出“用户创建成功”。根据上述要求编写 python 代码，完成后，将 controller 节点的IP 地址，用户名和密码提交。（考试系统会连接到你的 controller 节点，去执行 python 脚本，请准备好运行的环境，以便考试系统访问）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_user.py </span><br><span class="line">#coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">body = &#123;</span><br><span class="line">        &quot;auth&quot;:&#123;</span><br><span class="line">            &quot;identity&quot;:&#123;</span><br><span class="line">               &quot;methods&quot;:[&quot;password&quot;],</span><br><span class="line">               &quot;password&quot;:&#123;</span><br><span class="line">                   &quot;user&quot;:&#123;</span><br><span class="line">                      &quot;id&quot;:&quot;179fac1341f64033800765d4be2f09c2&quot;,</span><br><span class="line">                      &quot;password&quot;:&quot;000000&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;,</span><br><span class="line">             &quot;scope&quot;:&#123;</span><br><span class="line">                  &quot;project&quot;:&#123;</span><br><span class="line">                      &quot;id&quot;:&quot;ddc482a7bfb64a078f92260296ea723c&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;&#125;</span><br><span class="line">def get_token():</span><br><span class="line">    url = &quot;http://192.168.20.113:5000/v3/auth/tokens&quot;</span><br><span class="line">    re = requests.post(url,headers=headers,data=json.dumps(body)).headers[&quot;X-Subject-Token&quot;]</span><br><span class="line">    return re</span><br><span class="line">def create_user():</span><br><span class="line">    url = &quot;http://192.168.20.113:35357/v3/users&quot;</span><br><span class="line">    headers[&quot;X-Auth-Token&quot;] = get_token()</span><br><span class="line">    body=&#123;</span><br><span class="line">        &quot;user&quot;:&#123;</span><br><span class="line">           &quot;domain_id&quot;:&quot;8f508b3360814db4ae9397996299ba7e&quot;,</span><br><span class="line">           &quot;password&quot;:&quot;000000&quot;,</span><br><span class="line">           &quot;name&quot;:&quot;chinaskill&quot;,</span><br><span class="line">           &quot;description&quot;:&quot;API create user!&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">    re = requests.post(url,headers=headers,data=json.dumps(body)).json()</span><br><span class="line">create_user()</span><br><span class="line">print(&quot;用户创建成功&quot;)</span><br></pre></td></tr></table></figure>

<p>【题目 2】使用 python 调用 api 实现创建 flavor[2 分]<br>在自行搭建的 OpenStack 私有云平台或提供的 all-in-one 平台上。在 controller 节点的&#x2F;root目录下创建 create_flavor.py 文件，在该文件中编写 python 代码对接 openstack api，要求在openstack 私有云平台上创建一个云主机类型，名字为 pvm_flavor、vcpu 为 1 个、内存为1024m、硬盘为 20G、ID 为 9999。执行完代码要求输出“云主机类型创建成功”。根据上述要求编写 python 代码，完成后，将 controller 节点的 IP 地址，用户名和密码提交。（考试系统会连接到你的 controller 节点，去执行 python 脚本，请准备好运行的环境，以便考试系统访问）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_flavor.py </span><br><span class="line">#coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">body = &#123;</span><br><span class="line">          &quot;auth&quot;:&#123;</span><br><span class="line">            &quot;identity&quot;:&#123;</span><br><span class="line">               &quot;methods&quot;:[&quot;password&quot;],</span><br><span class="line">               &quot;password&quot;:&#123;</span><br><span class="line">                  &quot;user&quot;:&#123;</span><br><span class="line">                    &quot;id&quot;:&quot;179fac1341f64033800765d4be2f09c2&quot;,</span><br><span class="line">                    &quot;password&quot;:&quot;000000&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;,</span><br><span class="line">          &quot;scope&quot;:&#123;</span><br><span class="line">            &quot;project&quot;:&#123;</span><br><span class="line">               &quot;id&quot;:&quot;ddc482a7bfb64a078f92260296ea723c&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;&#125;</span><br><span class="line">def get_token():</span><br><span class="line">    url=&quot;http://192.168.20.113:5000/v3/auth/tokens&quot;</span><br><span class="line">    re = requests.post(url,headers=headers,data=json.dumps(body)).headers[&quot;X-Subject-Token&quot;]</span><br><span class="line">    return re</span><br><span class="line">def create_flavor():</span><br><span class="line">    url=&quot;http://192.168.20.113:8774/v2.1/flavors&quot;</span><br><span class="line">    headers[&quot;X-Auth-Token&quot;]=get_token()</span><br><span class="line">    body=&#123;</span><br><span class="line">       &quot;flavor&quot;:&#123;</span><br><span class="line">          &quot;name&quot;:&quot;pvm_flavor&quot;,</span><br><span class="line">          &quot;id&quot;:9999,</span><br><span class="line">          &quot;vcpus&quot;:1,</span><br><span class="line">          &quot;ram&quot;:1024,</span><br><span class="line">          &quot;disk&quot;:20</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">    re = requests.post(url,headers=headers,data=json.dumps(body)).json()</span><br><span class="line">create_flavor()</span><br><span class="line">print(&#x27;云主机类型创建成功&#x27;)</span><br></pre></td></tr></table></figure>

<p>【题目 3】使用 python 调用 api 实现创建网络[2 分]<br>在自行搭建的 OpenStack 私有云平台或提供的 all-in-one 平台上。在 controller 节点的&#x2F;root目录下创建 create_network.py 文件，编写 python 代码对接 OpenStack API，完成网络的创建。要求：（1）为平台创建内部网络 pvm_int，子网名称为 pvm_intsubnet;（2）设置云主机网络子网 IP 网段为 192.168.x.0&#x2F;24（其中 x 是考位号），网关为 192.168.x.1（如果存在同名内网，代码中需先进行删除操作）。执行完代码要求输出“网络创建成功”。根据上述要求编写 python 代码，完成后，将 controller 节点的 IP 地址，用户名和密码提交。（考试系统会连接到你的 controller 节点，去执行 python 脚本，请准备好运行的环境，以便考试系统访问）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat net.py </span><br><span class="line">#coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">url = &quot;http://192.168.20.113&quot;</span><br><span class="line">body=&#123;</span><br><span class="line">   &quot;auth&quot;:&#123;</span><br><span class="line">      &quot;identity&quot;:&#123;</span><br><span class="line">         &quot;methods&quot;:[&quot;password&quot;],</span><br><span class="line">         &quot;password&quot;:&#123;</span><br><span class="line">            &quot;user&quot;:&#123;</span><br><span class="line">               &quot;id&quot;:&quot;52c7c503e67b45fa9116baa4d302edad&quot;,</span><br><span class="line">               &quot;password&quot;:&quot;000000&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;,</span><br><span class="line">    &quot;scope&quot;:&#123;</span><br><span class="line">       &quot;project&quot;:&#123;</span><br><span class="line">           &quot;id&quot;:&quot;01622847ee724c3ba59b94b5a1afa4d0&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">headers=&#123;&#125;</span><br><span class="line">def get_token():</span><br><span class="line">    tokenurl=url+&quot;:5000/v3/auth/tokens&quot;</span><br><span class="line">    re=requests.post(url=tokenurl,headers=headers,data=json.dumps(body)).headers[&quot;X-Subject-Token&quot;]</span><br><span class="line">    return re</span><br><span class="line">def net_create():</span><br><span class="line">    neturl=url+&quot;:9696/v2.0/networks&quot;</span><br><span class="line">    suburl=url+&quot;:9696/v2.0/subnets&quot;</span><br><span class="line">    headers[&quot;X-Auth-Token&quot;]=get_token()</span><br><span class="line">    body=&#123;</span><br><span class="line">       &quot;network&quot;: &#123;</span><br><span class="line">            &quot;provider:physical_network&quot;: &quot;provider&quot;, #可以不写</span><br><span class="line">            &quot;project_id&quot;: &quot;01622847ee724c3ba59b94b5a1afa4d0&quot;, #可以不写</span><br><span class="line">            &quot;name&quot;: &quot;pvm_int&quot;,</span><br><span class="line">            &quot;admin_state_up&quot;: &quot;true&quot;, #可以不写</span><br><span class="line">            &quot;provider:network_type&quot;: &quot;vlan&quot; #可以不写</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line">    re=requests.post(url=neturl,headers=headers,data=json.dumps(body)).json()</span><br><span class="line">    print(re)</span><br><span class="line">    re=re.get(&#x27;network&#x27;)</span><br><span class="line">    id=re[&#x27;id&#x27;]</span><br><span class="line">    body1=&#123;</span><br><span class="line">       &quot;subnet&quot;: &#123;</span><br><span class="line">            &quot;enable_dhcp&quot;: &quot;true&quot;,</span><br><span class="line">            &quot;network_id&quot;: id,</span><br><span class="line">            &quot;gateway_ip&quot;: &quot;192.168.23.1&quot;,</span><br><span class="line">            &quot;allocation_pools&quot;: [</span><br><span class="line">             &#123;</span><br><span class="line">                &quot;start&quot;: &quot;192.168.23.10&quot;,</span><br><span class="line">                &quot;end&quot;: &quot;192.168.23.100&quot;</span><br><span class="line">             &#125;</span><br><span class="line">            ],</span><br><span class="line">            &quot;cidr&quot;: &quot;192.168.23.0/24&quot;,</span><br><span class="line">            &quot;project_id&quot;: &quot;01622847ee724c3ba59b94b5a1afa4d0&quot;,</span><br><span class="line">            &quot;name&quot;: &quot;pvm_intsubnet&quot;,</span><br><span class="line">            &quot;ip_version&quot;: 4</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">    re1=requests.post(url=suburl,headers=headers,data=json.dumps(body1)).json()  </span><br><span class="line">    print(re1)</span><br><span class="line">net_create()</span><br><span class="line">print(&quot;网络创建成功&quot;)</span><br></pre></td></tr></table></figure>

<p>【题目 4】使用 python 调用 api 实现创建镜像[2 分]<br>在自行搭建的 OpenStack 私有云平台或提供的 all-in-one 平台上。在 controller 节点的&#x2F;root目录下创建 create_image.py 文件，编写 python 代码对接 OpenStack API，完成镜像的上传。要求在 OpenStack 私有云平台中上传镜像 cirros-0.3.4-x86_64-disk.img，名字为 pvm_image，disk_format 为 qcow2，container_format 为 bare。执行完代码要求输出“镜像创建成功，id为:xxxxxx”。根据上述要求编写 python 代码，完成后，将 controller 节点的 IP 地址，用户名和密码提交。（考试系统会连接到你的 controller 节点，去执行 python 脚本，请准备好运行的环境，以便考试系统访问）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_image.py </span><br><span class="line">#coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">url = &quot;http://192.168.20.113&quot;</span><br><span class="line">body=&#123;</span><br><span class="line">   &quot;auth&quot;:&#123;</span><br><span class="line">      &quot;identity&quot;:&#123;</span><br><span class="line">         &quot;methods&quot;:[&quot;password&quot;],</span><br><span class="line">         &quot;password&quot;:&#123;</span><br><span class="line">            &quot;user&quot;:&#123;</span><br><span class="line">               &quot;id&quot;:&quot;52c7c503e67b45fa9116baa4d302edad&quot;,</span><br><span class="line">               &quot;password&quot;:&quot;000000&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;,</span><br><span class="line">    &quot;scope&quot;:&#123;</span><br><span class="line">       &quot;project&quot;:&#123;</span><br><span class="line">           &quot;id&quot;:&quot;01622847ee724c3ba59b94b5a1afa4d0&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">headers=&#123;&#125;</span><br><span class="line">def get_token():</span><br><span class="line">    tokenurl=url+&quot;:5000/v3/auth/tokens&quot;</span><br><span class="line">    re=requests.post(url=tokenurl,headers=headers,data=json.dumps(body)).headers[&quot;X-Subject-Token&quot;]</span><br><span class="line">    return re</span><br><span class="line"></span><br><span class="line">def create_image():</span><br><span class="line">    glanceurl=url+&quot;:9292/v2/images&quot;</span><br><span class="line">    headers[&quot;X-Auth-Token&quot;]=get_token()</span><br><span class="line">    body1=&#123;</span><br><span class="line">             &quot;name&quot;: &quot;pvm_image&quot;,</span><br><span class="line">             &quot;disk_format&quot;: &quot;qcow2&quot;,</span><br><span class="line">             &quot;container_format&quot;: &quot;bare&quot;</span><br><span class="line">&#125;</span><br><span class="line">    re=requests.post(url=glanceurl,headers=headers,data=json.dumps(body1)).json()</span><br><span class="line">    print(re)</span><br><span class="line">    id=re[&#x27;id&#x27;]    </span><br><span class="line">    return id   </span><br><span class="line">def put_image():</span><br><span class="line">    glanceurl=url+&quot;:9292/v2/images&quot;</span><br><span class="line">    image_id=create_image()</span><br><span class="line">    headers[&quot;Content-Type&quot;]=&quot;application/octet-stream&quot;</span><br><span class="line">    a=open(&quot;cirros-0.3.4-x86_64-disk.img&quot;,&quot;rb&quot;).read()</span><br><span class="line">    req=requests.put(&quot;http://192.168.20.113:9292/v2/images/&#123;&#125;/file&quot;.format(image_id),data=a,headers=headers)</span><br><span class="line">    print(&quot;镜像上传成功,id为:&quot;+image_id)</span><br><span class="line">put_image()</span><br></pre></td></tr></table></figure>

<p>【题目 5】使用 python 调用 api 实现创建云主机[2 分]<br>在自行搭建的 OpenStack 私有云平台或提供的 all-in-one 平台上。在 controller 节点的&#x2F;root目录下创建 create_vm.py 文件，编写 python 代码对接 OpenStack API，完成云主机的创建。要求使用 pvm_image、pvm_flavor、pvm_intsubnet 创建 1 台云主机 pvm1（如果存在同名虚拟主机，代码中需先进行删除操作）。执行完代码要求输出“创建云主机成功”。根据上述要求编写 python 代码，完成后，将 controller 节点的 IP 地址，用户名和密码提交。（考试系统会连接到你的 controller 节点，去执行 python 脚本，请准备好运行的环境，以便考试系统访问）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_server.py </span><br><span class="line">#coding=utf-8</span><br><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line">url = &quot;http://192.168.20.113&quot;</span><br><span class="line">body=&#123;</span><br><span class="line">   &quot;auth&quot;:&#123;</span><br><span class="line">      &quot;identity&quot;:&#123;</span><br><span class="line">         &quot;methods&quot;:[&quot;password&quot;],</span><br><span class="line">         &quot;password&quot;:&#123;</span><br><span class="line">            &quot;user&quot;:&#123;</span><br><span class="line">               &quot;id&quot;:&quot;52c7c503e67b45fa9116baa4d302edad&quot;,</span><br><span class="line">               &quot;password&quot;:&quot;000000&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;,</span><br><span class="line">    &quot;scope&quot;:&#123;</span><br><span class="line">       &quot;project&quot;:&#123;</span><br><span class="line">           &quot;id&quot;:&quot;01622847ee724c3ba59b94b5a1afa4d0&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">headers=&#123;&#125;</span><br><span class="line">def get_token():</span><br><span class="line">    tokenurl=url+&quot;:5000/v3/auth/tokens&quot;</span><br><span class="line">    re=requests.post(url=tokenurl,headers=headers,data=json.dumps(body)).headers[&quot;X-Subject-Token&quot;]</span><br><span class="line">    return re</span><br><span class="line">def create_server():</span><br><span class="line">    svurl=url+&quot;:8774/v2.1/servers&quot;</span><br><span class="line">    headers[&quot;X-Auth-Token&quot;]=get_token()</span><br><span class="line">    body=&#123;</span><br><span class="line">       &quot;server&quot;:&#123;</span><br><span class="line">           &quot;name&quot;: &quot;pvm1&quot;,</span><br><span class="line">           &quot;imageRef&quot;: &quot;b2da2db6-ede4-4007-8778-e2da9a11223f&quot;,</span><br><span class="line">           &quot;flavorRef&quot;: &quot;9999&quot;,</span><br><span class="line">           &quot;networks&quot;: [&#123;&quot;uuid&quot;:&quot;65b2bd8b-3d99-47ed-ba3f-31864ae77b19&quot;&#125;]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">    re=requests.post(url=svurl,headers=headers,data=json.dumps(body)).json()</span><br><span class="line">    print(re)</span><br><span class="line">create_server()</span><br><span class="line">print(&#x27;云主机创建成功&#x27;)</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-容器云-任务1</title>
    <url>/2022/03/31/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A11/</url>
    <content><![CDATA[<h1 id="云计算赛项第二场-容器云-任务1-江苏卷"><a href="#云计算赛项第二场-容器云-任务1-江苏卷" class="headerlink" title="云计算赛项第二场-容器云-任务1- 江苏卷"></a>云计算赛项第二场-容器云-任务1- 江苏卷</h1><h2 id="【任务1】Docker-CE-及私有仓库安装任务-5-分"><a href="#【任务1】Docker-CE-及私有仓库安装任务-5-分" class="headerlink" title="【任务1】Docker CE 及私有仓库安装任务[5 分]"></a>【任务1】Docker CE 及私有仓库安装任务[5 分]<span id="more"></span></h2><p><strong>【适用平台】私有云</strong></p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p><strong>master</strong></p>
<p><code>hostnamectl set-hostname master</code></p>
<p><code>bash</code></p>
<p><strong>node</strong></p>
<p><code>hostnamectl set-hostname node</code></p>
<p><code>bash</code></p>
<p>将<a href="http://172.19.25.11/iso/chinaskills_cloud_paas.iso%E4%B8%8B%E8%BD%BD%E5%88%B0%60master%60%E4%B8%BB%E6%9C%BA/opt%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B">http://172.19.25.11/iso/chinaskills_cloud_paas.iso下载到`master`主机/opt文件夹下</a></p>
<p><code>cd /opt</code></p>
<p><code>curl http://172.19.25.11/iso/chinaskills_cloud_paas.iso</code></p>
<p>将镜像挂载到&#x2F;mnt文件夹下</p>
<p><code>mount chinaskills_cloud_paas.iso /mnt</code></p>
<p>拷贝到&#x2F;opt文件夹下</p>
<p><code>cp -rfv /mnt/* /opt</code></p>
<p>取消挂载</p>
<p><code>umount /mnt</code></p>
<p>将<a href="http://172.19.25.11/iso/CentOS-7-x86_64-DVD-1804.iso%E4%B8%8B%E8%BD%BD%E5%88%B0%60master%60%E4%B8%BB%E6%9C%BA/opt%E6%96%87%E4%BB%B6%E5%A4%B9%E4%B8%8B">http://172.19.25.11/iso/CentOS-7-x86_64-DVD-1804.iso下载到`master`主机/opt文件夹下</a></p>
<p><code>cd /opt</code></p>
<p><code>curl -O http://172.19.25.11/iso/CentOS-7-x86_64-DVD-1804.iso</code></p>
<p>将镜像挂载到&#x2F;mnt文件夹下</p>
<p><code>mount CentOS-7-x86_64-DVD-1804.iso /mnt</code></p>
<p>拷贝到&#x2F;opt文件夹下</p>
<p><code>cp -rfv /mnt/* /opt</code></p>
<p>取消挂载</p>
<p><code>umount /mnt</code></p>
<p>将yum.repo.d下面的源全部移出</p>
<p><code>mv /etc/yum.repos.d/* /tmp</code></p>
<p>新建local.repo源</p>
<p><code>vi /etc/yum.repos.d/ local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[docker]</span><br><span class="line">baseurl=file:///opt/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[centos]</span><br><span class="line">baseurl=file:///opt/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><code>yum clean</code></p>
<p><code>yum list</code></p>
<p><strong>配置时间同步</strong></p>
<p>在master 节点上部署chrony 服务器，允许其他节点同步时间，启动服务并设置为开机启动；在node节点上指定master节点为上游NTP 服务器，重启服务并设为开机启动。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><code>yum install -y chrony</code></p>
<p><strong>配置master节点</strong></p>
<p><code>vi /etc/chrony.conf</code></p>
<p>删除默认规则（在默认规则前加上#号）</p>
<p>添加以下规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server master iburst</span><br><span class="line"></span><br><span class="line">allow 192.168.20.0/24</span><br><span class="line"></span><br><span class="line">local stratum 10</span><br></pre></td></tr></table></figure>

<p><strong>配置node节点</strong></p>
<p><code>vi /etc/chrony.conf</code></p>
<p>删除默认规则（在默认规则前加上#号）</p>
<p>添加以下规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server master iburst</span><br></pre></td></tr></table></figure>

<p>启动服务并设置为开机启动</p>
<p><code>systemctl restart chronyd</code></p>
<p><code>systemctl enable chronyd</code></p>
<p><strong>修改路由转发</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">modprobe br_netfilter</span><br><span class="line">echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">echo &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure>

<h3 id="【题目1】安装Docker-CE-和Docker-Compose-1-分"><a href="#【题目1】安装Docker-CE-和Docker-Compose-1-分" class="headerlink" title="【题目1】安装Docker CE 和Docker Compose[1 分]"></a>【题目1】安装Docker CE 和Docker Compose[1 分]</h3><p>使用提供的centos7.5-paas 镜像启动两台云主机master 和node，flavor 如上表所示。在master、node 各节点中分别安装DockerCE 和docker-compose。完成后提交master 节点的用户名、密码和IP 到答题框。</p>
<p><strong>安装docker</strong></p>
<p><code>yum install -y yum-u* device-mapper-p* lvm2</code></p>
<p><code>yum install -y docker-ce</code></p>
<p><code>systemctl restart docker</code></p>
<p><code>systemctl enable docker</code></p>
<p><strong>安装docker-compose</strong></p>
<p><code>cp -rfv /opt/docker-compose/v1.25.5-docker-compose-Linux-x86_64 /usr/local/bin/docker-compose</code></p>
<p><strong>检查docker版本</strong></p>
<p><code>docker -v</code></p>
<p><code>docker-compose version</code></p>
<p><strong>修改 Docker Cgroup Driver为systemd</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;insecure-registries&quot; : [&quot;0.0.0.0/0&quot;],</span><br><span class="line">&quot;registry-mirrors&quot;: [&quot;https://5twf62k1.mirror.aliyuncs.com&quot;], </span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure>



<h3 id="【题目2】安装私有仓库-1-分"><a href="#【题目2】安装私有仓库-1-分" class="headerlink" title="【题目2】安装私有仓库[1 分]"></a>【题目2】安装私有仓库[1 分]</h3><p>在master 节点安装私有仓库，导入&#x2F;opt&#x2F;images 目录下所有镜像，并推送到私有仓库。完成后提交master 节点的用户名、密码和IP 到答题框</p>
<p><strong>导入镜像</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for i in $(ls /opt/images|grep tar)</span><br><span class="line">do</span><br><span class="line">  docker load -i /opt/images/$i</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<p><strong>安装Harbor仓库</strong></p>
<p><code>cd  /opt</code></p>
<p><code>cd harbor</code></p>
<p><code>tar -zxvf harbor-offline-installer-v2.1.0.tgz</code></p>
<p><code>cd harbor</code></p>
<p><code>cp harbor.yml.tmpl harbor.yml</code></p>
<p> <code>vi harbor.yml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">hostname: 192.168.20.123 # 将域名修改为本机IP</span><br><span class="line"></span><br><span class="line">harbor_admin_password: Harbor12345</span><br><span class="line"></span><br><span class="line">#https: # 禁用https</span><br><span class="line"></span><br><span class="line"> # https port for harbor, default is 443</span><br><span class="line"></span><br><span class="line"> # port: 443</span><br><span class="line"></span><br><span class="line"> # The path of cert and key files for nginx</span><br><span class="line"></span><br><span class="line"> # certificate: /your/certificate/path</span><br><span class="line"></span><br><span class="line"> # private_key: /your/private/key/path</span><br></pre></td></tr></table></figure>

<p><code>./prepare</code></p>
<p><code>./install.sh --with-clair</code></p>
<p><strong>将镜像导入Harbor仓库：</strong></p>
<p> <code>./k8s_image_push.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入镜像仓库地址(不加http/https): 192.168.20.123</span><br><span class="line"></span><br><span class="line">输入镜像仓库用户名: admin</span><br><span class="line"></span><br><span class="line">输入镜像仓库用户密码: Harbor12345</span><br><span class="line"></span><br><span class="line">您设置的仓库地址为: 192.168.20.123,用户名: admin,密码: xxx</span><br><span class="line"></span><br><span class="line">是否确认(Y/N): Y</span><br></pre></td></tr></table></figure>

<h3 id="【题目3】容器编排-3-分"><a href="#【题目3】容器编排-3-分" class="headerlink" title="【题目3】容器编排[3 分]"></a>【题目3】容器编排[3 分]</h3><p>在master 节点上编写&#x2F;root&#x2F;docker-compose.yaml 文件，具体要求如下：<br>（1）容器1 名称：wordpress；镜像：wordpress:latest；端口映射：82:80；<br>（2）容器 2 名称：mysql；镜像：mysql:5.6;<br>（3）MySQL root用户密码：123456；</p>
<p>  (4)创建数据库wordpress。</p>
<p>完成后编排部署Wordpress。</p>
<p>查看本地镜像中是否有wordprees和mysql</p>
<p><code>docker images</code></p>
<p>若没有则</p>
<p><code>docker pull wordpress:latest</code></p>
<p><code>docker pull mysql:5.6</code></p>
<p>这样就可以继续了</p>
<p><code>cd /root</code></p>
<p><code>vi docker-compose.yaml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3.3&#x27;</span><br><span class="line">services:</span><br><span class="line">   mysql:</span><br><span class="line">     image: mysql:5.6</span><br><span class="line">     restart: always</span><br><span class="line">     environment:</span><br><span class="line">       MYSQL_ROOT_PASSWORD: 123456</span><br><span class="line">       MYSQL_DATABASE: wordpress</span><br><span class="line">       MYSQL_USER: wordpress</span><br><span class="line">       MYSQL_PASSWORD: wordpress</span><br><span class="line">   wordpress:</span><br><span class="line">     depends_on:</span><br><span class="line">       - mysql</span><br><span class="line">     image: wordpress:latest</span><br><span class="line">     ports:</span><br><span class="line">       - &quot;82:80&quot;</span><br><span class="line">     restart: always</span><br><span class="line">     environment:</span><br><span class="line">       WORDPRESS_DB_HOST: mysql:3306</span><br><span class="line">       WORDPRESS_DB_USER: wordpress</span><br><span class="line">       WORDPRESS_DB_PASSWORD: wordpress</span><br><span class="line">       WORDPRESS_DB_NAME: wordpress</span><br></pre></td></tr></table></figure>

<p>开始部署</p>
<p><code>docker-compose up -d</code></p>
<p>查看是否部署成功</p>
<p><code>docker ps</code></p>
<p>打开浏览器访问192.168.20.132:82</p>
]]></content>
      <tags>
        <tag>容器云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-2022样题-私有云</title>
    <url>/2022/04/25/%E4%BA%91%E8%AE%A1%E7%AE%97-2022%E6%A0%B7%E9%A2%98-%E7%A7%81%E6%9C%89%E4%BA%91/</url>
    <content><![CDATA[<h1 id="云计算赛项赛卷1"><a href="#云计算赛项赛卷1" class="headerlink" title="云计算赛项赛卷1"></a>云计算赛项赛卷1</h1><h2 id="OpenStack云平台运维"><a href="#OpenStack云平台运维" class="headerlink" title="OpenStack云平台运维"></a>OpenStack云平台运维</h2><p><strong>1.使用提供的云安全框架组件，将提供的OpenStack云平台的安全策略从http优化至https。</strong><span id="more"></span></p>
<p>使用提供的云安全框架组件，将自行搭建的OpenStack云平台的安全策略从http优化至https。<br><code>yum -y install mod_ssl</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/openstack-dashboard/local_settings</span><br><span class="line">##在DEBUG = False下增加4行</span><br><span class="line">USE_SSL = True</span><br><span class="line">CSRF_COOKIE_SECURE = True                              ##原文中有，去掉注释即可</span><br><span class="line">SESSION_COOKIE_SECURE = True                       ##原文中有，去掉注释即可</span><br><span class="line">SESSION_COOKIE_HTTPONLY = True</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/httpd/conf.d/ssl.conf</span><br><span class="line">##将SSLProtocol all -SSLv2 -SSLv3改成：</span><br><span class="line">SSLProtocol all -SSLv2</span><br></pre></td></tr></table></figure>

<p> <code>systemctl restart httpd</code></p>
<p><code>systemctl restart memcached</code></p>
<p><strong>2.在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：</strong></p>
<p>（1）设置内存超售比例为1.5倍；</p>
<p>（2）设置为120秒。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf</span><br><span class="line">ram_allocation_ratio=1.5 ##设置内存超售比例为1.5倍</span><br><span class="line">service_down_time = 120  ##设置nova服务心跳检查时间为120秒</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart *nova*</code></p>
<p><strong>3.在提供的OpenStack平台上，使用Swift对象存储服务，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储</strong></p>
<p> <code>vi /etc/glance/glance-api.conf</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[glance_store]</span><br><span class="line">#stores = file,http</span><br><span class="line">#default_store = file</span><br><span class="line">#filesystem_store_datadir = /var/lib/glance/images/</span><br><span class="line">default_store=swift</span><br><span class="line">stores=glance.store.swift.Store</span><br><span class="line">swift_store_auth_address=http://controller:5000/v3.0</span><br><span class="line">swift_store_endpoint_type=internalURL</span><br><span class="line">swift_store_multi_tenant=True</span><br><span class="line">swift_store_admin_tenants=service</span><br><span class="line">swift_store_user=glance</span><br><span class="line">swift_store_key=000000</span><br><span class="line">swift_store_container=glance</span><br><span class="line">swift_store_create_container_on_put=True</span><br></pre></td></tr></table></figure>

<p><strong>4.在提供的OpenStack平台上，编写heat模板createvm.yml文件，模板作用为按照要求创建一个云主机。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat createvm.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  server:</span><br><span class="line">    type: OS::Nova::Server</span><br><span class="line">    properties:</span><br><span class="line">      name: &quot;testvm1&quot;</span><br><span class="line">      image: cirros</span><br><span class="line">      flavor: m1.flavor</span><br><span class="line">      networks:</span><br><span class="line">        - network: extnet</span><br></pre></td></tr></table></figure>

<p><strong>5.在提供的OpenStack平台上，对cinder存储空间进行扩容操作，要求将cinder存储空间扩容10G。</strong></p>
<p><strong>创建一块新分区（vdb3）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# fdisk /dev/vdb</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (2 primary, 0 extended, 2 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (3,4，默认 3)：</span><br><span class="line">起始 扇区 (83888128-104857599，默认为 83888128)：</span><br><span class="line">将使用默认值 83888128</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (83888128-104857599，默认为 104857599)：</span><br><span class="line">将使用默认值 104857599</span><br><span class="line">分区 3 已设置为 Linux 类型，大小设为 10 GiB</span><br><span class="line">命令(输入 m 获取帮助)：w!</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">WARNING: Re-reading the partition table failed with error 16: 设备或资源忙.</span><br><span class="line">The kernel still uses the old table. The new table will be used at</span><br><span class="line">the next reboot or after you run partprobe(8) or kpartx(8)</span><br><span class="line">正在同步磁盘。</span><br></pre></td></tr></table></figure>

<p><strong>重启一下</strong></p>
<p><code>reboot</code></p>
<p><strong>vgextend cinder-volumes &#x2F;dev&#x2F;vdb3 添加</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# vgextend cinder-volumes /dev/vdb3</span><br><span class="line">  Physical volume &quot;/dev/vdb3&quot; successfully created.</span><br><span class="line">  Volume group &quot;cinder-volumes&quot; successfully extended</span><br></pre></td></tr></table></figure>

<p><strong>vgdisplay  查看</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# vgdisplay</span><br><span class="line">  --- Volume group ---</span><br><span class="line">  VG Name               cinder-volumes</span><br><span class="line">  System ID             </span><br><span class="line">  Format                lvm2</span><br><span class="line">  Metadata Areas        2</span><br><span class="line">  Metadata Sequence No  9</span><br><span class="line">  VG Access             read/write</span><br><span class="line">  VG Status             resizable</span><br><span class="line">  MAX LV                0</span><br><span class="line">  Cur LV                2</span><br><span class="line">  Open LV               0</span><br><span class="line">  Max PV                0</span><br><span class="line">  Cur PV                2</span><br><span class="line">  Act PV                2</span><br><span class="line">  VG Size               29.99 GiB</span><br><span class="line">  PE Size               4.00 MiB</span><br><span class="line">  Total PE              7678</span><br><span class="line">  Alloc PE / Size       4874 / &lt;19.04 GiB</span><br><span class="line">  Free  PE / Size       2804 / 10.95 GiB</span><br><span class="line">  VG UUID               LOomGq-bcsf-Lqpe-sgmi-hJU3-H7rk-8DHfz2</span><br></pre></td></tr></table></figure>

<p><strong>6.在OpenStack私有云平台，创建一台云主机，使用提供的软件包，编写一键部署脚本，要求可以一键部署gpmall商城应用系统。</strong></p>
<p><strong>7.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。</strong></p>
<p><strong>8.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>9.使用cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。</strong></p>
<p><strong>10.使用提供的iaas-error1镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为cinder服务无法正常使用，请结合报错信息排查错误，使cinder服务可以正常使用。</strong></p>
<h1 id="云计算赛项赛卷2"><a href="#云计算赛项赛卷2" class="headerlink" title="云计算赛项赛卷2"></a>云计算赛项赛卷2</h1><p><strong>1.使用自动搭建的OpenStack平台，登录数据库，创建库test，并在库test中创建表company（表结构如(id int not null primary key,name varchar(50),addr varchar(255))所示），在表company中插入一条数据(1,”alibaba”,”china”)</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; create database test;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">MariaDB [(none)]&gt; use test;</span><br><span class="line">Database changed</span><br><span class="line">MariaDB [test]&gt; create table company(</span><br><span class="line">    -&gt; id int not null primary key,</span><br><span class="line">    -&gt; name varchar(50),</span><br><span class="line">    -&gt; addr varchar(255));</span><br><span class="line">Query OK, 0 rows affected (0.15 sec)</span><br><span class="line">MariaDB [test]&gt; insert into company values(1,&quot;alibaba&quot;,&quot;chinas&quot;);</span><br><span class="line">Query OK, 1 row affected (0.02 sec)</span><br></pre></td></tr></table></figure>

<p><strong>2.OpenStack各服务内部通信都是通过RPC来交互，各agent都需要去连接RabbitMQ；随着各服务agent增多，MQ的连接数会随之增多，最终可能会到达上限，成为瓶颈。使用提供的Ope nStack私有云平台，通过修改limits.conf配置文件来修改RabbitMQ服务的最大连接数为10240</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller /]# cat /etc/security/limits.conf </span><br><span class="line">...........................................</span><br><span class="line">#&lt;domain&gt;      &lt;type&gt;  &lt;item&gt;         &lt;value&gt;</span><br><span class="line">#</span><br><span class="line">#*               soft    core            0</span><br><span class="line">#*               hard    rss             10000</span><br><span class="line">#@student        hard    nproc           20</span><br><span class="line">#@faculty        soft    nproc           20</span><br><span class="line">#@faculty        hard    nproc           50</span><br><span class="line">#ftp             hard    nproc           0</span><br><span class="line">#@student        -       maxlogins       4</span><br><span class="line">user             soft    nofile          10240</span><br><span class="line">user             hard    nofile          10240</span><br><span class="line"># End of file</span><br></pre></td></tr></table></figure>

<p><strong>3.在提供的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板create_user.yaml，创建名为heat-user的用户，属于admin项目，并赋予heat-user用户admin的权限，配置用户密码为123456。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_user.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  user:</span><br><span class="line">    type: OS::Keystone::User</span><br><span class="line">    properties:</span><br><span class="line">      name: heat-user</span><br><span class="line">      password: &quot;000000&quot;</span><br><span class="line">      domain: demo</span><br><span class="line">      default_project: admin</span><br><span class="line">      roles: [&#123;&quot;role&quot;:&quot;admin&quot;,&quot;project&quot;:&quot;admin&quot;&#125;]</span><br></pre></td></tr></table></figure>

<p><strong>4.在提供的OpenStack私有云平台上，使用cirros-0.3.4-x86_64-disk.img镜像，创建一个名为Gmirror1的镜像，要求启动该镜像的最小硬盘是30G、最小内存是2048M。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image create Gmirror1 --disk-format qcow2 --container-format bare --min-disk 30 --min-ram 2048 &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| Field            | Value                                                |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     |</span><br><span class="line">| container_format | bare                                                 |</span><br><span class="line">| created_at       | 2022-05-03T00:34:26Z                                 |</span><br><span class="line">| disk_format      | qcow2                                                |</span><br><span class="line">| file             | /v2/images/1a1ef251-a1d6-4f53-87aa-beef8fae8e64/file |</span><br><span class="line">| id               | 1a1ef251-a1d6-4f53-87aa-beef8fae8e64                 |</span><br><span class="line">| min_disk         | 30                                                   |</span><br><span class="line">| min_ram          | 2048                                                 |</span><br><span class="line">| name             | Gmirror1                                             |</span><br><span class="line">| owner            | f9ef9bd849374360807c1fa567b44862                     |</span><br><span class="line">| protected        | False                                                |</span><br><span class="line">| schema           | /v2/schemas/image                                    |</span><br><span class="line">| size             | 13287936                                             |</span><br><span class="line">| status           | active                                               |</span><br><span class="line">| tags             |                                                      |</span><br><span class="line">| updated_at       | 2022-05-03T00:34:27Z                                 |</span><br><span class="line">| virtual_size     | None                                                 |</span><br><span class="line">| visibility       | shared                                               |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>5.在提供的OpenStack私有云平台上，自行安装Swift服务，新建名为chinaskill的容器，将cirros-0.3.4-x86_64-disk.img镜像上传到chinaskill容器中，并设置分段存放，每一段大小为10M。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# swift post chinaskill</span><br><span class="line">[root@controller ~]# swift list</span><br><span class="line">chinaskill</span><br><span class="line">[root@controller ~]# swift upload chinaskill -S 10000000 cirros-0.3.4-x86_64-disk.img </span><br><span class="line">cirros-0.3.4-x86_64-disk.img segment 0</span><br><span class="line">cirros-0.3.4-x86_64-disk.img segment 1</span><br><span class="line">cirros-0.3.4-x86_64-disk.img</span><br></pre></td></tr></table></figure>

<p><strong>6.使用OpenStack私有云平台，创建两台云主机vm1和vm2，在这两台云主机上分别安装数据库服务，并配置成主从数据库，vm1节点为主库，vm2节点为从库（数据库密码设置为000000）。</strong></p>
<p><strong>（1）修改主机名</strong></p>
<p><strong>mysql1</strong></p>
<p><code>hostnamectl set-hostname mysql1</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>mysql2</strong></p>
<p><code>hostnamectl set-hostname mysql2</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>（2）配置hosts文件</strong></p>
<p>两个节点配置&#x2F;etc&#x2F;hosts文件，修改为如下</p>
<p><code>vi /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">192.168.20.107  mysql1</span><br><span class="line">192.168.20.109  mysql2</span><br><span class="line">192.168.20.137  mycat</span><br></pre></td></tr></table></figure>

<p><strong>（3）配置YUM源</strong></p>
<p>两个节点均使用提供的mariadb–10.3.23-repo.tar.gz的压缩包，解压并放在&#x2F;opt目录下，进入&#x2F;etc&#x2F;yum.repos.d目录下，将原来的repo文件移除，新建local.repo文件并编辑内容，具体操作命令如下：</p>
<p><code>curl -O http://172.19.25.11/mariadb-10.3.23-repo.tar.gz</code></p>
<p> <code>tar -zxvf mariadb-10.3.23-repo.tar.gz -C /opt</code></p>
<p><code>rm -rf /etc/yum.repos.d/*</code></p>
<p><code>vi /etc/yum.repos.d/local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line">name=mariadb</span><br><span class="line">baseurl=file:///opt/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><strong>（4）安装数据库服务并启动</strong></p>
<p>配置完毕后，两个节点安装数据库服务，命令如下：</p>
<p><code>yum install -y mariadb mariadb-server</code></p>
<p><code>systemctl start mariadb</code></p>
<p><code>systemctl enable mariadb</code></p>
<p><strong>(5）初始化数据库</strong></p>
<p>两个节点初始化数据库，配置数据库root密码为000000，命令如下：</p>
<p><code>mysql_secure_installation</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/bin/mysql_secure_installation: line 379: find_mysql_client: command not found</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB</span><br><span class="line"></span><br><span class="line">   SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!</span><br><span class="line"></span><br><span class="line">In order to log into MariaDB to secure it, we&#x27;ll need the current</span><br><span class="line">password for the root user. If you&#x27;ve just installed MariaDB, and</span><br><span class="line">you haven&#x27;t set the root password yet, the password will be blank,</span><br><span class="line">so you should just press enter here.</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Enter current password for root (enter for none):     #默认按Enter键</span><br><span class="line">OK, successfully used password, moving on...</span><br><span class="line"></span><br><span class="line">Setting the root password ensures that nobody can log into the MariaDB</span><br><span class="line">root user without the proper authorisation.</span><br><span class="line"></span><br><span class="line">Set root password? [Y/n] y</span><br><span class="line"></span><br><span class="line">New password:                 #输入数据库root密码000000</span><br><span class="line">Re-enter new password:             #再次输入密码000000</span><br><span class="line">Password updated successfully!</span><br><span class="line">Reloading privilege tables..</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">By default, a MariaDB installation has an anonymous user, allowing anyone</span><br><span class="line">to log into MariaDB without having to have a user account created for</span><br><span class="line">them. This is intended only for testing, and to make the installation</span><br><span class="line">go a bit smoother. You should remove them before moving into a</span><br><span class="line">production environment.</span><br><span class="line"></span><br><span class="line">Remove anonymous users? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"> </span><br><span class="line">Normally, root should only be allowed to connect from &#x27;localhost&#x27;. This</span><br><span class="line">ensures that someone cannot guess at the root password from the network.</span><br><span class="line"> </span><br><span class="line">Disallow root login remotely? [Y/n] n</span><br><span class="line"> ... skipping.</span><br><span class="line"></span><br><span class="line">By default, MariaDB comes with a database named &#x27;test&#x27; that anyone can</span><br><span class="line">access. This is also intended only for testing, and should be removed</span><br><span class="line">before moving into a production environment.</span><br><span class="line"></span><br><span class="line">Remove test database and access to it? [Y/n] y</span><br><span class="line"> \- Dropping test database...</span><br><span class="line"> ... Success!</span><br><span class="line"> \- Removing privileges on test database...</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">Reloading the privilege tables will ensure that all changes made so far</span><br><span class="line">will take effect immediately.</span><br><span class="line"></span><br><span class="line">Reload privilege tables now? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"> </span><br><span class="line">Cleaning up...</span><br><span class="line"></span><br><span class="line">All done! If you&#x27;ve completed all of the above steps, your MariaDB</span><br><span class="line">installation should now be secure.</span><br><span class="line"></span><br><span class="line">Thanks for using MariaDB!</span><br></pre></td></tr></table></figure>

<p><strong>（6）配置mysql1主节点</strong></p>
<p>修改mysql1节点的数据库配置文件，在配置文件&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf中的[mysqld]增添如下内容。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# cat /etc/my.cnf.d/server.cnf</span><br><span class="line">... ...</span><br><span class="line">[mysqld]</span><br><span class="line">log_bin = mysql-bin            #记录操作日志</span><br><span class="line">binlog_ignore_db = mysql         #不同步MySQL系统数据库</span><br><span class="line">server_id = 12              #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例如172.30.11.12，server_id就写12</span><br></pre></td></tr></table></figure>

<p>重启数据库服务，并进入数据库，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# systemctl restart mariadb</span><br><span class="line">[root@mysql1 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 9</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"> </span><br><span class="line">MariaDB [(none)]&gt; </span><br></pre></td></tr></table></figure>

<p>在mysql1节点，授权在任何客户端机器上可以以root用户登录到数据库，然后在主节点上创建一个user用户连接节点mysql2，并赋予从节点同步主节点数据库的权限。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; grant all privileges on *.* to root@&#x27;%&#x27; identified by &quot;000000&quot;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; grant replication slave on *.* to &#x27;user&#x27;@&#x27;mysql2&#x27; identified by &#x27;000000&#x27;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><strong>（7）配置mysql2从节点</strong></p>
<p>修改mysql2节点的数据库配置文件，在配置文件&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf中的[mysqld]增添如下内容。</p>
<p><code>[root@mysql2 ~]# cat /etc/my.cnf.d/server.cnf</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">... ...</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">log_bin = mysql-bin            #记录操作日志</span><br><span class="line">binlog_ignore_db = mysql         #不同步MySQL系统数据库</span><br><span class="line">server_id = 13              #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例如172.30.11.13，server_id就写13</span><br><span class="line"></span><br><span class="line">... ...</span><br></pre></td></tr></table></figure>

<p>修改完配置文件后，重启数据库服务，并在从节点mysql2上登录MariaDB数据库，配置从节点连接主节点的连接信息。master_host为主节点主机名mysql1，master_user为上一步中创建的用户user，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql2 ~]# systemctl restart mariadb</span><br><span class="line">[root@mysql2 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 9</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; change master to master_host=&#x27;mysql1&#x27;,master_user=&#x27;user&#x27;,master_password=&#x27;000000&#x27;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>

<p>配置完毕主从数据库之间的连接信息之后，开启从节点服务。使用show slave status\G命令，并查看从节点服务状态，如果Slave_IO_Running和Slave_SQL_Running的状态都为YES，则从节点服务开启成功。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; start slave;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show slave status\G</span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">        Slave_IO_State: Waiting for master to send event</span><br><span class="line">          Master_Host: mysql1</span><br><span class="line">          Master_User: user</span><br><span class="line">         Master_Port: 3306</span><br><span class="line">         Connect_Retry: 60</span><br><span class="line">........</span><br><span class="line">      Slave_IO_Running: Yes</span><br><span class="line">       Slave_SQL_Running: Yes</span><br><span class="line">.........</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到Slave_IO_Running和Slave_SQL_Running的状态都是Yes，配置数据库主从集群成功。</p>
<p><strong>（8）主节点创建数据库</strong></p>
<p>先在主节点mysql1中创建库test，并在库test中创建表company，插入表数据，创建完成后，查看表company数据，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# mysql -uroot -p000000</span><br><span class="line"></span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line"></span><br><span class="line">Your MariaDB connection id is 11</span><br><span class="line"></span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"> </span><br><span class="line">MariaDB [(none)]&gt; create database test;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">MariaDB [(none)]&gt; use test;</span><br><span class="line">Database changed</span><br><span class="line">MariaDB [test]&gt; create table company(id int not null primary key,name varchar(50),addr varchar(255));</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"> </span><br><span class="line">MariaDB [test]&gt; insert into company values(1,&quot;alibaba&quot;,&quot;china&quot;);</span><br><span class="line"></span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; select * from company;</span><br><span class="line"></span><br><span class="line">+----+---------+-------+</span><br><span class="line">| id | name  | addr |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| 1 | alibaba | china |</span><br><span class="line">+----+---------+-------+</span><br><span class="line"></span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><strong>（9）从节点验证复制功能</strong></p>
<p>登录mysql2节点的数据库，查看数据库列表。找到test数据库，查询表，并查询内容验证从数据库的复制功能，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql2 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 12</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line"> </span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"> </span><br><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database      |</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| information_schema |</span><br><span class="line">| mysql       |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test        |</span><br><span class="line">+--------------------+</span><br><span class="line">4 rows in set (0.00 sec) </span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; use test;</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line">Database changed</span><br><span class="line">MariaDB [test]&gt; show tables;</span><br><span class="line">+----------------+</span><br><span class="line">| Tables_in_test |</span><br><span class="line">+----------------+</span><br><span class="line">| company    |</span><br><span class="line">+----------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">MariaDB [test]&gt; select * from company;</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| id | name  | addr |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| 1 | alibaba | china |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>可以查看到主数据库中刚刚创建的库、表、信息，验证从数据库的复制功能成功。</p>
<p><strong>7.在OpenStack私有云平台，创建一台云主机，使用提供的软件包，编写一键部署脚本，要求可以一键部署owncloud云网盘应用系统。</strong></p>
<p><strong>8.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>9.使用cloudkitty计费服务，处理虚拟机实例(compute)、云硬盘(volume)、镜像(image)、网络进出流量(network.bw.in, network.bw.out)、浮动IP(network.floating)的计费数据并进行计费规则创建，以达到费用核算目的。</strong></p>
<p><strong>10.使用提供的iaas-error2镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为glance服务无法正常使用，请结合报错信息排查错误，使glance服务可以正常使用。</strong></p>
<h1 id="云计算赛项赛卷3"><a href="#云计算赛项赛卷3" class="headerlink" title="云计算赛项赛卷3"></a>云计算赛项赛卷3</h1><p><strong>1.使用提供的OpenStack私有云平台，修改相关配置文件，启用-device virtio-net-pci in kvm。</strong></p>
<p><strong>2.在提供的OpenStack平台上，通过修改相关参数对openstack平台进行调优操作，相应的调优操作有：</strong></p>
<p>（1）预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf</span><br><span class="line">vcpu_pin_set= 3-16</span><br></pre></td></tr></table></figure>

<p>（2）设置cpu超售比例为4倍；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf</span><br><span class="line">cpu_allocation_ratio=4.0</span><br></pre></td></tr></table></figure>

<p><strong>3.在提供的OpenStack平台上，对mencached服务进行操作使memcached的缓存由64MB变为256MB。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/sysconfig/memcached </span><br><span class="line">PORT=&quot;11211&quot;</span><br><span class="line">USER=&quot;memcached&quot;</span><br><span class="line">MAXCONN=&quot;1024&quot;</span><br><span class="line">CACHESIZE=&quot;256&quot; #缓存</span><br><span class="line">OPTIONS=&quot;-l 127.0.0.1,::1,controller&quot;</span><br></pre></td></tr></table></figure>

<p><strong>4.在提供的OpenStack平台上，编写heat模板createnet.yml文件，模板作用为按照要求创建一个网络和子网。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat createnet.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  network:</span><br><span class="line">    type: OS::Neutron::Net</span><br><span class="line">    properties:</span><br><span class="line">      name: &quot;extnet&quot;</span><br><span class="line">      shared: false</span><br><span class="line">      admin_state_up: true</span><br><span class="line">  subnet:</span><br><span class="line">    type: OS::Neutron::Subnet</span><br><span class="line">    properties:</span><br><span class="line">      cidr: 10.10.2.0/24</span><br><span class="line">      enable_dhcp: true</span><br><span class="line">      gateway_ip: 10.10.2.1</span><br><span class="line">      allocation_pools:</span><br><span class="line">        - start: 10.10.2.20</span><br><span class="line">          end: 10.10.2.100</span><br><span class="line">      name: &quot;subextnet&quot;</span><br><span class="line">      network_id: &#123;get_resource: &quot;network&quot;&#125;</span><br></pre></td></tr></table></figure>

<p><strong>5.使用提供的OpenStack私有云平台，修改普通用户权限，使普通用户不能对镜像进行创建和删除操作</strong></p>
<p><strong>6.在OpenStack私有云平台，创建一台云主机，并创建一个40G大小的cinder块存储，将块存储连接到云主机，然后在云主机上对云硬盘进行操作。要求分出2个大小为10G的分区，使用这2个分区，创建名为&#x2F;dev&#x2F;md0、raid级别为0的磁盘阵列，最后将md0格式化为ext4格式并挂载至&#x2F;mnt目录。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# fdisk /dev/vdc</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (1-4，默认 1)：</span><br><span class="line">起始 扇区 (2048-83886079，默认为 2048)：</span><br><span class="line">将使用默认值 2048</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (2048-83886079，默认为 83886079)：+10G</span><br><span class="line">分区 1 已设置为 Linux 类型，大小设为 10 GiB</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (2-4，默认 2)：</span><br><span class="line">起始 扇区 (20973568-83886079，默认为 20973568)：</span><br><span class="line">将使用默认值 20973568</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (20973568-83886079，默认为 83886079)：+10G</span><br><span class="line">分区 2 已设置为 Linux 类型，大小设为 10 GiB</span><br><span class="line">命令(输入 m 获取帮助)：t</span><br><span class="line">分区号 (1,2，默认 2)：1</span><br><span class="line">Hex 代码(输入 L 列出所有代码)：fd</span><br><span class="line">已将分区“Linux”的类型更改为“Linux raid autodetect”</span><br><span class="line">命令(输入 m 获取帮助)：t</span><br><span class="line">分区号 (1,2，默认 2)：</span><br><span class="line">Hex 代码(输入 L 列出所有代码)：fd</span><br><span class="line">已将分区“Linux”的类型更改为“Linux raid autodetect”</span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br><span class="line">[root@mysql1 ~]# lsblk</span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  50G  0 disk </span><br><span class="line">└─vda1 253:1    0  50G  0 part /</span><br><span class="line">vdb    253:16   0  50G  0 disk /mnt</span><br><span class="line">vdc    253:32   0  40G  0 disk </span><br><span class="line">├─vdc1 253:33   0  10G  0 part </span><br><span class="line">└─vdc2 253:34   0  10G  0 part </span><br></pre></td></tr></table></figure>

<p><strong>创建磁盘阵列</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# mdadm -C /dev/md0 -l 0 -n 2 /dev/vdc1 /dev/vdc2</span><br><span class="line">mdadm: Fail create md0 when using /sys/module/md_mod/parameters/new_array</span><br><span class="line">mdadm: Defaulting to version 1.2 metadata</span><br><span class="line">mdadm: array /dev/md0 started.</span><br><span class="line">[root@mysql1 ~]# lsblk</span><br><span class="line">NAME    MAJ:MIN RM SIZE RO TYPE  MOUNTPOINT</span><br><span class="line">vda     253:0    0  50G  0 disk  </span><br><span class="line">└─vda1  253:1    0  50G  0 part  /</span><br><span class="line">vdb     253:16   0  50G  0 disk  /mnt</span><br><span class="line">vdc     253:32   0  40G  0 disk  </span><br><span class="line">├─vdc1  253:33   0  10G  0 part  </span><br><span class="line">│ └─md0   9:0    0  20G  0 raid0 </span><br><span class="line">└─vdc2  253:34   0  10G  0 part  </span><br><span class="line">  └─md0   9:0    0  20G  0 raid0 </span><br></pre></td></tr></table></figure>

<p><strong>格式化为ext4格式</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# mkfs.ext4 /dev/md0</span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">文件系统标签=</span><br><span class="line">OS type: Linux</span><br><span class="line">块大小=4096 (log=2)</span><br><span class="line">分块大小=4096 (log=2)</span><br><span class="line">Stride=128 blocks, Stripe width=256 blocks</span><br><span class="line">1310720 inodes, 5238272 blocks</span><br><span class="line">261913 blocks (5.00%) reserved for the super user</span><br><span class="line">第一个数据块=0</span><br><span class="line">Maximum filesystem blocks=2153775104</span><br><span class="line">160 block groups</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">8192 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class="line">        4096000</span><br><span class="line">Allocating group tables: 完成                            </span><br><span class="line">正在写入inode表: 完成                            </span><br><span class="line">Creating journal (32768 blocks): 完成</span><br><span class="line">Writing superblocks and filesystem accounting information: 完成</span><br></pre></td></tr></table></figure>

<p><strong>挂载到mnt</strong></p>
<p><code>mount /dev/md0 /mnt/</code></p>
<p><strong>7.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。</strong></p>
<p><strong>8.使用cyborg硬件加速服务，实现硬件资源的发现、上报、挂载＼卸载等资源管理。</strong></p>
<p><strong>9.在OpenStack私有云平台，创建一台云主机，编写脚本，要求可以完成数据库的定期备份，并把数据库备份文件存放在&#x2F;opt目录下。</strong></p>
<p><strong>10.使用提供的iaas-error3镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为nova服务无法正常使用，创建云主机报错，请结合报错信息排查错误，使nova服务可以正常使用，可以正常创建云主机。</strong></p>
<h1 id="云计算赛项赛卷4"><a href="#云计算赛项赛卷4" class="headerlink" title="云计算赛项赛卷4"></a>云计算赛项赛卷4</h1><p><strong>1.在提供的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板create_net.yaml，创建名为Heat-Network网络，选择不共享；创建子网名为Heat-Subnet，子网网段设置为10.20.2.0&#x2F;24，开启DHCP服务，地址池为10.20.2.20-10.20.2.100。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat createnet.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  network:</span><br><span class="line">    type: OS::Neutron::Net</span><br><span class="line">    properties:</span><br><span class="line">      name: &quot;extnet&quot;</span><br><span class="line">      shared: false</span><br><span class="line">      admin_state_up: true</span><br><span class="line">  subnet:</span><br><span class="line">    type: OS::Neutron::Subnet</span><br><span class="line">    properties:</span><br><span class="line">      cidr: 10.10.2.0/24</span><br><span class="line">      enable_dhcp: true</span><br><span class="line">      gateway_ip: 10.10.2.1</span><br><span class="line">      allocation_pools:</span><br><span class="line">        - start: 10.10.2.20</span><br><span class="line">          end: 10.10.2.100</span><br><span class="line">      name: &quot;subextnet&quot;</span><br><span class="line">      network_id: &#123;get_resource: &quot;network&quot;&#125;</span><br></pre></td></tr></table></figure>

<p><strong>2.在提供的OpenStack私有云平台，创建一台云主机（镜像使用CentOS7.9，flavor使用带临时磁盘50G的），配置该主机为nfs的server端，将该云主机中的&#x2F;mnt&#x2F;test目录进行共享（目录不存在可自行创建）。然后配置controller节点为nfs的client端，要求将&#x2F;mnt&#x2F;test目录作为glance后端存储的挂载目录。</strong></p>
<p><strong>nfs服务端：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@nfs ~]# umount /mnt</span><br><span class="line">[root@nfs ~]# lsblk</span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  100G  0 disk </span><br><span class="line">└─vda1 253:1    0  100G  0 part /</span><br><span class="line">vdb    253:16   0   50G  0 disk </span><br><span class="line">[root@nfs ~]# </span><br><span class="line">[root@nfs ~]# mkfs.ext4 /dev/vdb</span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">文件系统标签=</span><br><span class="line">OS type: Linux</span><br><span class="line">块大小=4096 (log=2)</span><br><span class="line">分块大小=4096 (log=2)</span><br><span class="line">Stride=0 blocks, Stripe width=0 blocks</span><br><span class="line">3276800 inodes, 13107200 blocks</span><br><span class="line">655360 blocks (5.00%) reserved for the super user</span><br><span class="line">第一个数据块=0</span><br><span class="line">Maximum filesystem blocks=2162163712</span><br><span class="line">400 block groups</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">8192 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class="line">        4096000, 7962624, 11239424</span><br><span class="line">Allocating group tables: 完成                            </span><br><span class="line">正在写入inode表: 完成                            </span><br><span class="line">Creating journal (32768 blocks): 完成</span><br><span class="line">Writing superblocks and filesystem accounting information: 完成   </span><br><span class="line">[root@nfs ~]# mount /dev/vdb /mnt</span><br><span class="line">[root@nfs ~]# lsblk</span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  100G  0 disk </span><br><span class="line">└─vda1 253:1    0  100G  0 part /</span><br><span class="line">vdb    253:16   0   50G  0 disk /mnt</span><br></pre></td></tr></table></figure>

<p><code>yum install -y nfs-utils rpcbind</code></p>
<p> <code>mkdir /mnt/test</code></p>
<p><code>echo /mnt/test *(rw,async,no_root_squash) &gt; /etc/exports</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@nfs ~]# systemctl restart rpcbind</span><br><span class="line">[root@nfs ~]# systemctl restart nfs</span><br><span class="line">[root@nfs ~]# systemctl enable rpcbind</span><br><span class="line">[root@nfs ~]# systemctl enable nfs</span><br></pre></td></tr></table></figure>

<p><strong>nfs客户端：</strong></p>
<p><code>echo 192.168.20.106:/mnt/test /var/lib/glance/images nfs default netdev 0 0 &gt; /etc/fstab</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# systemctl restart rpcbind</span><br><span class="line">[root@controller ~]# systemctl restart nfs</span><br><span class="line">[root@controller ~]# systemctl enable rpcbind</span><br><span class="line">[root@controller ~]# systemctl enable nfs</span><br><span class="line">[root@controller ~]# showmount -e 192.168.20.106</span><br><span class="line">Export list for 192.168.20.106:</span><br><span class="line">/mnt/test *</span><br><span class="line">[root@controller ~]# mount -t nfs 192.168.20.106:/mnt/test /var/lib/glance/images/</span><br><span class="line">[root@controller ~]# chown -R glance:glance /var/lib/glance/images/</span><br><span class="line">[root@controller ~]# ll /var/lib/glance/</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x 2 glance glance 4096 4月  25 06:18 images</span><br><span class="line">[root@controller ~]# mount </span><br><span class="line">...............................................................</span><br><span class="line">192.168.20.106:/mnt/test on /var/lib/glance/images type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.20.105,local_lock=none,addr=192.168.20.106)</span><br></pre></td></tr></table></figure>

<p><strong>3.使用OpenStack私有云平台，找到virsh中ID为10的云主机（若不存在请自行创建）。在云主机所在的物理节点，进入virsh交互式界面，使用virsh命令，将memory虚拟机的内存调整为5242880KB大小。</strong></p>
<p><strong>4.使用OpenStack私有云平台，创建一台云主机，创建完之后对该云主机进行打快照处理，并将该快照保存至&#x2F;root&#x2F;cloudsave目录，保存名字为csccvm.qcow2。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server image create vm1 --name csccvm</span><br><span class="line">[root@controller ~]# mkdir /root/cloudsave</span><br><span class="line">[root@controller ~]# openstack image save --file /root/cloudsave/csccvm.qcow2 csccvm</span><br></pre></td></tr></table></figure>

<p><strong>5.使用OpenStack私有云平台，使用centos7.9镜像，flavor使用1vcpu&#x2F;2G内存&#x2F;40G硬盘，创建云主机cscc_vm，假设在使用过程中，发现该云主机配置太低，需要调整，请修改相应配置，将dashboard界面上的云主机调整实例大小可以使用，将该云主机实例大小调整为2vcpu&#x2F;4G内存&#x2F;40G硬盘。</strong></p>
<p><code>sed -i &#39;s/#allow_resize_to_same_host=false/allow_resize_to_same_host=True/g&#39; /etc/nova/nova.conf</code></p>
<p><code>systemctl restart *nova*</code></p>
<p>然后图形化操作</p>
<p><strong>6.使用OpenStack私有云平台，创建三个云主机，使用提供的软件包安装RabbitMQ服务，安装完毕后，搭建RabbitMQ集群，并打开RabbitMQ服务的图形化监控页面插件。集群使用普通集群模式，其中第一台做磁盘节点，另外两台做内存节点。</strong></p>
<p><strong>（1）修改主机名</strong></p>
<p>对这3台虚拟机进行修改主机名的操作，主机名修改为rabbitmq1，rabbitmq2，rabbitmq3。命令如下</p>
<p><strong>rabbitmaq1</strong></p>
<p><code>hostnamectl set-hostname rabbitmq1</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p> <strong>rabbitmaq2</strong></p>
<p><code>hostnamectl set-hostname rabbitmq2</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p> <strong>rabbitmaq3</strong></p>
<p><code>hostnamectl set-hostname rabbitmq3</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>（2）修改hosts</strong></p>
<p>三个节点都配置hosts</p>
<p><code>vi /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">192.168.20.121  rabbitmq1</span><br><span class="line">192.168.20.128  rabbitmq2</span><br><span class="line">192.168.20.116  rabbitmq3</span><br></pre></td></tr></table></figure>

<p><strong>（4）配置yum源</strong></p>
<p>三个节点均使用提供的rabbitmq-repo.tar.gz的压缩包，上传至虚拟机的&#x2F;root目录下，解压并放在&#x2F;opt目录下，进入&#x2F;etc&#x2F;yum.repos.d目录下，将原来的repo文件移除，新建local.repo文件并编辑内容，具体操作命令如下：</p>
<p><code>curl -O http://172.19.25.11/rabbitmq-repo.tar.gz</code></p>
<p><code>tar -zxvf rabbitmq-repo.tar.gz -C /opt/</code></p>
<p><code>mv /etc/yum.repos.d/* /tmp/</code></p>
<p><code>vi local.repo</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[rabbitmq]</span><br><span class="line">name=rabbitmq</span><br><span class="line">baseurl=file:///opt/rabbitmq-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><strong>（5）安装RabbitMQ服务并启动</strong></p>
<p>配置完毕后，三个节点安装RabbitMQ服务，命令如下：</p>
<p><code>yum install -y rabbitmq-server</code></p>
<p>rabbitmq1节点启动RabbitMQ服务并查看服务状态，命令如下：</p>
<p><code>systemctl start rabbitmq-server</code></p>
<p><code>systemctl status rabbitmq-server</code></p>
<p><strong>（6）配置界面访问</strong></p>
<p>RabbitMQ提供了一个非常友好的图形化监控页面插件（rabbitmq_management），让我们可以一目了然看见Rabbit的状态或集群状态。启用图形化页面插件的具体命令如下：</p>
<p><code>rabbitmq-plugins enable rabbitmq_management</code></p>
<p><code>service rabbitmq-server restart</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]# netstat -ntpl</span><br><span class="line"></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address      Foreign Address     State    PID/Program name  </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:25672      0.0.0.0:*        LISTEN   13685/beam     </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:111       0.0.0.0:*        LISTEN   500/rpcbind     </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:4369      0.0.0.0:*        LISTEN   11320/epmd     </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:22       0.0.0.0:*        LISTEN   1162/sshd      </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:15672      0.0.0.0:*        LISTEN   13685/beam     </span><br><span class="line"></span><br><span class="line">tcp    0   0 127.0.0.1:25      0.0.0.0:*        LISTEN   932/master     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::5672         :::*          LISTEN   13685/beam     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::111         :::*          LISTEN   500/rpcbind     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::4369         :::*          LISTEN   11320/epmd     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::22          :::*          LISTEN   1162/sshd      </span><br><span class="line"></span><br><span class="line">tcp6    0   0 ::1:25         :::*          LISTEN   932/master  </span><br></pre></td></tr></table></figure>

<p>可以看到15672端口已开放，打开浏览器，输入rabbitmq1节点的IP+端口15672（<a href="http://172.30.11.12:15672）访问RabbitMQ监控界面，使用默认的用户名和密码登录（用户名和密码都为guest）">http://172.30.11.12:15672）访问RabbitMQ监控界面，使用默认的用户名和密码登录（用户名和密码都为guest）</a></p>
<p><strong>（7）配置节点间的通信</strong></p>
<p>RabbitMQ的集群是依附于erlang集群来工作的，所以必须先构建起一个erlang集群。erlang集群中各节点是由magic cookie来实现的，每个节点上要保持相同的.erlang.cookie文件，这个cookie存放在&#x2F;var&#x2F;lib&#x2F;rabbitmq&#x2F;.erlang.cookie中，文件是400的权限。必须保证各节点cookie一致，不然节点之间就无法通信。</p>
<p>查看rabbitmq1节点的.erlang.cookie文件，并将该文件复制到rabbitmq2和rabbitmq3节点的&#x2F;var&#x2F;lib&#x2F;rabbitmq&#x2F;目录下，命令如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]# cat /var/lib/rabbitmq/.erlang.cookie </span><br><span class="line"></span><br><span class="line">EZYGPUJOTSESXPAUFMWO</span><br><span class="line"></span><br><span class="line">[root@rabbitmq1 ~]# scp /var/lib/rabbitmq/.erlang.cookie root@rabbitmq2:/var/lib/rabbitmq/</span><br><span class="line"></span><br><span class="line">[root@rabbitmq1 ~]# scp /var/lib/rabbitmq/.erlang.cookie root@rabbitmq3:/var/lib/rabbitmq/</span><br></pre></td></tr></table></figure>

<p>将.erlang.cookie文件传至rabbitmq2和rabbitmq3节点后，需要修改该文件的用户与用户组，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]#cd /var/lib/rabbitmq/</span><br><span class="line">[root@rabbitmq1 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie</span><br><span class="line">[root@rabbitmq2 ~]#cd /var/lib/rabbitmq/</span><br><span class="line">[root@rabbitmq2 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie</span><br><span class="line">[root@rabbitmq3 ~]#cd /var/lib/rabbitmq/</span><br><span class="line">[root@rabbitmq3 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie</span><br></pre></td></tr></table></figure>

<p><strong>（8）配置节点加入集群</strong></p>
<p>在rabbitmq2、rabbitmq3节点执行如下命令，将这两个节点作为RAM节点加入到RabbitMQ集群中，具体命令如下：</p>
<p>rabbitmq2节点：</p>
<p><code>systemctl start rabbitmq-server</code></p>
<p><code>service rabbitmq-server restart</code></p>
<p>rabbitmq3节点：</p>
<p><code>systemctl start rabbitmq-server</code></p>
<p><code>service rabbitmq-server restart</code></p>
<p>上面这俩步千万不要忘记打</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rabbitmq2节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">Stopping node rabbit@rabbitmq2 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmqctl join_cluster --ram rabbit@rabbitmq1</span><br><span class="line"></span><br><span class="line">Clustering node rabbit@rabbitmq2 with rabbit@rabbitmq1 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmqctl start_app</span><br><span class="line"></span><br><span class="line">Starting node rabbit@rabbitmq2 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">rabbitmq3节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">Stopping node rabbit@rabbitmq3 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmqctl join_cluster --ram rabbit@rabbitmq1</span><br><span class="line"></span><br><span class="line">Clustering node rabbit@rabbitmq3 with rabbit@rabbitmq1 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmqctl start_app</span><br><span class="line"></span><br><span class="line">Starting node rabbit@rabbitmq3 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br></pre></td></tr></table></figure>

<p>默认rabbitmq启动后是磁盘节点，在这个cluster命令下，rabbitmq2和rabbitmq3是内存节点，rabbitmq1是磁盘节点。</p>
<p>如果要使rabbitmq2、rabbitmq3都是磁盘节点，去掉–ram参数即可。</p>
<p>如果想要更改节点类型，可以使用命令rabbitmqctl change_cluster_node_type disc(ram)，前提是必须停掉Rabbit应用。</p>
<p><strong>（9）配置RAM节点启用界面</strong></p>
<p>在rabbitmq2和rabbitmq3节点上启用rabbitmq_management，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rabbitmq2节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmq-plugins enable rabbitmq_management</span><br><span class="line"></span><br><span class="line">The following plugins have been enabled:</span><br><span class="line"></span><br><span class="line"> mochiweb</span><br><span class="line"></span><br><span class="line"> webmachine</span><br><span class="line"></span><br><span class="line"> rabbitmq_web_dispatch</span><br><span class="line"></span><br><span class="line"> amqp_client</span><br><span class="line"></span><br><span class="line"> rabbitmq_management_agent</span><br><span class="line"></span><br><span class="line"> rabbitmq_management</span><br><span class="line"></span><br><span class="line">Plugin configuration has changed. Restart RabbitMQ for changes to take effect.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# systemctl restart rabbitmq-server</span><br><span class="line"></span><br><span class="line">rabbitmq3节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmq-plugins enable rabbitmq_management</span><br><span class="line"></span><br><span class="line">The following plugins have been enabled:</span><br><span class="line"></span><br><span class="line"> mochiweb</span><br><span class="line"></span><br><span class="line"> webmachine</span><br><span class="line"></span><br><span class="line"> rabbitmq_web_dispatch</span><br><span class="line"></span><br><span class="line"> amqp_client</span><br><span class="line"></span><br><span class="line"> rabbitmq_management_agent</span><br><span class="line"></span><br><span class="line"> rabbitmq_management</span><br><span class="line"></span><br><span class="line">Plugin configuration has changed. Restart RabbitMQ for changes to take effect.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# systemctl restart rabbitmq-server</span><br></pre></td></tr></table></figure>

<p>启用rabbitmq2节点和rabbitmq3节点的监控界面后，登录</p>
<p><strong>（10）查看集群状态</strong></p>
<p>在RabbitMQ集群的任一节点上，可以查看RabbitMQ集群的状态，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]# rabbitmqctl cluster_status</span><br><span class="line"></span><br><span class="line">Cluster status of node rabbit@rabbitmq1 ...</span><br><span class="line"></span><br><span class="line">[&#123;nodes,[&#123;disc,[rabbit@rabbitmq1]&#125;,&#123;ram,[rabbit@rabbitmq3,rabbit@rabbitmq2]&#125;]&#125;,</span><br><span class="line"></span><br><span class="line"> &#123;running_nodes,[rabbit@rabbitmq3,rabbit@rabbitmq2,rabbit@rabbitmq1]&#125;,</span><br><span class="line"></span><br><span class="line"> &#123;cluster_name,&lt;&lt;&quot;rabbit@rabbitmq1&quot;&gt;&gt;&#125;,</span><br><span class="line"></span><br><span class="line"> &#123;partitions,[]&#125;]</span><br><span class="line"></span><br><span class="line">...done.</span><br></pre></td></tr></table></figure>

<p>可以查看到rabbitmq1节点为disc磁盘节点，rabbitmq2节点和rabbitmq3节点为RAM内存节点。</p>
<p><strong>7.使用karbor服务，保护云平台上的卷（一般就是映射到VMs上的可进行读写的数据存储载体或设备）</strong></p>
<p><strong>8.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>9.使用octavia负载均衡服务，实现云平台上的负载均衡。</strong></p>
<p><strong>10.使用提供的iaas-error4镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为dashboard界面服务无法正常使用，请结合报错信息排查错误，使dashboard界面服务可以正常使用。</strong></p>
<h1 id="云计算赛项赛卷5"><a href="#云计算赛项赛卷5" class="headerlink" title="云计算赛项赛卷5"></a>云计算赛项赛卷5</h1><p><strong>1.使用OpenStack私有云平台，创建三个云主机，使用提供的软件包安装RabbitMQ服务，安装完毕后，搭建RabbitMQ集群，并打开RabbitMQ服务的图形化监控页面插件。集群使用普通集群模式，其中第一台做磁盘节点，另外两台做内存节点。</strong></p>
<p>略</p>
<p><strong>2.在openstack私有云平台上，创建云主机VM1，镜像使用cirros。创建成功后，将云主机打快照并保存到controller节点&#x2F;root&#x2F;cloudsave目录下，保存名字为csccvm.qcow2。最后使用qemu-img相关命令，将镜像的campat版本修改为0.10（该操作是为了适配某些低版本的云平台）。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server image create vm1 --name csccvm</span><br><span class="line">[root@controller ~]# mkdir /root/cloudsave</span><br><span class="line">[root@controller ~]# openstack image save --file /root/cloudsave/csccvm.qcow2 csccvm</span><br><span class="line">[root@controller ~]# cd /var/lib/glance/images/</span><br><span class="line">[root@controller images]# ll</span><br><span class="line">总用量 47464</span><br><span class="line">-rw-r----- 1 glance glance 22020096 5月   9 03:25 10b86659-1951-452b-9dbf-1ba3d5ad2f9d</span><br><span class="line">-rw-r----- 1 glance glance 13287936 5月   8 02:08 bd74df22-1558-4fff-b4f3-f78aee4275d6</span><br><span class="line">-rw-r----- 1 glance glance 13287936 5月   9 02:39 ccf243c9-8cb7-453e-902b-de66867c7e5f</span><br><span class="line">[root@controller images]# qemu-img amend -f qcow2 -o compat=0.10 10b86659-1951-452b-9dbf-1ba3d5ad2f9d</span><br></pre></td></tr></table></figure>

<p><strong>3.在提供OpenStack私有云平台，创建一台云主机，云主机名为cscc_vm，镜像使用cirros、flavor自行创建（配置自定义）。然后创建一块大小为1G的云硬盘，命名为block，并挂载到该云主机。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cinder create --name block 1</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| Property                       | Value                                |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| attachments                    | []                                   |</span><br><span class="line">| availability_zone              | nova                                 |</span><br><span class="line">| bootable                       | false                                |</span><br><span class="line">| consistencygroup_id            | None                                 |</span><br><span class="line">| created_at                     | 2022-05-09T03:28:56.000000           |</span><br><span class="line">| description                    | None                                 |</span><br><span class="line">| encrypted                      | False                                |</span><br><span class="line">| id                             | 1dd10e41-f0c6-4feb-a7e8-ddb1d99b067f |</span><br><span class="line">| metadata                       | &#123;&#125;                                   |</span><br><span class="line">| migration_status               | None                                 |</span><br><span class="line">| multiattach                    | False                                |</span><br><span class="line">| name                           | block                                |</span><br><span class="line">| os-vol-host-attr:host          | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:migstat | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:name_id | None                                 |</span><br><span class="line">| os-vol-tenant-attr:tenant_id   | 0047a899f1b34aaba102c89bf5dbeab4     |</span><br><span class="line">| replication_status             | None                                 |</span><br><span class="line">| size                           | 1                                    |</span><br><span class="line">| snapshot_id                    | None                                 |</span><br><span class="line">| source_volid                   | None                                 |</span><br><span class="line">| status                         | creating                             |</span><br><span class="line">| updated_at                     | None                                 |</span><br><span class="line">| user_id                        | 73ba03dfca0f4de6953a79e478cd035f     |</span><br><span class="line">| volume_type                    | None                                 |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack server add volume vm1 block</span><br></pre></td></tr></table></figure>

<p><strong>4.在提供的OpenStack平台上，编写heat模板createcinder.yml文件，模板作用为按照要求创建一个云硬盘。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat createcinder.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  cinder:</span><br><span class="line">    type: OS::Cinder::Volume</span><br><span class="line">    properties:</span><br><span class="line">      name: dsb</span><br><span class="line">      size: 10</span><br></pre></td></tr></table></figure>

<p><strong>5.在OpenStack私有云平台，创建一台云主机，并创建一个40G大小的cinder块存储，将块存储连接到云主机，然后在云主机上对云硬盘进行操作。要求分出4个大小为5G的分区，使用这4个分区，创建名为&#x2F;dev&#x2F;md5、raid级别为5的磁盘阵列加一个热备盘（&#x2F;dev&#x2F;vdb4为热备盘）。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# fdisk /dev/vdd</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">Device does not contain a recognized partition table</span><br><span class="line">使用磁盘标识符 0x206f71f2 创建新的 DOS 磁盘标签。</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (1-4，默认 1)：</span><br><span class="line">起始 扇区 (2048-83886079，默认为 2048)：</span><br><span class="line">将使用默认值 2048</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (2048-83886079，默认为 83886079)：+5G</span><br><span class="line">分区 1 已设置为 Linux 类型，大小设为 5 GiB</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (2-4，默认 2)：</span><br><span class="line">起始 扇区 (10487808-83886079，默认为 10487808)：</span><br><span class="line">将使用默认值 10487808</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (10487808-83886079，默认为 83886079)：+5G</span><br><span class="line">分区 2 已设置为 Linux 类型，大小设为 5 GiB</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (2 primary, 0 extended, 2 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (3,4，默认 3)：</span><br><span class="line">起始 扇区 (20973568-83886079，默认为 20973568)：</span><br><span class="line">将使用默认值 20973568</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (20973568-83886079，默认为 83886079)：+5G</span><br><span class="line">分区 3 已设置为 Linux 类型，大小设为 5 GiB</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (3 primary, 0 extended, 1 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default e): p</span><br><span class="line">已选择分区 4</span><br><span class="line">起始 扇区 (31459328-83886079，默认为 31459328)：</span><br><span class="line">将使用默认值 31459328</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (31459328-83886079，默认为 83886079)：+5G</span><br><span class="line">分区 4 已设置为 Linux 类型，大小设为 5 GiB</span><br><span class="line">命令(输入 m 获取帮助)：t</span><br><span class="line">分区号 (1-4，默认 4)：1</span><br><span class="line">Hex 代码(输入 L 列出所有代码)：fd</span><br><span class="line">已将分区“Linux”的类型更改为“Linux raid autodetect”</span><br><span class="line">命令(输入 m 获取帮助)：t</span><br><span class="line">分区号 (1-4，默认 4)：2</span><br><span class="line">Hex 代码(输入 L 列出所有代码)：fd</span><br><span class="line">已将分区“Linux”的类型更改为“Linux raid autodetect”</span><br><span class="line">命令(输入 m 获取帮助)：t</span><br><span class="line">分区号 (1-4，默认 4)：3</span><br><span class="line">Hex 代码(输入 L 列出所有代码)：fd</span><br><span class="line">已将分区“Linux”的类型更改为“Linux raid autodetect”</span><br><span class="line">命令(输入 m 获取帮助)：t</span><br><span class="line">分区号 (1-4，默认 4)：</span><br><span class="line">Hex 代码(输入 L 列出所有代码)：fd</span><br><span class="line">已将分区“Linux”的类型更改为“Linux raid autodetect”</span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br><span class="line">[root@mysql1 ~]# mdadm -C /dev/md5 -l 5 -n 3 -x 1 /dev/vdd1 /dev/vdd2 /dev/vdd3 /dev/vdd4</span><br><span class="line">mdadm: Fail create md5 when using /sys/module/md_mod/parameters/new_array</span><br><span class="line">mdadm: Defaulting to version 1.2 metadata</span><br><span class="line">mdadm: array /dev/md5 started.</span><br><span class="line">[root@mysql1 ~]# lsblk</span><br><span class="line">NAME    MAJ:MIN RM SIZE RO TYPE  MOUNTPOINT</span><br><span class="line">vda     253:0    0  50G  0 disk  </span><br><span class="line">└─vda1  253:1    0  50G  0 part  /</span><br><span class="line">vdb     253:16   0  50G  0 disk  /mnt</span><br><span class="line">vdd     253:48   0  40G  0 disk  </span><br><span class="line">├─vdd1  253:49   0   5G  0 part  </span><br><span class="line">│ └─md5   9:5    0  10G  0 raid5 </span><br><span class="line">├─vdd2  253:50   0   5G  0 part  </span><br><span class="line">│ └─md5   9:5    0  10G  0 raid5 </span><br><span class="line">├─vdd3  253:51   0   5G  0 part  </span><br><span class="line">│ └─md5   9:5    0  10G  0 raid5 </span><br><span class="line">└─vdd4  253:52   0   5G  0 part  </span><br><span class="line">  └─md5   9:5    0  10G  0 raid5 </span><br><span class="line">[root@mysql1 ~]# mdadm -D /dev/md5</span><br><span class="line">/dev/md5:</span><br><span class="line">           Version : 1.2</span><br><span class="line">     Creation Time : Mon May  9 03:40:45 2022</span><br><span class="line">        Raid Level : raid5</span><br><span class="line">        Array Size : 10475520 (9.99 GiB 10.73 GB)</span><br><span class="line">     Used Dev Size : 5237760 (5.00 GiB 5.36 GB)</span><br><span class="line">      Raid Devices : 3</span><br><span class="line">     Total Devices : 4</span><br><span class="line">       Persistence : Superblock is persistent</span><br><span class="line"></span><br><span class="line">       Update Time : Mon May  9 03:41:31 2022</span><br><span class="line">             State : clean </span><br><span class="line">    Active Devices : 3</span><br><span class="line">   Working Devices : 4</span><br><span class="line">    Failed Devices : 0</span><br><span class="line">     Spare Devices : 1</span><br><span class="line">            Layout : left-symmetric</span><br><span class="line">        Chunk Size : 512K</span><br><span class="line"></span><br><span class="line">Consistency Policy : resync</span><br><span class="line"></span><br><span class="line">              Name : mysql1:5  (local to host mysql1)</span><br><span class="line">              UUID : d90bb669:5411f2f1:d6f25855:d80efe7e</span><br><span class="line">            Events : 18</span><br><span class="line">    Number   Major   Minor   RaidDevice State</span><br><span class="line">       0     253       49        0      active sync   /dev/vdd1</span><br><span class="line">       1     253       50        1      active sync   /dev/vdd2</span><br><span class="line">       4     253       51        2      active sync   /dev/vdd3</span><br><span class="line">       3     253       52        -      spare   /dev/vdd4</span><br></pre></td></tr></table></figure>

<p><strong>6.使用提供的OpenStack平台，创建一台云主机，根据提供的软件包，编写一键部署LNMP+WordPress的部署脚本。</strong></p>
<p><strong>7.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。</strong></p>
<p><strong>8.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>9.使用cloudkitty计费服务，使用noop计费模型处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。</strong></p>
<p><strong>10.使用提供的iaas-error5镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为无法创建用户和项目，请结合报错信息排查错误，使keystone服务可以正常使用，可以正常创建用户与项目。</strong></p>
<h1 id="云计算赛项赛卷6"><a href="#云计算赛项赛卷6" class="headerlink" title="云计算赛项赛卷6"></a>云计算赛项赛卷6</h1><p><strong>1.使用提供的OpenStack私有云平台，修改相关参数对openstack平台进行调优操作，相应的调优操作有：</strong></p>
<p>（1）预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf </span><br><span class="line">vcpu_pin_set= 3-16</span><br></pre></td></tr></table></figure>

<p>（2）设置cpu超售比例为4倍；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf </span><br><span class="line">cpu_allocation_ratio=4.0</span><br></pre></td></tr></table></figure>

<p>（3）设置内存超售比例为1.5倍；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf </span><br><span class="line">ram_allocation_ratio=1.5</span><br></pre></td></tr></table></figure>

<p>（4）预留2048mb内存，这部分内存不能被虚拟机使用；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf </span><br><span class="line">reserved_host_memory_mb=2048</span><br></pre></td></tr></table></figure>

<p>（5）预留10240mb磁盘，这部分磁盘不能被虚拟机使用；</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf </span><br><span class="line">reserved_host_disk_mb=10240</span><br></pre></td></tr></table></figure>

<p>（6）设置nova服务心跳检查时间为120秒。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/nova/nova.conf </span><br><span class="line">service_down_time = 120</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart *nova*</code></p>
<p><strong>2.使用OpenStack私有云平台，创建一台云主机，创建完之后对该云主机进行打快照处理，并将该快照保存至&#x2F;root&#x2F;cloudsave目录，保存名字为csccvm.qcow2。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server image create vm1 --name csccvm</span><br><span class="line">[root@controller ~]# mkdir /root/cloudsave</span><br><span class="line">[root@controller ~]# openstack image save --file /root/cloudsave/csccvm.qcow2 csccvm</span><br></pre></td></tr></table></figure>

<p><strong>3.在提供的OpenStack平台上，使用Swift对象存储服务，修改相应的配置文件，使对象存储Swift作为glance镜像服务的后端存储。</strong></p>
<p> <code>vi /etc/glance/glance-api.conf</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[glance_store]</span><br><span class="line">#stores = file,http</span><br><span class="line">#default_store = file</span><br><span class="line">#filesystem_store_datadir = /var/lib/glance/images/</span><br><span class="line">default_store=swift</span><br><span class="line">stores=glance.store.swift.Store</span><br><span class="line">swift_store_auth_address=http://controller:5000/v3.0</span><br><span class="line">swift_store_endpoint_type=internalURL</span><br><span class="line">swift_store_multi_tenant=True</span><br><span class="line">swift_store_admin_tenants=service</span><br><span class="line">swift_store_user=glance</span><br><span class="line">swift_store_key=000000</span><br><span class="line">swift_store_container=glance</span><br><span class="line">swift_store_create_container_on_put=True</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart *glance*</code></p>
<p><strong>4.在提供的OpenStack平台上，编写heat模板createswift.yml文件，模板作用为按照要求创建容器。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat swift.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  swift:</span><br><span class="line">    type: OS::Swift::Container</span><br><span class="line">    properties:</span><br><span class="line">      name: skill</span><br></pre></td></tr></table></figure>

<p><strong>5.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>6.使用cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。</strong></p>
<p><strong>7.使用OpenStack私有云平台，创建两台云主机vm1和vm2，在这两台云主机上分别安装数据库服务，并配置成主从数据库，vm1节点为主库，vm2节点为从库（数据库密码设置为000000）。</strong></p>
<p>略</p>
<p><strong>8.使用OpenStack私有云平台，创建三台云主机vm1、vm2和vm3，首先使用两台云主机完成MariaDB数据库的主从配置，接着根据提供的数据库中间件Mycat。完成Mycat读写分离数据库的配置安装（逻辑库名称使用“USERDB”，数据库密码使用000000）。</strong></p>
<p>略</p>
<p><strong>9.在OpenStack私有云平台，创建一台云主机，使用提供的软件包，编写脚本，要求可以一键部署nfs服务。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mycat ~]# cat nfs.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line">yum install -y nfs-utils rpcbind</span><br><span class="line">umount /mnt</span><br><span class="line">mkfs.ext4 /dev/vdb</span><br><span class="line">mount /dev/vdb /mnt</span><br><span class="line">mkdir /mnt/test</span><br><span class="line">echo &#x27;/mnt/test *(rw,async,no_root_squash)&#x27; &gt; /etc/exports</span><br><span class="line">systemctl restart rpcbind</span><br><span class="line">systemctl restart nfs</span><br><span class="line">systemctl enable rpcbind</span><br><span class="line">systemctl enable nfs</span><br></pre></td></tr></table></figure>

<p><strong>10.使用提供的iaas-error6镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为所有的命令均无法正常使用，请结合报错信息排查错误，使OpenStack平台服务可以正常使用。</strong></p>
<h1 id="云计算赛项赛卷7"><a href="#云计算赛项赛卷7" class="headerlink" title="云计算赛项赛卷7"></a>云计算赛项赛卷7</h1><p><strong>1.在提供的OpenStack平台上，使用Cinder块存储服务，创建块存储卷，并使用该卷连接到云主机。</strong></p>
<p>略</p>
<p><strong>2.在提供的OpenStack平台上，编写heat模板createuser.yml文件，模板作用为按照要求创建项目与用户，并赋予用户admin权限。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat user.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  user:</span><br><span class="line">    type: OS::Keystone::User</span><br><span class="line">    properties:</span><br><span class="line">      name: heat-user</span><br><span class="line">      password: 000000</span><br><span class="line">      domain: demo</span><br><span class="line">      default_project: admin</span><br><span class="line">      roles: [&#123;&quot;role&quot;:&quot;admin&quot;,&quot;project&quot;:&quot;admin&quot;&#125;]</span><br></pre></td></tr></table></figure>

<p><strong>3.在提供的OpenStack平台上，上传cirros镜像，要求启动该镜像起码需要1G内存和10G硬盘。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image create cirros1 --min-disk 10 --min-ram 1024 --disk-format qcow2 --container-format bare &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| Field            | Value                                                |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     |</span><br><span class="line">| container_format | bare                                                 |</span><br><span class="line">| created_at       | 2022-05-09T05:16:23Z                                 |</span><br><span class="line">| disk_format      | qcow2                                                |</span><br><span class="line">| file             | /v2/images/0621890a-626c-485e-b6e6-db8bbee96898/file |</span><br><span class="line">| id               | 0621890a-626c-485e-b6e6-db8bbee96898                 |</span><br><span class="line">| min_disk         | 10                                                   |</span><br><span class="line">| min_ram          | 1024                                                 |</span><br><span class="line">| name             | cirros1                                              |</span><br><span class="line">| owner            | 0047a899f1b34aaba102c89bf5dbeab4                     |</span><br><span class="line">| protected        | False                                                |</span><br><span class="line">| schema           | /v2/schemas/image                                    |</span><br><span class="line">| size             | 13287936                                             |</span><br><span class="line">| status           | active                                               |</span><br><span class="line">| tags             |                                                      |</span><br><span class="line">| updated_at       | 2022-05-09T05:16:24Z                                 |</span><br><span class="line">| virtual_size     | None                                                 |</span><br><span class="line">| visibility       | shared                                               |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>4.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。</strong></p>
<p><strong>5.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>6.使用cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。</strong></p>
<p><strong>7.在OpenStack私有云平台，创建一台云主机，编写定时任务脚本，要求可以在每个星期的周三晚上11点，定时自动备份数据库。</strong></p>
<p><strong>8.在OpenStack私有云平台，创建一台云主机，并创建一个40G大小的cinder块存储，将块存储连接到云主机，然后在云主机上对云硬盘进行操作。要求分出三个大小为5G的分区，使用这三个分区，创建名chinaskill-vg的卷组。然后创建名chinaskill-lv的逻辑卷，大小为12G。</strong></p>
<p><code>yum install openstack-cinder -y</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# fdisk /dev/vdb</span><br><span class="line">欢迎使用 fdisk (util-linux 2.23.2)。</span><br><span class="line">更改将停留在内存中，直到您决定将更改写入磁盘。</span><br><span class="line">使用写入命令前请三思。</span><br><span class="line">Device does not contain a recognized partition table</span><br><span class="line">使用磁盘标识符 0x1ef41573 创建新的 DOS 磁盘标签。</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (1-4，默认 1)：</span><br><span class="line">起始 扇区 (2048-83886079，默认为 2048)：</span><br><span class="line">将使用默认值 2048</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (2048-83886079，默认为 83886079)：+5G</span><br><span class="line">分区 1 已设置为 Linux 类型，大小设为 5 GiB</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (2-4，默认 2)：</span><br><span class="line">起始 扇区 (10487808-83886079，默认为 10487808)：</span><br><span class="line">将使用默认值 10487808</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (10487808-83886079，默认为 83886079)：+5G</span><br><span class="line">分区 2 已设置为 Linux 类型，大小设为 5 GiB</span><br><span class="line">命令(输入 m 获取帮助)：n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (2 primary, 0 extended, 2 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">分区号 (3,4，默认 3)：</span><br><span class="line">起始 扇区 (20973568-83886079，默认为 20973568)：</span><br><span class="line">将使用默认值 20973568</span><br><span class="line">Last 扇区, +扇区 or +size&#123;K,M,G&#125; (20973568-83886079，默认为 83886079)：+5G</span><br><span class="line">分区 3 已设置为 Linux 类型，大小设为 5 GiB</span><br><span class="line">命令(输入 m 获取帮助)：w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">正在同步磁盘。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# vgcreate chinaskill-vg /dev/vdb1 /dev/vdb2 /dev/vdb3</span><br><span class="line">  Physical volume &quot;/dev/vdb1&quot; successfully created.</span><br><span class="line">  Physical volume &quot;/dev/vdb2&quot; successfully created.</span><br><span class="line">  Physical volume &quot;/dev/vdb3&quot; successfully created.</span><br><span class="line">  Volume group &quot;chinaskill-vg&quot; successfully created</span><br></pre></td></tr></table></figure>

<p><strong>9.使用OpenStack私有云平台，创建三个云主机，使用提供的软件包安装RabbitMQ服务，安装完毕后，搭建RabbitMQ集群，并打开RabbitMQ服务的图形化监控页面插件。集群使用普通集群模式，其中第一台做磁盘节点，另外两台做内存节点。</strong></p>
<p>略</p>
<p><strong>10.使用提供的iaas-error7镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为neutron网络命令无法正常使用，请结合报错信息排查错误，使neutron服务可以正常使用。</strong></p>
<h1 id="云计算赛项赛卷8"><a href="#云计算赛项赛卷8" class="headerlink" title="云计算赛项赛卷8"></a><strong>云计算赛项赛卷8</strong></h1><p><strong>1.使用提供的OpenStack私有云平台，修改普通用户权限，使普通用户不能对镜像进行创建和删除操作。</strong></p>
<p><strong>2.在提供的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板create_user.yaml，创建名为heat-user的用户，属于admin项目，并赋予heat-user用户admin的权限，配置用户密码为123456。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat user.yml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  user:</span><br><span class="line">    type: OS::Keystone::User</span><br><span class="line">    properties:</span><br><span class="line">      name: heat-user</span><br><span class="line">      password: 000000</span><br><span class="line">      domain: demo</span><br><span class="line">      default_project: admin</span><br><span class="line">      roles: [&#123;&quot;role&quot;:&quot;admin&quot;,&quot;project&quot;:&quot;admin&quot;&#125;]</span><br></pre></td></tr></table></figure>

<p><strong>3.使用提供的OpenStack私有云平台，优化KVM的I&#x2F;O调度算法，将默认的deadline修改为none模式。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# echo none &gt; /sys/block/vda/queue/scheduler </span><br><span class="line">[root@controller ~]# cat /sys/block/vda/queue/scheduler </span><br><span class="line">[none] mq-deadline kyber </span><br></pre></td></tr></table></figure>

<p><strong>4.在提供的OpenStack私有云平台，创建一台云主机，配置该主机为nfs的server端，将该云主机中的&#x2F;mnt&#x2F;test目录进行共享（目录不存在可自行创建）。然后配置controller节点为nfs的client端，要求将&#x2F;mnt&#x2F;test目录作为glance后端存储的挂载目录。</strong></p>
<p>略</p>
<p><strong>5.使用提供的OpenStack私有云平台，申请三台云主机，使用提供的http源，在两个节点自行安装redis服务并启动，配置redis的访问需要密码，密码设置为123456。然后将这三个redis节点配置为redis的一主二从三哨兵架构</strong></p>
<p>主节点：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf </span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">daemonize yes</span><br><span class="line">masterauth 123456</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<p>从节点1：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf</span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">daemonize yes</span><br><span class="line">slaveof 192.168.20.104 6379</span><br><span class="line">masterauth 123456</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@redis-2 ~]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; info</span><br><span class="line">.................</span><br><span class="line"># Replication</span><br><span class="line">role:slave</span><br><span class="line">master_host:192.168.20.104</span><br><span class="line">master_port:6379</span><br><span class="line">master_link_status:up</span><br><span class="line">master_last_io_seconds_ago:7</span><br><span class="line">master_sync_in_progress:0</span><br><span class="line">slave_repl_offset:589</span><br><span class="line">slave_priority:100</span><br><span class="line">slave_read_only:1</span><br><span class="line">connected_slaves:0</span><br><span class="line">master_repl_offset:0</span><br><span class="line">repl_backlog_active:0</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:0</span><br><span class="line">repl_backlog_histlen:0</span><br><span class="line">....................</span><br></pre></td></tr></table></figure>

<p>从节点2：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf</span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">daemonize yes</span><br><span class="line">slaveof 192.168.20.104 6379</span><br><span class="line">masterauth 123456</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@redis-3 ~]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; info</span><br><span class="line">.................</span><br><span class="line"># Replication</span><br><span class="line">role:slave</span><br><span class="line">master_host:192.168.20.104</span><br><span class="line">master_port:6379</span><br><span class="line">master_link_status:up</span><br><span class="line">master_last_io_seconds_ago:7</span><br><span class="line">master_sync_in_progress:0</span><br><span class="line">slave_repl_offset:589</span><br><span class="line">slave_priority:100</span><br><span class="line">slave_read_only:1</span><br><span class="line">connected_slaves:0</span><br><span class="line">master_repl_offset:0</span><br><span class="line">repl_backlog_active:0</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:0</span><br><span class="line">repl_backlog_histlen:0</span><br><span class="line">....................</span><br></pre></td></tr></table></figure>

<p>1、哨兵模式搭建：<br>哨兵模式需要修改sentinel.conf文件，三台服务器均为此配置</p>
<p><code>vi /etc/redis-sentinel.conf</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">端口默认为26379。</span><br><span class="line">port 26379</span><br><span class="line">关闭保护模式，可以外部访问。</span><br><span class="line">protected-mode no</span><br><span class="line">设置为后台启动。</span><br><span class="line">daemonize yes</span><br><span class="line">指定主机IP地址和端口(三台配置均为指定主机ip)，并且指定当有2台哨兵认为主机宕机，则对主机进行容灾切换。mymaster：设置master名字，必须在其它有用到该名字的命令之前设置</span><br><span class="line">sentinel monitor mymaster 192.168.20.104 6379 2</span><br><span class="line">当在Redis实例中开启了requirepass，这里就需要提供密码。</span><br><span class="line">sentinel auth-pass mymaster 123456</span><br><span class="line">这里设置了主机多少秒无响应，则认为挂了。此处3秒</span><br><span class="line">sentinel down-after-milliseconds mymaster 3000</span><br><span class="line">主备切换时，最多有多少个slave同时对新的master进行同步，这里设置为默认的1。</span><br><span class="line">snetinel parallel-syncs mymaster 1</span><br><span class="line">故障转移的超时时间，这里设置为三分钟。</span><br><span class="line">sentinel failover-timeout mymaster 180000</span><br></pre></td></tr></table></figure>

<p><strong>启动哨兵模式</strong></p>
<p><code>redis-sentinel /etc/redis-sentinel.conf</code></p>
<p><strong>6.使用提供的OpenStack私有云平台，申请三台云主机，分别命令为node1、node2、node3，使用提供的软件包，在这三个节点上安装数据库服务，数据库密码设置为123456。将这三个节点配置为数据库高可用集群即MariaDB_Galera_Cluster。</strong></p>
<p><strong>7.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。</strong></p>
<p><strong>8.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>9.使用cloudkitty计费服务，使用hashmap计费模型处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。</strong></p>
<p><strong>10.使用提供的iaas-error8镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为上传镜像一直处于pending状态，请结合报错信息排查错误，使glance服务可以正常使用。</strong></p>
<h1 id="云计算赛项赛卷9"><a href="#云计算赛项赛卷9" class="headerlink" title="云计算赛项赛卷9"></a>云计算赛项赛卷9</h1><p><strong>1.OpenStack各服务内部通信都是通过RPC来交互，各agent都需要去连接RabbitMQ；随着各服务agent增多，MQ的连接数会随之增多，最终可能会到达上限，成为瓶颈。使用提供的OpenStack私有云平台，通过修改limits.conf配置文件来修改RabbitMQ服务的最大连接数为10240。</strong></p>
<p>略</p>
<p><strong>2.在提供的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板create_net.yaml，创建名为Heat-Network网络，选择不共享；创建子网名为Heat-Subnet，子网网段设置为10.20.2.0&#x2F;24，开启DHCP服务，地址池为10.20.2.20-10.20.2.100。</strong></p>
<p>略</p>
<p><strong>3.使用提供的OpenStack私有云平台，申请三台云主机，分别命令为node1、node2、node3，使用提供的软件包，在这三个节点上安装数据库服务，数据库密码设置为123456。将这三个节点配置为数据库高可用集群即MariaDB_Galera_Cluster。</strong></p>
<p><strong>4.使用上一题配置完成的数据库集群，使用提供的mariadb-repo.tar.gz软件包，安装haproxy负载均衡服务。配置node1节点为负载均衡的窗口，配置负载均衡为轮询算法；HA服务监听的端口为node1节点的3307端口；配置访问三个节点的权重依次为1,2,4。</strong></p>
<p><strong>5.在OpenStack私有云平台，创建一台云主机，编写脚本，使得可以一键部署ELK系统。</strong></p>
<p><strong>6.在提供的OpenStack平台上，上传cirros镜像，并对该镜像打上一个test的标签。</strong></p>
<p><code>openstack image set cirros --tag test</code></p>
<p><strong>7.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。</strong></p>
<p><strong>8.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>9.使用cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。</strong></p>
<p><strong>10.使用提供的iaas-error9镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为cinder服务因创建的卷太多，无法满足使用，请结合报错信息排查错误，扩容cinder存储池，使cinder服务可以正常使用。</strong></p>
<h1 id="云计算赛项赛卷10"><a href="#云计算赛项赛卷10" class="headerlink" title="云计算赛项赛卷10"></a>云计算赛项赛卷10</h1><p><strong>1.在提供的OpenStack平台上，通过nova的相关命令创建云主机类型，名字exam，ID为1234，内存为1024，硬盘为20G，虚拟内核数量为2。并修改云平台中默认每个tenant的实例配额为20个。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# nova flavor-create --is-public true exam 1234 1024 20 2</span><br><span class="line">+------+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+</span><br><span class="line">| ID   | Name | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public | Description |</span><br><span class="line">+------+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+</span><br><span class="line">| 1234 | exam | 1024      | 20   | 0         |      | 2     | 1.0         | True      | -           |</span><br><span class="line">+------+------+-----------+------+-----------+------+-------+-------------+-----------+-------------+</span><br></pre></td></tr></table></figure>

<p><strong>2.在提供的OpenStack平台上，使用Zun组件创建容器。</strong></p>
<p><strong>3.在日常运维管理中，安装包的依赖关系冲突是经常存在的。在控制节点，安装libguestfs-tools工具的时候，会发生依赖包的冲突，请解决依赖关系的报错，完成libguestfs-tools工具的安装。</strong></p>
<p><strong>4.使用提供的OpenStack私有云平台，假设在使用过程中，发现该创建的云主机配置太低，需要调整，请修改相应配置，将dashboard界面上的云主机调整实例大小可以使用。</strong></p>
<p><strong>5.在提供的OpenStack平台上，使用ceilometer相关命令，修改名称为cpu_hi的告警状态为不生效。</strong></p>
<p><strong>6.使用manila共享文件系统服务，使manila为多租户云环境中的共享文件系统提供统一的管理服务。</strong></p>
<p><strong>7.使用Blazar服务，使得管理员可以在OpenStack中为虚拟（实例，卷等）和物理（主机，存储等）的不同资源类型提供资源预留的能力。</strong></p>
<p><strong>8.使用cloudkitty计费服务，处理来自不同监控指标后端的数据并进行计费规则创建。以达到费用核算目的。</strong></p>
<p><strong>9.在OpenStack私有云平台，创建一台云主机，编写脚本，要求可以完成数据库的定期备份，把数据库备份文件存放在&#x2F;opt目录下，要求每个备份文件都已时间命名，并只保留一个月的备份文件，超过一个月的自动删除。</strong></p>
<p><strong>10.使用提供的iaas-error10镜像创建云主机，创建后的云主机内有错误的OpenStack平台，错误现象为安全组策略无法正常使用，请结合报错信息排查错误，使安全组可以正常使用。</strong></p>
]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>“云计算-容器云-任务2”</title>
    <url>/2022/03/31/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A12/</url>
    <content><![CDATA[<h1 id="云计算赛项第二场-容器云-江苏卷-任务2"><a href="#云计算赛项第二场-容器云-江苏卷-任务2" class="headerlink" title="云计算赛项第二场-容器云-江苏卷-任务2"></a>云计算赛项第二场-容器云-江苏卷-任务2</h1><h2 id="【任务2】基于Docker-容器的web-应用系统部署-10-分"><a href="#【任务2】基于Docker-容器的web-应用系统部署-10-分" class="headerlink" title="【任务2】基于Docker 容器的web 应用系统部署[10 分]"></a>【任务2】基于Docker 容器的web 应用系统部署[10 分]<span id="more"></span></h2><p>将MariaDB 数据库组件、Redis 消息组件、RabbitMQ 消息组件、Nacos-Registry 注册中心服务组件和前端Nginx 组件按照要求进行容器化。（所需要的软件包mall-swarm.tar.gz在http 服务中）。<br><strong>【适用平台】私有云</strong></p>
<h3 id="【题目1】容器化部署Redis"><a href="#【题目1】容器化部署Redis" class="headerlink" title="【题目1】容器化部署Redis"></a>【题目1】容器化部署Redis</h3><p>在 master 节 点 上 编 写 &#x2F;root&#x2F;redis&#x2F;Dockerfile文 件 构 建chinaskill-redis:v1.1 镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；<br>（2）作者：Chinaskill；<br>（3）修改配置文件中的bind 127.0.0.1为bind 0.0.0.0；<br>（4）设置Redis免密，并关闭保护模式；<br>（5）开放端口：6379；<br>（6）设置服务开机自启。<br>完成后构建镜像，并提交master 节点的用户名、密码和IP 到答题框。</p>
<p><code>cd /root</code></p>
<p><code>mkdir redis</code></p>
<p><code>cd redis</code></p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">RUN yum -y install redis</span><br><span class="line">RUN sed -i &#x27;s/bind 127.0.0.1/bind 0.0.0.0/g&#x27; /etc/redis.conf &amp;&amp; sed -i &#x27;s/protected-mode yes/protected-mode no/g&#x27; /etc/redis.conf</span><br><span class="line">EXPOSE 6379</span><br><span class="line">CMD [&quot;redis-server&quot;,&quot;/etc/redis.conf&quot;]</span><br></pre></td></tr></table></figure>

<p><code>vi local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[docker]</span><br><span class="line">baseurl=http://172.19.25.11/paas/kubernetes-repo/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[centos]</span><br><span class="line">baseurl=ftp://192.168.20.132/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><code>docker build -t chinaskill-redis:v1.1 .</code></p>
<h3 id="【题目2】容器化部署MariaDB"><a href="#【题目2】容器化部署MariaDB" class="headerlink" title="【题目2】容器化部署MariaDB"></a>【题目2】容器化部署MariaDB</h3><p>在 master 节 点 上 编 写 &#x2F;root&#x2F;mariadb&#x2F;Dockerfile 文 件 构 建chinaskill-mariadb:v1.1 镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；<br>（2）作者：Chinaskill；<br>（3）安装并初始化mariadb，密码：root；<br>（4）设置数据库编码为UTF-8；<br>（5）开放端口：3306<br>（6）设置mariadb 开机自启。<br>完成后构建镜像，并提交master 节点的用户名、密码和IP 到答题框。</p>
<p><code>cd /root</code></p>
<p><code>mkdir mariadb</code></p>
<p><code>cd mariadb</code></p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">ADD gpmall.sql /opt/</span><br><span class="line">ADD start.sh /opt/</span><br><span class="line">RUN yum -y install mariadb-server \</span><br><span class="line">&amp;&amp; chmod +x /opt/start.sh \</span><br><span class="line">&amp;&amp; /opt/start.sh</span><br><span class="line">EXPOSE 3306</span><br><span class="line">ENV LC_ALL en_US.UTF-8</span><br><span class="line">CMD mysqld_safe</span><br></pre></td></tr></table></figure>

<p><code>cp /root/redis/local.repo /root/mariadb/</code></p>
<p><code>cp /opt/ChinaskillMall/gpmall.sql /root/mariadb/</code></p>
<p><code>vi start.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">mysql_install_db --user=mysql</span><br><span class="line">mysqld_safe &amp;</span><br><span class="line">sleep 3</span><br><span class="line">mysqladmin -u root password &#x27;123456&#x27;</span><br><span class="line">mysql -uroot -p123456 -e &quot;grant all privileges on *.* to &#x27;root&#x27;@&#x27;%&#x27; identified by &#x27;123456&#x27;;&quot;</span><br><span class="line">mysql -uroot -p123456 -e &quot;create database gpmall;use gpmall;source /opt/gpmall.sql;&quot;</span><br></pre></td></tr></table></figure>

<p><code>docker build -t chinaskill-mariadb:v1.1 .</code></p>
<h3 id="【题目3】容器化部署zookeeper"><a href="#【题目3】容器化部署zookeeper" class="headerlink" title="【题目3】容器化部署zookeeper"></a>【题目3】容器化部署zookeeper</h3><p>在 master 节 点 上 编 写 &#x2F;root&#x2F;zookeeper&#x2F;Dockerfile 文 件 构 建chinaskill-zookeeper:v1.1 镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；<br>（2）作者：Chinaskill；<br>（3）开放端口：2181;<br>（4）设置 服务开机自启。<br>完成后构建镜像，并提交master 节点的用户名、密码和IP 到答题框。</p>
<p><code>cd /root</code></p>
<p><code>mkdir zookeeper</code></p>
<p><code>cd zookeeper</code></p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">EXPOSE 2181</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">COPY ftp.repo /etc/yum.repos.d/ftp.repo</span><br><span class="line">ADD zookeeper-3.4.14.tar.gz /opt</span><br><span class="line">RUN yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel \</span><br><span class="line">&amp;&amp; mv /opt/zookeeper-3.4.14/conf/zoo_sample.cfg /opt/zookeeper-3.4.14/conf/zoo.cfg</span><br><span class="line">CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;/opt/zookeeper-3.4.14/bin/zkServer.sh start &amp;&amp; tail -f /etc/shadow&quot;]</span><br></pre></td></tr></table></figure>

<p><code>vi ftp.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[docker]</span><br><span class="line">baseurl=http://172.19.25.11/paas/kubernetes-repo/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[centos]</span><br><span class="line">baseurl=ftp://192.168.20.132/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><code>cp /opt/ChinaskillMall/zookeeper-3.4.14.tar.gz /root/zookeeper/</code></p>
<p><code>docker build -t chinaskill-zookeeper:v1.1 .</code></p>
<h3 id="【题目4】容器化部署kafka"><a href="#【题目4】容器化部署kafka" class="headerlink" title="【题目4】容器化部署kafka"></a>【题目4】容器化部署kafka</h3><p>在 master 节 点 上 编 写 &#x2F;root&#x2F;kafka&#x2F;Dockerfile 文 件 构 建chinaskill-kafka:v1.1 镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；<br>（2）作者：Chinaskill；<br>（3）开放端口：9092;<br>（4）设置 服务开机自启。<br>完成后构建镜像，并提交master 节点的用户名、密码和IP 到答题框。</p>
<p><code>cd /root</code></p>
<p><code>mkdir kafka</code></p>
<p><code>cd kafka</code></p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">ADD zookeeper-3.4.14.tar.gz /opt</span><br><span class="line">ADD kafka_2.11-1.1.1.tgz /opt</span><br><span class="line">RUN yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel \</span><br><span class="line"> &amp;&amp; mv /opt/zookeeper-3.4.14/conf/zoo_sample.cfg /opt/zookeeper-3.4.14/conf/zoo.cfg</span><br><span class="line">EXPOSE 9092</span><br><span class="line">CMD [&quot;sh&quot;,&quot;-c&quot;,&quot;/opt/zookeeper-3.4.14/bin/zkServer.sh start &amp;&amp; /opt/kafka_2.11-1.1.1/bin/kafka-server-start.sh /opt/kafka_2.11-1.1.1/config/server.properties&quot;]</span><br></pre></td></tr></table></figure>

<p><code>cp /opt/ChinaskillMall/kafka_2.11-1.1.1.tgz /root/kafka/</code></p>
<p><code>cp /opt/ChinaskillMall/zookeeper-3.4.14.tar.gz /root/kafka/</code></p>
<p><code>cp /root/redis/local.repo /root/kafka</code></p>
<p><code>docker build -t chinaskill-kafka:v1.1 .</code></p>
<h3 id="【题目5】容器化部署nginx"><a href="#【题目5】容器化部署nginx" class="headerlink" title="【题目5】容器化部署nginx"></a>【题目5】容器化部署nginx</h3><p>在 master 节 点 上 编 写 &#x2F;root&#x2F;nginx&#x2F;Dockerfile 文 件 构 建chinaskill-nginx:v1.1 镜像，具体要求如下：</p>
<p>（1）基础镜像：centos:centos7.5.1804；<br>（2）作者：Chinaskill；<br>（3）开放端口 80 8081 8082 8083;<br>（4）设置 服务开机自启。<br>完成后构建镜像，并提交master 节点的用户名、密码和IP 到答题框。</p>
<p><code>cd /root</code></p>
<p><code>mkdir nginx</code></p>
<p><code>cd nginx</code></p>
<p><code>vi Dockerfile</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM centos:centos7.5.1804</span><br><span class="line">MAINTAINER Chinaskill</span><br><span class="line">RUN rm -rf /etc/yum.repos.d/*</span><br><span class="line">ADD local.repo /etc/yum.repos.d/</span><br><span class="line">ADD *.jar /root/</span><br><span class="line">ADD setup.sh /root/</span><br><span class="line">RUN yum install -y nginx java-1.8.0-openjdk java-1.8.0-openjdk-devel </span><br><span class="line">RUN sed -i &#x27;1a location /shopping &#123; proxy_pass http://127.0.0.1:8081; &#125;&#x27; /etc/nginx/conf.d/default.conf </span><br><span class="line">RUN sed -i &#x27;2a location /user &#123; proxy_pass http://127.0.0.1:8082; &#125;&#x27; /etc/nginx/conf.d/default.conf </span><br><span class="line">RUN sed -i &#x27;3a location /casher &#123; proxy_pass http://127.0.0.1:8083; &#125;&#x27; /etc/nginx/conf.d/default.conf </span><br><span class="line">RUN chmod +x /root/setup.sh </span><br><span class="line">RUN rm -rf /usr/share/nginx/html/</span><br><span class="line">EXPOSE 80 8081 8082 8083</span><br><span class="line">ADD dist/ /usr/share/nginx/html/</span><br><span class="line">CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]</span><br></pre></td></tr></table></figure>

<p><code>vi local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[centos]</span><br><span class="line">baseurl=ftp://192.168.20.132/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[docker]</span><br><span class="line">baseurl=ftp://192.168.20.132/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[gpmal]</span><br><span class="line">baseurl=http://172.19.25.11/paas/ChinaskillMall/gpmall-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><code>cp /opt/ChinaskillMall/*.jar /root/nginx/</code></p>
<p><code>cp /opt/ChinaskillMall/dist/ /root/nginx/</code></p>
<p><code>vi setup.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">nohup java -jar /root/shopping-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /root/user-provider-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /root/gpmall-shopping-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br><span class="line">nohup java -jar /root/gpmall-user-0.0.1-SNAPSHOT.jar &amp;</span><br><span class="line">sleep 5</span><br></pre></td></tr></table></figure>

<p><code>docker build -t chinaskill-nginx:v1.1 .</code></p>
<h3 id="【题目6】容器编排"><a href="#【题目6】容器编排" class="headerlink" title="【题目6】容器编排"></a>【题目6】容器编排</h3><p>在master 节点上编写&#x2F;root&#x2F;chinaskill&#x2F;docker-compose.yaml 文件，具体要求如下：<br>（1）容器1 名称：mall-mysql；镜像：chinaskill-mariadb:v1.1；端口映射：13306:3306；<br>（2）容器 2 名称：mall-redis；镜像：chinaskill-redis:v1.1;端口映射：16379:6379;</p>
<p>（3）容器3名称：mall-kafka; 镜像：chinaskill-kafka:v1.1;端口映射：19092:9092;</p>
<p>（4）容器4名称：mall-zookeeper；镜像：chinaskill-zookeeper:v1.1;端口映射：12181:2181;</p>
<p>（5）容器5名称：mall-nginx；镜像：chinaskill-nginx:v1.1;端口映射：83:80;1443:443;</p>
<p><code>cd /root</code></p>
<p><code>mkdir chinaskil</code></p>
<p><code>cd chinaskill</code></p>
<p><code>vi docker-compose.yaml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">version: &#x27;3.3&#x27;</span><br><span class="line">services:</span><br><span class="line">  mall-mysql:</span><br><span class="line">    image: chinaskill-mariadb:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 13306:3306</span><br><span class="line"></span><br><span class="line">  mall-redis:</span><br><span class="line">    image: chinaskill-redis:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 16379:6379</span><br><span class="line"></span><br><span class="line">  mall-kafka:</span><br><span class="line">    image: chinaskill-kafka:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 19092:9092</span><br><span class="line"></span><br><span class="line">  mall-zookeeper:</span><br><span class="line">    image: chinaskill-zookeeper:v1.1</span><br><span class="line">    ports:</span><br><span class="line">      - 12181:2181</span><br><span class="line"></span><br><span class="line">  mall-nginx:</span><br><span class="line">    image: chinaskill-nginx:v1.1</span><br><span class="line">    depends_on:</span><br><span class="line">      - mall-mysql</span><br><span class="line">      - mall-redis</span><br><span class="line">      - mall-zookeeper</span><br><span class="line">      - mall-kafka</span><br><span class="line">    links:</span><br><span class="line">      - mall-mysql:mysql.mall</span><br><span class="line">      - mall-redis:redis.mall</span><br><span class="line">      - mall-zookeeper:zookeeper.mall</span><br><span class="line">      - mall-kafka:kafka.mall</span><br><span class="line">    ports:</span><br><span class="line">      - 83:80</span><br><span class="line">      - 1443:443</span><br><span class="line">    command: [&quot;sh&quot;,&quot;-c&quot;,&quot;/root/setup.sh &amp;&amp; nginx &amp;&amp; tail -f /etc/shadow&quot;]</span><br></pre></td></tr></table></figure>

<p><code>docker-compose up -d</code></p>
<p>查看是否开启成功</p>
<p><code>docker ps</code></p>
]]></content>
      <tags>
        <tag>容器云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-容器云-任务3</title>
    <url>/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/</url>
    <content><![CDATA[<h1 id="云计算赛项第二场-容器云-江苏卷-任务3"><a href="#云计算赛项第二场-容器云-江苏卷-任务3" class="headerlink" title="云计算赛项第二场-容器云-江苏卷-任务3"></a>云计算赛项第二场-容器云-江苏卷-任务3</h1><h2 id="【任务3】基于Kubernetes-构建持续集成"><a href="#【任务3】基于Kubernetes-构建持续集成" class="headerlink" title="【任务3】基于Kubernetes 构建持续集成"></a>【任务3】基于Kubernetes 构建持续集成<span id="more"></span></h2><h3 id="【题目1】安装Jenkins-环境-2-分"><a href="#【题目1】安装Jenkins-环境-2-分" class="headerlink" title="【题目1】安装Jenkins 环境[2 分]"></a>【题目1】安装Jenkins 环境[2 分]</h3><p>在master 节点上使用镜像jenkins&#x2F;Jenkins:2.262-centos部署Jenkins 服务，具体要求如下：<br>（1）容器名称：jenkins；<br>（2）端口映射：8080:8080；<br>（3）使用root 身份生成容器；<br>（4）离线安装Jenkins 插件；<br>（5）设置Jenkins 用户：chinaskill；密码：000000；<br>（6）在授权策略中配置“任何用户可以做任何事(没有任何限制)”。<br>使用chinaskill 用户登录Jenkins，完成后提交master 节点的用户名、密码和IP 到答题框。</p>
<p><strong>创建挂载目录</strong></p>
<p><code>mkdir -p /home/jenkins_home</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d --name jenkins -p 8080:8080 -u root \</span><br><span class="line">-v /home/jenkins_home/:/var/jenkins_home \</span><br><span class="line">-v /var/run/docker.sock:/var/run/docker.sock \</span><br><span class="line">-v /usr/bin/docker:/usr/bin/docker \</span><br><span class="line">-v /usr/bin/kubectl:/usr/local/bin/kubectl \</span><br><span class="line">-v /root/.kube/:/root/.kube/ \</span><br><span class="line">--restart always jenkins/jenkins:2.262-centos</span><br></pre></td></tr></table></figure>

<p><strong>离线安装插件</strong></p>
<p><code>cp -rf /opt/plugins/* /home/jenkins_home/plugins/</code></p>
<p><strong>重启容器</strong></p>
<p><code>docker restart jenkins</code></p>
<p><strong>进入容器</strong></p>
<p><code>docker exec -it jenkins bash</code></p>
<p><strong>查看密码</strong></p>
<p><code>cat /var/jenkins_home/secrets/initialAdminPassword</code> </p>
<p><strong>浏览器输入192.168.20.132:8080</strong></p>
<p> 输入密码&gt;选择插件来安装&gt;取消所有选择&gt;安装</p>
<h3 id="【题目2】安装Gitlab-环境-1-分"><a href="#【题目2】安装Gitlab-环境-1-分" class="headerlink" title="【题目2】安装Gitlab 环境[1 分]"></a>【题目2】安装Gitlab 环境[1 分]</h3><p>在master 节点上使用镜像gitlab&#x2F;gitlab-ce:12.9.2-ce.0部署Gitlab 服务，具体要求如下：<br>（1）容器名称：gitlab；<br>（2）端口映射：1022:22、81:80、443:443；<br>（3）容器重启策略：always；<br>（4）设置root 用户及密码；<br>（5）使用root 用户登录Gitlab，密码：00000000；<br>（6）新建项目Springcloud，将&#x2F;opt&#x2F;ChinaskillProject 中的代码上传到ChinaskillProject 项目中。<br>完成后提交master 节点的用户名、密码和IP 到答题框。</p>
<p><strong>创建挂载目录</strong></p>
<p><code>mkdir -k /home/gitlab&#123;config,logs,data&#125;</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -h gitlab -p 1022:22 -p 81:80 -p 443:443 \</span><br><span class="line">-v /home/gitlab/config/:/etc/gitlab \</span><br><span class="line">-v /home/gitlab/logs/:/var/log/gitlab \</span><br><span class="line">-v /home/gitlab/data/:/var/opt/gitlab \</span><br><span class="line">--restart always --privileged=true \</span><br><span class="line">--name mygitlab gitlab/gitlab-ce:12.9.2-ce.0</span><br></pre></td></tr></table></figure>

<p><strong>确实密码为00000000</strong></p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab.jpg" class="">

<p><strong>用户名为root，密码为00000000</strong></p>
<p><strong>新建项目</strong></p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab2.jpg" class="">

<p><strong>项目名为ChinaskillProject</strong></p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab3.jpg" class="">

<p> <strong>安装git</strong></p>
<p><code>yum install -y git</code></p>
<p><code>git config --global user.name &quot;Administrator&quot;</code></p>
<p><code>git config --global user.email &quot;admin@example.com&quot;</code></p>
<p><code>git clone http://192.168.20.132:443/root/chinaskillproject.git</code></p>
<p>查看是否克隆到本地</p>
<p><strong>将&#x2F;opt&#x2F;ChinaskillProject 中的代码上传到ChinaskillProject 项目中。</strong></p>
<p><code>cd chinaskillproject</code></p>
<p><code>cp -rf /opt/ChinaskillProject/* /root/chinaskillproject/</code></p>
<p><code>git add .</code></p>
<p><code>git commit -m &quot;add README&quot;</code></p>
<p><code>git push -u origin master</code></p>
<h3 id="【题目3】配置Jenkins-连接Gitlab-1-分"><a href="#【题目3】配置Jenkins-连接Gitlab-1-分" class="headerlink" title="【题目3】配置Jenkins 连接Gitlab[1 分]"></a>【题目3】配置Jenkins 连接Gitlab[1 分]</h3><p>配置Jenkins 连接Gitlab，具体要求如下：<br>（1）设置Outbound requests；<br>（2）生成“Access Tokens”并命名为jenkins；<br>（3）设置Jenkins 取消对’&#x2F;project’ end-point 进行身份验证；<br>（4）测试Jenkins 与Gitlab 的连通性。<br>完成后提交master 节点的用户名、密码和IP 到答题框。</p>
<p><strong>设置Outbound requests</strong></p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab4.jpg" class="">

<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab5.jpg" class="">

<h3 id="【题目3】配置Jenkins-连接Gitlab-1-分-1"><a href="#【题目3】配置Jenkins-连接Gitlab-1-分-1" class="headerlink" title="【题目3】配置Jenkins 连接Gitlab[1 分]"></a>【题目3】配置Jenkins 连接Gitlab[1 分]</h3><p>配置Jenkins 连接Gitlab，具体要求如下：<br>（1）设置Outbound requests；<br>（2）生成“Access Tokens”并命名为jenkins；<br>（3）设置Jenkins 取消对’&#x2F;project’ end-point 进行身份验证；<br>（4）测试Jenkins 与Gitlab 的连通性。<br>完成后提交master 节点的用户名、密码和IP 到答题框。</p>
<p><strong>设置Outbound requests</strong></p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab6.jpg" class="">

<p>复制访问令牌</p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab7.jpg" class="">

<p>测试Jenkins 与Gitlab 的连通性</p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab8.jpg" class="">

<p>访问令牌复制到API token</p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab9.jpg" class="">

<p>点击Test Connection</p>
<h3 id="4-配置Jenkins连接maven"><a href="#4-配置Jenkins连接maven" class="headerlink" title="4.配置Jenkins连接maven"></a>4.配置Jenkins连接maven</h3><p>采用docker in docker的方式在Jenkins内安装maven</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cp  /opt/ChinaskillMall/apache-maven-3.6.3-bin.tar.gz /root/jenkins/</span><br><span class="line">[root@master ~]#  docker exec -it jenkins bash</span><br><span class="line">[root@b957b62337cc]# tar -zxvf /var/jenkins_home/apache-maven-3.6.3-bin.tar.gz -C .</span><br><span class="line">[root@2927036f2450]# mv apache-maven-3.6.3 /usr/local/maven</span><br><span class="line">//配置maven环境变量</span><br><span class="line">[root@b957b62337cc]# vi /etc/profile</span><br><span class="line">export  M2_HOME=/usr/local/maven</span><br><span class="line">export  PATH=$PATH:$M2_HOME/bin</span><br><span class="line">[root@2927036f2450  /]# vim /root/.bashrc</span><br><span class="line">if [ -f /etc/bashrc  ]; then</span><br><span class="line">        . /etc/bashrc</span><br><span class="line">        source  /etc/profile</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>在Jenkins中配置maven信息</p>
<p>Dashboard-全局工具配置</p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab10.jpg" class="">

<h3 id="5-配置CI-x2F-CD"><a href="#5-配置CI-x2F-CD" class="headerlink" title="5.配置CI&#x2F;CD"></a>5.配置CI&#x2F;CD</h3><p>新建流水线任务</p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab11.jpg" class="">

<p>勾选Build when a change is pushed to GitLab. GitLab webhook URL: <a href="http://192.168.20.102:8080/project/ChinaskillProject">http://192.168.20.102:8080/project/ChinaskillProject</a></p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab12.jpg" class="">

<p>复制 secret token</p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab13.jpg" class="">

<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab14.jpg" class="">

<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab15.jpg" class="">

<p>生成ssh密钥</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cd ChinaskillProject/</span><br><span class="line">[root@master ChinaskillProject]# ssh-keygen </span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">/root/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? </span><br><span class="line">[root@master ChinaskillProject]# cd ..</span><br><span class="line">[root@master ~]# cat /root/.ssh/id_rsa</span><br><span class="line">-----BEGIN RSA PRIVATE KEY-----</span><br><span class="line">MIIEowIBAAKCAQEAz5EcPpwUTM4IPR1wpi2u7l3nelQi29eXjLXhQI9swCgHxu+M</span><br><span class="line">tAl0kQJvFOPAg7DUxTjForAVAMkzZljDd4uTV+VCairuFeXApyM+LokxZkvJsvsH</span><br><span class="line">upXwVqvGEifGP+tQbC9tPhIQ2a1aRZBiSYetuDjZdjwQcxgARfgJjqeH7Rw5red8</span><br><span class="line">V4e7VgF8e6RBVJycSVqfuysxyFVBsUCS1vHAK7GvjmfoUJ/t8mv4s/ClL+PzzZe4</span><br><span class="line">dUO3OftAKU6qIFrpf6kC57/LsZmNWWBy9suYXpK+92Yo8cOwtBkdBtGJSo5EblzA</span><br><span class="line">hUQjSftoC9SuTw3wN6DwwV5RcqUe99vDYVSVowIDAQABAoIBACB4Rr7Uk6hgIpHM</span><br><span class="line">5S5Zvx+yuN8+AiX5188/NHlw9kYo/O510sDnKcD1bIMMDbWMF/yyINiOSvrQfqXo</span><br><span class="line">BMvSnb4GMxmAlbX/Nt/ud2+4Lm4eNPM2qcLaabplLJrydeTR3sbNtH0dPl2O7Kwt</span><br><span class="line">JfLyso469mm0g649du17G6OhOXYxyATNMX7AnNFRpH5zKft+FhSpVNxdAoOEYMKt</span><br><span class="line">1ucUf5GoWCnbrl0caaXyLqlRGAuTa3+EqlaQXhOv7cq5L13v6xbSDpCHh0krnKk6</span><br><span class="line">G/4Ui+styCVM7Dm0imMztRGULE6pIwnV/MLeDYBfhSjWckXm1ofgt3TPDQ+vQqqW</span><br><span class="line">yqrtz+ECgYEA6DRDdirL+pA2jf0yZR2v60L3+AIUoVyn3EkqTVmiEqEDGj1drK8t</span><br><span class="line">U9/f/QiTPVIlq17GN6iErvMCM9E11/d0EKTiTDe2T4dmu7fEL5KUpndMCJeHZzL1</span><br><span class="line">Ythq5AVfnfxShJx+RI5VY/p+VLNWSCcIIFenK4BBC7O12o74ENOHeS8CgYEA5NaA</span><br><span class="line">aI+QnUQVTJTTvBX2JB8SNh0fyArRi/wmA3Gj0CWz1OMSnhhwVeS0a6aTm7BPVgsh</span><br><span class="line">kBnjX6xnYccXq4bo0bHLrFXKXRVzTCquNjPCCC8mRG5TlO6hdFHKLuKmaRZc3jW5</span><br><span class="line">9kzC/Ub5F4gl0zhGoBAjVqVVj0qQHrUp4kFHZc0CgYA5nF3wf4XEUOt4Jw3N0KPI</span><br><span class="line">9wCW481Ci76KBQ0dy9NOU+x1IiyPn5bbbHwiR3JgkHUQI5+CR6lDzr8JJCr7vkVp</span><br><span class="line">q78Jsfjlmm5+vIVMWE3RT1/p8c6CaD5Bm/hJQpwKkoPWqw0Y7Ud6WrflQ+lwVV7K</span><br><span class="line">x3SOcm+w+5Fu/HHHmvPNuQKBgHYpDETxXIrYDOduMaZ88IvWP2ERyMdSTsEpgbdl</span><br><span class="line">hmCEF0dluMdJdzfi0AaGDNHnSA/1z27I708HhppGY0J+dtlQJQrngYZu/QnnP+1c</span><br><span class="line">7GOtdOUA0sekMFoPoYZ0IdlBYtKai1tSZ5zAeP5dnqph9JXRv22OEY/bwG8avHXH</span><br><span class="line">n0ZBAoGBAJc5nHZXXFCLiBw6af+3yOJXWzH6D8XqHbedoCEg9gKIld9mL5XKvX7S</span><br><span class="line">5ul60aLUrOS/CTrPNVaeD9RI+mLFevZ72cGfB/+MYaeT7+IjtzxjbZixyCl8K1An</span><br><span class="line">F15HoJX0BWon5FTqp06Fdy3Y4fPgqwwO2/IdAkOGi8AxKftwWLfC</span><br><span class="line">-----END RSA PRIVATE KEY-----</span><br></pre></td></tr></table></figure>

<p>复制密钥</p>
<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab16.jpg" class="">

<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab17.jpg" class="" title="]()![gitlab17">

<p>写流水线脚本</p>
<p>构建微服务项目中的gateway和config服务,</p>
<p>将构建后的镜像自动上传到Harbor仓库，</p>
<p>并自动发布gateway和config服务到Kubernetes集群的springcloud命名空间下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">node&#123;</span><br><span class="line">    stage(&#x27;git clone&#x27;)&#123;</span><br><span class="line">        //check CODE 从代码仓库克隆</span><br><span class="line">        git credentialsId: &#x27;7fc32029-1f4f-4b77-a161-47dc4dae39ef&#x27;, url: &#x27;http://192.168.20.102:81/root/ChinaskillProject.git&#x27;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">stage(&#x27;maven build&#x27;)&#123;</span><br><span class="line"></span><br><span class="line">//maven编译</span><br><span class="line">        sh  &#x27;&#x27;&#x27;/usr/local/maven/bin/mvn package -DskipTests -f  /var/jenkins_home/workspace/ChinaskillProject&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">stage(&#x27;image build&#x27;)&#123;</span><br><span class="line"></span><br><span class="line">//构建镜像，并将镜像ID变量作为tag</span><br><span class="line">        sh &#x27;&#x27;&#x27;</span><br><span class="line">              echo $BUILD_ID</span><br><span class="line">              docker build -t 192.168.20.102/chinaskillproject/gateway:$BUILD_ID -f  /var/jenkins_home/workspace/ChinaskillProject/gateway/Dockerfile   /var/jenkins_home/workspace/ChinaskillProject/gateway</span><br><span class="line">              docker build -t 192.168.20.102/chinaskillproject/config:$BUILD_ID -f  /var/jenkins_home/workspace/ChinaskillProject/config/Dockerfile   /var/jenkins_home/workspace/ChinaskillProject/config&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">stage(&#x27;upload registry&#x27;)&#123;</span><br><span class="line"></span><br><span class="line">//上传构建完成的镜像到harbor镜像仓库</span><br><span class="line">        sh &#x27;&#x27;&#x27;docker login  192.168.20.102 -u=admin -p=Harbor12345</span><br><span class="line">            docker push 192.168.20.102/chinaskillproject/gateway:$BUILD_ID</span><br><span class="line">            docker push 192.168.20.102/chinaskillproject/config:$BUILD_ID&#x27;&#x27;&#x27;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">stage(&#x27;deploy k8s&#x27;)&#123;</span><br><span class="line">    //创建命名空间，部署yaml脚本到k8s集群</span><br><span class="line">   sh &#x27;sed -i &quot;s/sqshq\\/piggymetrics-gateway/192.168.20.102\\/chinaskillproject\\/gateway:$BUILD_ID/g&quot; /var/jenkins_home/workspace/ChinaskillProject/yaml/deployment/gateway-deployment.yaml&#x27; </span><br><span class="line">   sh &#x27;sed -i &quot;s/sqshq\\/piggymetrics-config/192.168.20.102\\/chinaskillproject\\/config:$BUILD_ID/g&quot; /var/jenkins_home/workspace/ChinaskillProject/yaml/deployment/config-deployment.yaml&#x27; </span><br><span class="line">   sh &#x27;kubectl create ns springcloud&#x27;</span><br><span class="line">   sh &#x27;kubectl apply -f  /var/jenkins_home/workspace/ChinaskillProject/yaml/deployment/gateway-deployment.yaml  --kubeconfig=/root/.kube/config&#x27;</span><br><span class="line">   sh &#x27;kubectl apply -f  /var/jenkins_home/workspace/ChinaskillProject/yaml/deployment/config-deployment.yaml  --kubeconfig=/root/.kube/config&#x27;</span><br><span class="line">   sh &#x27;kubectl apply -f  /var/jenkins_home/workspace/ChinaskillProject/yaml/svc/gateway-svc.yaml --kubeconfig=/root/.kube/config&#x27;</span><br><span class="line">   sh &#x27;kubectl apply -f  /var/jenkins_home/workspace/ChinaskillProject/yaml/svc/config-svc.yaml --kubeconfig=/root/.kube/config&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab18.jpg" class="">

<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab20.jpg" class="">

<img src="/2022/04/09/%E4%BA%91%E8%AE%A1%E7%AE%97-%E5%AE%B9%E5%99%A8%E4%BA%91-%E4%BB%BB%E5%8A%A13/gitlab21.jpg" class="">
]]></content>
      <tags>
        <tag>容器云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-私有云</title>
    <url>/2022/03/29/%E4%BA%91%E8%AE%A1%E7%AE%97-%E7%A7%81%E6%9C%89%E4%BA%91-%E4%BB%BB%E5%8A%A11/</url>
    <content><![CDATA[<h1 id="云计算赛项第一场-私有云"><a href="#云计算赛项第一场-私有云" class="headerlink" title="云计算赛项第一场-私有云"></a>云计算赛项第一场-私有云</h1><h2 id="云计算-私有云-基础运维任务"><a href="#云计算-私有云-基础运维任务" class="headerlink" title="云计算-私有云-基础运维任务"></a>云计算-私有云-基础运维任务<span id="more"></span></h2><p><strong>某企业拟使用OpenStack 搭建一个企业云平台，以实现资源池化弹性管理、企业应用集</strong><br><strong>中管理、统一安全认证和授权等管理。</strong></p>
<table>
<thead>
<tr>
<th>设备名称</th>
<th>主机名</th>
<th>接口</th>
<th>IP 地址</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>云服务器1</td>
<td>Controller</td>
<td>eth0 <br/>eth1</td>
<td>172.129.x.0&#x2F;24 <br/>自定义</td>
<td>Vlan x<br/>自行创建</td>
</tr>
<tr>
<td>云服务器2</td>
<td>Compute</td>
<td>eth0<br/>eth1</td>
<td>172.129.x.0&#x2F;24 <br/>自定义</td>
<td>Vlan x<br/>自行创建</td>
</tr>
<tr>
<td>云服务器3<br/>…<br/>云服务器n</td>
<td>自定义</td>
<td>eth0</td>
<td>172.129.x.0&#x2F;24</td>
<td>用于实操题</td>
</tr>
<tr>
<td>PC-1</td>
<td></td>
<td>本地连接</td>
<td>172.24.16.0&#x2F;24</td>
<td>PC 使用</td>
</tr>
</tbody></table>
<p>说明：<br>1.竞赛使用集群模式进行，比赛时给每个参赛队提供独立的租户与用户，各用户的资源配额相同，选手通过用户名与密码登录竞赛用私有云平台，创建云主机进行相应答题；<br>2.表中的x 为工位号；在进行OpenStack 搭建时的第二块网卡地址根据题意自行创建；<br>3.根据图表给出的信息，检查硬件连线及网络设备配置，确保网络连接正常；<br>4.考试所需要的账号资源、竞赛资源包与附件均会在考位信息表与设备确认单中给出；<br>5.竞赛过程中，为确保服务器的安全，请自行修改服务器密码；在考试系统提交信息时，请确认自己的IP 地址，用户名和密码。</p>
<h2 id="基础运维任务-5-分"><a href="#基础运维任务-5-分" class="headerlink" title="基础运维任务[5 分]"></a>基础运维任务[5 分]</h2><p><strong>【适用平台】私有云</strong></p>
<h3 id="【题目1】基础环境配置-1-5-分"><a href="#【题目1】基础环境配置-1-5-分" class="headerlink" title="【题目1】基础环境配置[1.5 分]"></a>【题目1】基础环境配置[1.5 分]</h3><p>使用提供的用户名密码，登录提供的OpenStack 私有云平台，自行使用CentOS7.5 镜像创建两台云主机，控制节点 flavor 使用 4v_12G_100G 的配置，计算节点 flavor 使用4v_8G_100G_50G 的配置。第一张网卡使用提供的网络，第二张网卡使用的网络自行创（网段为10.10.X.0&#x2F;24，X 为工位号）。创建完云主机后确保网络正常通信，然后按以下要求配置服务器：<br><strong>（1）设置控制节点主机名为controller，设置计算节点主机名为compute；</strong></p>
<p><code>hostnamesctl set-hostname controller</code></p>
<p><code>hostnamesctl set-hostname compute</code></p>
<p><strong>（2）修改hosts 文件将IP 地址映射为主机名；</strong></p>
<p><strong>controller&#x2F;compute</strong></p>
<p><code>vi /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">192.168.20.123	controller</span><br><span class="line"></span><br><span class="line">192.168.20.110	compute</span><br></pre></td></tr></table></figure>

<p>完成后提交控制节点的用户名、密码和IP 地址到答题框。<br>1.使用hostnamectl 命令查看主机名为controller 计0.5 分</p>
<p><code>hostnamectl</code></p>
<p>2.查看hosts 文件中有正确的主机名和IP 映射计0.5 分</p>
<p><code>cat /etc/hosts</code></p>
<p>3.控制节点正确使用两块网卡计0.5 分</p>
<p><code>cat /etc/sysconfig/network-scripts/ifcfg-enp*</code></p>
<h3 id="【题目2】Yum-源配置-1-分"><a href="#【题目2】Yum-源配置-1-分" class="headerlink" title="【题目2】Yum 源配置[1 分]"></a>【题目2】Yum 源配置[1 分]</h3><p>使用提供的http 服务地址，在http 服务下，存在centos7.5 和iaas 的网络yum 源，使用该http 源作为安装iaas 平台的网络源。分别设置controller 节点和compute 节点的yum 源文件http.repo。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller&#x2F;compute</strong></p>
<p><code>mv /etc/yum.repos.d/* /tmp/</code></p>
<p><code>vi /etc/yum.repos.d/http.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[centos]</span><br><span class="line"></span><br><span class="line">name=centos</span><br><span class="line"></span><br><span class="line">baseurl=http://172.19.25.11/centos</span><br><span class="line"></span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[iaas]</span><br><span class="line"></span><br><span class="line">name=iaas</span><br><span class="line"></span><br><span class="line">baseurl=http://172.19.25.11/iaas/iaas/iaas-repo</span><br><span class="line"></span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p>1.查看&#x2F;etc&#x2F;yum.repos.d&#x2F;http.repo 文件，有正确的baseurl 路径，计1 分</p>
<p><code>cat /etc/yum.repos.d/http.repo</code></p>
<p>清除缓存</p>
<p><code>yum clean</code>        </p>
<p>检验yum是否配置正确</p>
<p>&#96;&#96;yum list&#96;        </p>
<h3 id="【题目3】时间同步配置-1-5-分"><a href="#【题目3】时间同步配置-1-5-分" class="headerlink" title="【题目3】时间同步配置[1.5 分]"></a>【题目3】时间同步配置[1.5 分]</h3><p>在controller 节点上部署chrony 服务器，允许其他节点同步时间，启动服务并设置为开机启动；在compute 节点上指定controller 节点为上游NTP 服务器，重启服务并设为开机启动。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><code>yum install -y chrony</code></p>
<p><strong>配置controller节点</strong></p>
<p><code>vi /etc/chrony.conf</code></p>
<p>删除默认规则（在默认规则前加上#号）</p>
<p>添加以下规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server controller iburst</span><br><span class="line"></span><br><span class="line">allow 192.168.20.0/24</span><br><span class="line"></span><br><span class="line">local stratum 10</span><br></pre></td></tr></table></figure>

<p><strong>配置compute节点</strong></p>
<p><code>vi /etc/chrony.conf</code></p>
<p>删除默认规则（在默认规则前加上#号）</p>
<p>添加以下规则</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server controller iburst</span><br></pre></td></tr></table></figure>

<p>启动服务并设置为开机启动</p>
<p><code>systemctl restart chronyd</code></p>
<p><code>systemctl enable chronyd</code></p>
<p>1.查看&#x2F;etc&#x2F;chrony.conf 配置文件，有正确的配置文件计1 分</p>
<p><code>cat /etc/chrony.conf</code></p>
<p>2.查看时间同步服务的状态，正常计0.5 分</p>
<p><code>systemctl status chronyd</code></p>
<h3 id="【题目4】计算节点分区-1-分"><a href="#【题目4】计算节点分区-1-分" class="headerlink" title="【题目4】计算节点分区[1 分]"></a>【题目4】计算节点分区[1 分]</h3><p>在compute 节点上利用空白分区划分2 个20G 分区。完成后提交计算节点的用户名、密码和IP 地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# umount /dev/vdb</span><br><span class="line"></span><br><span class="line">[root@compute ~]# fdisk /dev/vdb</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line"></span><br><span class="line">Changes will remain in memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write command.</span><br><span class="line"></span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p</span><br><span class="line">Using default response p</span><br><span class="line">Partition number (1-4, default 1): 1</span><br><span class="line">First sector (2048-125829119, default 2048): </span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-125829119, default 125829119): +20G</span><br><span class="line">Partition 1 of type Linux and of size 20 GiB is set</span><br><span class="line"></span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): p</span><br><span class="line">Partition number (2-4, default 2): 2</span><br><span class="line">First sector (41945088-125829119, default 41945088): </span><br><span class="line">Using default value 41945088</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (41945088-125829119, default 125829119): +20G</span><br><span class="line">Partition 2 of type Linux and of size 20 GiB is set</span><br><span class="line"></span><br><span class="line">Command (m for help): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br></pre></td></tr></table></figure>

<p>1.使用lsblk 命令查看分区，有两个20G 分区计1 分，每个0.5 分</p>
<p><code>lsblk</code></p>
<p><strong>云计算-私有云-基础运维任务结束</strong></p>
]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-私有云-任务2</title>
    <url>/2022/03/30/%E4%BA%91%E8%AE%A1%E7%AE%97-%E7%A7%81%E6%9C%89%E4%BA%91-%E4%BB%BB%E5%8A%A12/</url>
    <content><![CDATA[<h1 id="云计算赛项第一场-私有云"><a href="#云计算赛项第一场-私有云" class="headerlink" title="云计算赛项第一场-私有云"></a>云计算赛项第一场-私有云</h1><h2 id="【任务2】OpenStack-搭建任务-10-分"><a href="#【任务2】OpenStack-搭建任务-10-分" class="headerlink" title="【任务2】OpenStack 搭建任务[10 分]"></a>【任务2】OpenStack 搭建任务[10 分]</h2><span id="more"></span>

<h3 id="【题目1】基础安装-1-分"><a href="#【题目1】基础安装-1-分" class="headerlink" title="【题目1】基础安装[1 分]"></a>【题目1】基础安装[1 分]</h3><p>任务需要完成openstack 平台的安装搭建及运维任务（无安装脚本）</p>
<p>使用提供的脚本框架iaas-pre-host.sh 和openrc.sh 环境变量文件，填充脚本（只需填充中文注释下的内容），在controller 和compute 节点上分别安装openstack 平台的基础组件并完成相关配置。安装完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><code>yum install -y iaas-xiandian</code></p>
<p>编辑文件&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh,此文件是安装过程中的各项参数，根据每项参数上一行的说明及服务器实际情况进行配置。</p>
<p><code>vi /etc/xiandian/openrc.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">HOST_IP=192.168.20.123</span><br><span class="line"></span><br><span class="line">HOST_PASS=000000</span><br><span class="line"></span><br><span class="line">HOST_NAME=controller</span><br><span class="line"></span><br><span class="line">HOST_IP_NODE=192.168.20.110</span><br><span class="line"></span><br><span class="line">HOST_PASS_NODE=000000</span><br><span class="line"></span><br><span class="line">HOST_NAME_NODE=compute</span><br><span class="line"></span><br><span class="line">network_segment_IP=192.168.20.0/24</span><br><span class="line"></span><br><span class="line">RABBIT_USER=openstack</span><br><span class="line"></span><br><span class="line">RABBIT_PASS=000000</span><br><span class="line"></span><br><span class="line">DB_PASS=000000</span><br><span class="line"></span><br><span class="line">DOMAIN_NAME=demo</span><br><span class="line"></span><br><span class="line">ADMIN_PASS=000000</span><br><span class="line"></span><br><span class="line">DEMO_PASS=000000</span><br><span class="line"></span><br><span class="line">KEYSTONE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">GLANCE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">GLANCE_PASS=000000</span><br><span class="line"></span><br><span class="line">NOVA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">NOVA_PASS=000000</span><br><span class="line"></span><br><span class="line">NEUTRON_DBPASS=000000</span><br><span class="line"></span><br><span class="line">NEUTRON_PASS=000000</span><br><span class="line"></span><br><span class="line">METADATA_SECRET=000000</span><br><span class="line"></span><br><span class="line">INTERFACE_IP=192.168.20.123/192.168.20.110（controllerIP/computeIP）</span><br><span class="line"></span><br><span class="line">INTERFACE_NAME=enp9s0 （外部网络网卡名称）</span><br><span class="line"></span><br><span class="line">Physical_NAME=provider （外部网络适配器名称）</span><br><span class="line"></span><br><span class="line">minvlan=101 （vlan网络范围的第一个vlanID）</span><br><span class="line"></span><br><span class="line">maxvlan=200 （vlan网络范围的最后一个vlanID）</span><br><span class="line"></span><br><span class="line">CINDER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">CINDER_PASS=000000</span><br><span class="line"></span><br><span class="line">BLOCK_DISK=vdb1 （空白分区）</span><br><span class="line"></span><br><span class="line">SWIFT_PASS=000000</span><br><span class="line"></span><br><span class="line">OBJECT_DISK=vdb2 （空白分区）</span><br><span class="line"></span><br><span class="line">STORAGE_LOCAL_NET_IP=192.168.20.110</span><br><span class="line"></span><br><span class="line">HEAT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">HEAT_PASS=000000</span><br><span class="line"></span><br><span class="line">ZUN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">ZUN_PASS=000000</span><br><span class="line"></span><br><span class="line">KURYR_DBPASS=000000</span><br><span class="line"></span><br><span class="line">KURYR_PASS=000000</span><br><span class="line"></span><br><span class="line">CEILOMETER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">CEILOMETER_PASS=000000</span><br><span class="line"></span><br><span class="line">AODH_DBPASS=000000</span><br><span class="line"></span><br><span class="line">AODH_PASS=000000</span><br></pre></td></tr></table></figure>

<p><strong>controller&#x2F;compute</strong></p>
<p>执行脚本</p>
<p><code>iaas-pre-host.sh</code></p>
<h3 id="【题目2】数据库与基础服务安装-1-分"><a href="#【题目2】数据库与基础服务安装-1-分" class="headerlink" title="【题目2】数据库与基础服务安装[1 分]"></a>【题目2】数据库与基础服务安装[1 分]</h3><p>使用提供的脚本框架iaas-install-mysql.sh，填充脚本（只需填充中文注释下的内容），在controller 节点上安装mariadb、mencached、rabbitmq 等服务并完成相关配置。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-mysql.sh</code></p>
<p><strong>进入数据库</strong></p>
<p><code>mysql -uroot -p000000</code></p>
<p><strong>创建chinaskilldb数据库</strong></p>
<p><code>create database chinaskilldb;</code></p>
<p><strong>进入库</strong></p>
<p><code>use chinaskilldb;</code></p>
<p><strong>创建testable表</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">create table testable(</span><br><span class="line">    -&gt; id int not null primary key,</span><br><span class="line">    -&gt; Teamname varchar(50),</span><br><span class="line">    -&gt; remarks varchar(255));</span><br></pre></td></tr></table></figure>

<p><strong>插入数据</strong></p>
<p><code>insert into testable values(1,&quot;cloud&quot;,&quot;chinaskill&quot;);</code></p>
<p><strong>将memcached的缓存大小从64Mib改成256Mib。</strong></p>
<p><code>sed -i &#39;s/64/256/g&#39; /etc/sysconfig/memcached</code> </p>
<p><strong>使用rabbitmq命令 创建用户，并设置Administrators限权</strong></p>
<p><code>rabbitmqctl add_user chinaskill 000000</code></p>
<p><code>rabbitmqctl set_permissions chinaskill &quot;.*&quot; &quot;.*&quot; &quot;.*&quot;</code></p>
<h3 id="【题目3】Keystone-服务安装-1-分"><a href="#【题目3】Keystone-服务安装-1-分" class="headerlink" title="【题目3】Keystone 服务安装[1 分]"></a>【题目3】Keystone 服务安装[1 分]</h3><p>使用提供的脚本框架iaas-install-keystone.sh，填充脚本（只需填充中文注释下的内容），在controller 节点上安装keystone 服务并完成相关配置。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong></p>
<p><code>iaas-install-keystone.sh</code></p>
<p><strong>创建一个用户</strong></p>
<p><code>source /etc/keystone/admin-openrc.sh</code> </p>
<p><code>openstack user create --domain demo --password 000000 chinaskill</code></p>
<h3 id="【题目4】Glance-安装-1-分"><a href="#【题目4】Glance-安装-1-分" class="headerlink" title="【题目4】Glance 安装[1 分]"></a>【题目4】Glance 安装[1 分]</h3><p>使用提供的脚本框架iaas-install-glance.sh，填充脚本（只需填充中文注释下的内容），在controller 节点上安装glance 服务并完成相关配置。安装完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-glance.sh</code></p>
<p><strong>上传一个镜像</strong></p>
<p><code>*glance image-create --name cirros --disk-format qcow2 --container bare --progress &lt; openstack/cirros-0.3.4-x86_64-disk.img*</code> </p>
<h3 id="【题目5】Nova-安装-1-分"><a href="#【题目5】Nova-安装-1-分" class="headerlink" title="【题目5】Nova 安装[1 分]"></a>【题目5】Nova 安装[1 分]</h3><p>使用提供的脚本框架iaas-install-nova-controller.sh 和iaas-install-nova-compute.sh，填充脚本（只需填充中文注释下的内容），在controller 和compute 节点上安装nova 服务并完成配置。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-nova-controller.sh</code></p>
<p><strong>创建一个测试云主机类型</strong></p>
<p><code>[root@controller ~]*# openstack flavor create --id 1 --disk 20 --ram 1024 test</code></p>
<p><strong>compute</strong> </p>
<p><code>iaas-install-nova-compute.sh</code></p>
<h3 id="【题目6】Neutron-安装-1-分"><a href="#【题目6】Neutron-安装-1-分" class="headerlink" title="【题目6】Neutron 安装[1 分]"></a>【题目6】Neutron 安装[1 分]</h3><p>使用提供的脚本框架iaas-install-neutron-controller.sh和iaas-install-neutron-compute.sh，填充脚本（只需填充中文注释下的内容），在controller 和compute 节点上安装neutron 服务并完成配置。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-neutron-controller</code></p>
<p><strong>创建云主机网络extnet</strong></p>
<p><code>openstack network create --share --external --provider-physical-network provider --provider-network-type vlan extnet</code></p>
<p><strong>子网extsubnet</strong></p>
<p><code>openstack subnet create --network extnet --gateway=192.168.200.1 --subnet-range 192.168.200.0/24 extsubnet</code></p>
<p>这里也可以图形化操作</p>
<p><strong>compute</strong> </p>
<p><code>iaas-install-neutron-compute.sh</code></p>
<h3 id="【题目7】Doshboard-安装-1-分"><a href="#【题目7】Doshboard-安装-1-分" class="headerlink" title="【题目7】Doshboard 安装[1 分]"></a>【题目7】Doshboard 安装[1 分]</h3><p>使用提供的脚本框架iaas-install-dashboard.sh，填充脚本（只需填充中文注释下的内容），在controller 节点上安装dashboard 服务并完成相关配置。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-dashboard.sh</code></p>
<h3 id="【题目8】Cinder-安装-1-分"><a href="#【题目8】Cinder-安装-1-分" class="headerlink" title="【题目8】Cinder 安装[1 分]"></a>【题目8】Cinder 安装[1 分]</h3><p>使用提供的脚本框架iaas-install-cinder-controller.sh 和iaas-install-cinder-compute.sh，填充脚本（只需填充中文注释下的内容），在controller 和compute 节点上安装cinder 服务并完成配置。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-cinder-controller.sh</code></p>
<p><strong>compute</strong> </p>
<p><code>iaas-install-cinder-compute.sh</code></p>
<h3 id="【题目9】Swift-安装-1-分"><a href="#【题目9】Swift-安装-1-分" class="headerlink" title="【题目9】Swift 安装[1 分]"></a>【题目9】Swift 安装[1 分]</h3><p>使用提供的脚本框架iaas-install-swift-controller.sh 和iaas-install-swift-compute.sh，填充脚本（只需填充中文注释下的内容），在controller 和compute 节点上安装swift 服务并完成配置。完成后提交控制节点的用户名、密码和IP 地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-swift-controller.sh</code></p>
<p><strong>compute</strong> </p>
<p><code>iaas-install-swift-compute.sh</code></p>
<h3 id="【题目10】Heat-安装-1-分"><a href="#【题目10】Heat-安装-1-分" class="headerlink" title="【题目10】Heat 安装[1 分]"></a>【题目10】Heat 安装[1 分]</h3><p>使用提供的脚本框架iaas-install-heat.sh，填充脚本（只需填充中文注释下的内容），在controller 节点上安装heat 服务并完成相关配置。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>controller</strong> </p>
<p><code>iaas-install-heat.sh</code></p>
<p><strong>OpenStack 搭建任务完成</strong></p>
]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-私有云-任务3</title>
    <url>/2022/04/08/%E4%BA%91%E8%AE%A1%E7%AE%97-%E7%A7%81%E6%9C%89%E4%BA%91-%E4%BB%BB%E5%8A%A13/</url>
    <content><![CDATA[<h1 id="云计算赛项第一场-私有云"><a href="#云计算赛项第一场-私有云" class="headerlink" title="云计算赛项第一场-私有云"></a>云计算赛项第一场-私有云</h1><h2 id="【任务3】OpenStack运维任务-10分"><a href="#【任务3】OpenStack运维任务-10分" class="headerlink" title="【任务3】OpenStack运维任务[10分]"></a>【任务3】OpenStack运维任务[10分]<span id="more"></span></h2><h3 id="【题目1】heat模板管理"><a href="#【题目1】heat模板管理" class="headerlink" title="【题目1】heat模板管理"></a>【题目1】heat模板管理</h3><p>在openstack私有云平台上，在&#x2F;root目录下编写模板flavor.yaml，创建名为“m1.flavor”、 ID 为 1234、内存为1024MB、硬盘为20GB、vcpu数量为 1的云主机类型。完成后提交控制节点的用户名、密码和IP地址到答题框。（在提交信息前请准备好yaml模板执行的环境）</p>
<p><code>cd /root</code></p>
<p><code>vi flavor.yaml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  server:</span><br><span class="line">    type: OS::Nova::Flavor</span><br><span class="line">    properties:</span><br><span class="line">      disk: 20</span><br><span class="line">      flavorid: 1234</span><br><span class="line">      name: m1.flavor</span><br><span class="line">      ram: 1024</span><br><span class="line">      vcpus: 1</span><br></pre></td></tr></table></figure>

<p><code>heat stack-create m1.flavor -f flavor.yaml</code> </p>
<h3 id="【题目2】云主机管理"><a href="#【题目2】云主机管理" class="headerlink" title="【题目2】云主机管理"></a>【题目2】云主机管理</h3><p>在openstack私有云平台上，基于“cirros”镜像、flavor使用“m1.flavor”、extnet的网络，创建一台虚拟机VM1，启动VM1，并使用PC机能远程登录到VM1。提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><code>openstack server create --image cirros --flavor m1.flavor --network extnet VM1</code></p>
<h3 id="【题目3】虚拟机调整flavor"><a href="#【题目3】虚拟机调整flavor" class="headerlink" title="【题目3】虚拟机调整flavor"></a>【题目3】虚拟机调整flavor</h3><p>使用OpenStack私有云平台，使用centos7.5镜像，flavor使用1vcpu&#x2F;2G内存&#x2F;40G硬盘，创建云主机cscc_vm，假设在使用过程中，发现该云主机配置太低，需要调整，请修改相应配置，将dashboard界面上的云主机调整实例大小可以使用，将该云主机实例大小调整为2vcpu&#x2F;4G内存&#x2F;40G硬盘。完成后提交所修改配置文件节点的IP地址、用户名和密码到答题框。</p>
<p><code>sed -i &#39;s/#allow_resize_to_same_host=false/allow_resize_to_same_host=True/g&#39; /etc/nova/nova.conf</code></p>
<p><code>systemctl restart *nova*</code></p>
<p><code>vi flavor2.yaml</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  server:</span><br><span class="line">    type: OS::Nova::Flavor</span><br><span class="line">    properties:</span><br><span class="line">      disk: 40</span><br><span class="line">      flavorid: 1235</span><br><span class="line">      name: m2.flavor</span><br><span class="line">      ram: 4096</span><br><span class="line">      vcpus: 2</span><br></pre></td></tr></table></figure>

<p><code>heat stack-create m2.flavor -f flavor2.yaml</code> </p>
<p>然后图形化操作</p>
<h3 id="【题目4】swift后端存储"><a href="#【题目4】swift后端存储" class="headerlink" title="【题目4】swift后端存储"></a>【题目4】swift后端存储</h3><p> <code>vi /etc/glance/glance-api.conf</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[glance_store]</span><br><span class="line">#stores = file,http</span><br><span class="line">#default_store = file</span><br><span class="line">#filesystem_store_datadir = /var/lib/glance/images/</span><br><span class="line">default_store=swift</span><br><span class="line">stores=glance.store.swift.Store</span><br><span class="line">swift_store_auth_address=http://controller:5000/v3.0</span><br><span class="line">swift_store_endpoint_type=internalURL</span><br><span class="line">swift_store_multi_tenant=True</span><br><span class="line">swift_store_admin_tenants=service</span><br><span class="line">swift_store_user=glance</span><br><span class="line">swift_store_key=000000</span><br><span class="line">swift_store_container=glance</span><br><span class="line">swift_store_create_container_on_put=True</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart *glance*</code></p>
<h3 id="【题目5】RabbitMQ集群"><a href="#【题目5】RabbitMQ集群" class="headerlink" title="【题目5】RabbitMQ集群"></a>【题目5】RabbitMQ集群</h3><p>使用OpenStack私有云平台，创建三个centos7.5系统的云主机，使用附件\私有云附件\RabbitMQ目录下的软件包安装RabbitMQ服务，安装完毕后，搭建RabbitMQ集群，并打开RabbitMQ服务的图形化监控页面插件。集群使用普通集群模式，其中第一台做磁盘节点，另外两台做内存节点。完成后提交磁盘节点的用户名、密码和IP地址到答题框。</p>
<p><strong>（1）修改主机名</strong></p>
<p>对这3台虚拟机进行修改主机名的操作，主机名修改为rabbitmq1，rabbitmq2，rabbitmq3。命令如下</p>
<p><strong>rabbitmaq1</strong></p>
<p><code>hostnamectl set-hostname rabbitmq1</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p> <strong>rabbitmaq2</strong></p>
<p><code>hostnamectl set-hostname rabbitmq2</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p> <strong>rabbitmaq3</strong></p>
<p><code>hostnamectl set-hostname rabbitmq3</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>（2）修改hosts</strong></p>
<p>三个节点都配置hosts</p>
<p><code>vi /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">192.168.20.121  rabbitmq1</span><br><span class="line">192.168.20.128  rabbitmq2</span><br><span class="line">192.168.20.116  rabbitmq3</span><br></pre></td></tr></table></figure>

<p><strong>（4）配置yum源</strong></p>
<p>三个节点均使用提供的rabbitmq-repo.tar.gz的压缩包，上传至虚拟机的&#x2F;root目录下，解压并放在&#x2F;opt目录下，进入&#x2F;etc&#x2F;yum.repos.d目录下，将原来的repo文件移除，新建local.repo文件并编辑内容，具体操作命令如下：</p>
<p><code>curl -O http://172.19.25.11/rabbitmq-repo.tar.gz</code></p>
<p><code>tar -zxvf rabbitmq-repo.tar.gz -C /opt/</code></p>
<p><code>mv /etc/yum.repos.d/* /tmp/</code></p>
<p><code>vi local.repo</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[rabbitmq]</span><br><span class="line">name=rabbitmq</span><br><span class="line">baseurl=file:///opt/rabbitmq-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><strong>（5）安装RabbitMQ服务并启动</strong></p>
<p>配置完毕后，三个节点安装RabbitMQ服务，命令如下：</p>
<p><code>yum install -y rabbitmq-server</code></p>
<p>rabbitmq1节点启动RabbitMQ服务并查看服务状态，命令如下：</p>
<p><code>systemctl start rabbitmq-server</code></p>
<p><code>systemctl status rabbitmq-server</code></p>
<p><strong>（6）配置界面访问</strong></p>
<p>RabbitMQ提供了一个非常友好的图形化监控页面插件（rabbitmq_management），让我们可以一目了然看见Rabbit的状态或集群状态。启用图形化页面插件的具体命令如下：</p>
<p><code>rabbitmq-plugins enable rabbitmq_management</code></p>
<p><code>service rabbitmq-server restart</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]# netstat -ntpl</span><br><span class="line"></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line"></span><br><span class="line">Proto Recv-Q Send-Q Local Address      Foreign Address     State    PID/Program name  </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:25672      0.0.0.0:*        LISTEN   13685/beam     </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:111       0.0.0.0:*        LISTEN   500/rpcbind     </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:4369      0.0.0.0:*        LISTEN   11320/epmd     </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:22       0.0.0.0:*        LISTEN   1162/sshd      </span><br><span class="line"></span><br><span class="line">tcp    0   0 0.0.0.0:15672      0.0.0.0:*        LISTEN   13685/beam     </span><br><span class="line"></span><br><span class="line">tcp    0   0 127.0.0.1:25      0.0.0.0:*        LISTEN   932/master     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::5672         :::*          LISTEN   13685/beam     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::111         :::*          LISTEN   500/rpcbind     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::4369         :::*          LISTEN   11320/epmd     </span><br><span class="line"></span><br><span class="line">tcp6    0   0 :::22          :::*          LISTEN   1162/sshd      </span><br><span class="line"></span><br><span class="line">tcp6    0   0 ::1:25         :::*          LISTEN   932/master  </span><br></pre></td></tr></table></figure>

<p>可以看到15672端口已开放，打开浏览器，输入rabbitmq1节点的IP+端口15672（<a href="http://172.30.11.12:15672）访问RabbitMQ监控界面，使用默认的用户名和密码登录（用户名和密码都为guest）">http://172.30.11.12:15672）访问RabbitMQ监控界面，使用默认的用户名和密码登录（用户名和密码都为guest）</a></p>
<p><strong>（7）配置节点间的通信</strong></p>
<p>RabbitMQ的集群是依附于erlang集群来工作的，所以必须先构建起一个erlang集群。erlang集群中各节点是由magic cookie来实现的，每个节点上要保持相同的.erlang.cookie文件，这个cookie存放在&#x2F;var&#x2F;lib&#x2F;rabbitmq&#x2F;.erlang.cookie中，文件是400的权限。必须保证各节点cookie一致，不然节点之间就无法通信。</p>
<p>查看rabbitmq1节点的.erlang.cookie文件，并将该文件复制到rabbitmq2和rabbitmq3节点的&#x2F;var&#x2F;lib&#x2F;rabbitmq&#x2F;目录下，命令如下</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]# cat /var/lib/rabbitmq/.erlang.cookie </span><br><span class="line"></span><br><span class="line">EZYGPUJOTSESXPAUFMWO</span><br><span class="line"></span><br><span class="line">[root@rabbitmq1 ~]# scp /var/lib/rabbitmq/.erlang.cookie root@rabbitmq2:/var/lib/rabbitmq/</span><br><span class="line"></span><br><span class="line">[root@rabbitmq1 ~]# scp /var/lib/rabbitmq/.erlang.cookie root@rabbitmq3:/var/lib/rabbitmq/</span><br></pre></td></tr></table></figure>

<p>将.erlang.cookie文件传至rabbitmq2和rabbitmq3节点后，需要修改该文件的用户与用户组，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]#cd /var/lib/rabbitmq/</span><br><span class="line">[root@rabbitmq1 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie</span><br><span class="line">[root@rabbitmq2 ~]#cd /var/lib/rabbitmq/</span><br><span class="line">[root@rabbitmq2 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie</span><br><span class="line">[root@rabbitmq3 ~]#cd /var/lib/rabbitmq/</span><br><span class="line">[root@rabbitmq3 rabbitmq]# chown rabbitmq:rabbitmq .erlang.cookie</span><br></pre></td></tr></table></figure>

<p><strong>（8）配置节点加入集群</strong></p>
<p>在rabbitmq2、rabbitmq3节点执行如下命令，将这两个节点作为RAM节点加入到RabbitMQ集群中，具体命令如下：</p>
<p>rabbitmq2节点：</p>
<p><code>systemctl start rabbitmq-server</code></p>
<p><code>service rabbitmq-server restart</code></p>
<p>rabbitmq3节点：</p>
<p><code>systemctl start rabbitmq-server</code></p>
<p><code>service rabbitmq-server restart</code></p>
<p>上面这俩步千万不要忘记打</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rabbitmq2节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">Stopping node rabbit@rabbitmq2 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmqctl join_cluster --ram rabbit@rabbitmq1</span><br><span class="line"></span><br><span class="line">Clustering node rabbit@rabbitmq2 with rabbit@rabbitmq1 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmqctl start_app</span><br><span class="line"></span><br><span class="line">Starting node rabbit@rabbitmq2 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">rabbitmq3节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmqctl stop_app</span><br><span class="line"></span><br><span class="line">Stopping node rabbit@rabbitmq3 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmqctl join_cluster --ram rabbit@rabbitmq1</span><br><span class="line"></span><br><span class="line">Clustering node rabbit@rabbitmq3 with rabbit@rabbitmq1 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmqctl start_app</span><br><span class="line"></span><br><span class="line">Starting node rabbit@rabbitmq3 ...</span><br><span class="line"></span><br><span class="line">...done.</span><br></pre></td></tr></table></figure>

<p>默认rabbitmq启动后是磁盘节点，在这个cluster命令下，rabbitmq2和rabbitmq3是内存节点，rabbitmq1是磁盘节点。</p>
<p>如果要使rabbitmq2、rabbitmq3都是磁盘节点，去掉–ram参数即可。</p>
<p>如果想要更改节点类型，可以使用命令rabbitmqctl change_cluster_node_type disc(ram)，前提是必须停掉Rabbit应用。</p>
<p><strong>（9）配置RAM节点启用界面</strong></p>
<p>在rabbitmq2和rabbitmq3节点上启用rabbitmq_management，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rabbitmq2节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# rabbitmq-plugins enable rabbitmq_management</span><br><span class="line"></span><br><span class="line">The following plugins have been enabled:</span><br><span class="line"></span><br><span class="line"> mochiweb</span><br><span class="line"></span><br><span class="line"> webmachine</span><br><span class="line"></span><br><span class="line"> rabbitmq_web_dispatch</span><br><span class="line"></span><br><span class="line"> amqp_client</span><br><span class="line"></span><br><span class="line"> rabbitmq_management_agent</span><br><span class="line"></span><br><span class="line"> rabbitmq_management</span><br><span class="line"></span><br><span class="line">Plugin configuration has changed. Restart RabbitMQ for changes to take effect.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq2 rabbitmq]# systemctl restart rabbitmq-server</span><br><span class="line"></span><br><span class="line">rabbitmq3节点：</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# rabbitmq-plugins enable rabbitmq_management</span><br><span class="line"></span><br><span class="line">The following plugins have been enabled:</span><br><span class="line"></span><br><span class="line"> mochiweb</span><br><span class="line"></span><br><span class="line"> webmachine</span><br><span class="line"></span><br><span class="line"> rabbitmq_web_dispatch</span><br><span class="line"></span><br><span class="line"> amqp_client</span><br><span class="line"></span><br><span class="line"> rabbitmq_management_agent</span><br><span class="line"></span><br><span class="line"> rabbitmq_management</span><br><span class="line"></span><br><span class="line">Plugin configuration has changed. Restart RabbitMQ for changes to take effect.</span><br><span class="line"></span><br><span class="line">[root@rabbitmq3 rabbitmq]# systemctl restart rabbitmq-server</span><br></pre></td></tr></table></figure>

<p>启用rabbitmq2节点和rabbitmq3节点的监控界面后，登录</p>
<p><strong>（10）查看集群状态</strong></p>
<p>在RabbitMQ集群的任一节点上，可以查看RabbitMQ集群的状态，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@rabbitmq1 ~]# rabbitmqctl cluster_status</span><br><span class="line"></span><br><span class="line">Cluster status of node rabbit@rabbitmq1 ...</span><br><span class="line"></span><br><span class="line">[&#123;nodes,[&#123;disc,[rabbit@rabbitmq1]&#125;,&#123;ram,[rabbit@rabbitmq3,rabbit@rabbitmq2]&#125;]&#125;,</span><br><span class="line"></span><br><span class="line"> &#123;running_nodes,[rabbit@rabbitmq3,rabbit@rabbitmq2,rabbit@rabbitmq1]&#125;,</span><br><span class="line"></span><br><span class="line"> &#123;cluster_name,&lt;&lt;&quot;rabbit@rabbitmq1&quot;&gt;&gt;&#125;,</span><br><span class="line"></span><br><span class="line"> &#123;partitions,[]&#125;]</span><br><span class="line"></span><br><span class="line">...done.</span><br></pre></td></tr></table></figure>

<p>可以查看到rabbitmq1节点为disc磁盘节点，rabbitmq2节点和rabbitmq3节点为RAM内存节点。</p>
<h3 id="【题目6】主从数据库管理"><a href="#【题目6】主从数据库管理" class="headerlink" title="【题目6】主从数据库管理"></a>【题目6】主从数据库管理</h3><p>使用OpenStack私有云平台，使用centos7.5系统，创建两台云主机mysql1和mysql2；在这2台云主机上安装据库（使用附件中提供的MariaDB目录下的mariadb-repo作为安装源）并配置为主从数据库（master为主节点、slave为从节点、数据库密码设置为000000）；</p>
<p><strong>（1）修改主机名</strong></p>
<p><strong>mysql1</strong></p>
<p><code>hostnamectl set-hostname mysql1</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>mysql2</strong></p>
<p><code>hostnamectl set-hostname mysql2</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>（2）配置hosts文件</strong></p>
<p>两个节点配置&#x2F;etc&#x2F;hosts文件，修改为如下</p>
<p><code>vi /etc/hosts</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">192.168.20.107  mysql1</span><br><span class="line">192.168.20.109  mysql2</span><br><span class="line">192.168.20.137  mycat</span><br></pre></td></tr></table></figure>

<p><strong>（3）配置YUM源</strong></p>
<p>两个节点均使用提供的mariadb–10.3.23-repo.tar.gz的压缩包，解压并放在&#x2F;opt目录下，进入&#x2F;etc&#x2F;yum.repos.d目录下，将原来的repo文件移除，新建local.repo文件并编辑内容，具体操作命令如下：</p>
<p><code>curl -O http://172.19.25.11/mariadb-10.3.23-repo.tar.gz</code></p>
<p> <code>tar -zxvf mariadb-10.3.23-repo.tar.gz -C /opt</code></p>
<p><code>rm -rf /etc/yum.repos.d/*</code></p>
<p><code>vi /etc/yum.repos.d/local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line">name=mariadb</span><br><span class="line">baseurl=file:///opt/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><strong>（4）安装数据库服务并启动</strong></p>
<p>配置完毕后，两个节点安装数据库服务，命令如下：</p>
<p><code>yum install -y mariadb mariadb-server</code></p>
<p><code>systemctl start mariadb</code></p>
<p><code>systemctl enable mariadb</code></p>
<p><strong>(5）初始化数据库</strong></p>
<p>两个节点初始化数据库，配置数据库root密码为000000，命令如下：</p>
<p><code>mysql_secure_installation</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/usr/bin/mysql_secure_installation: line 379: find_mysql_client: command not found</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">NOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MariaDB</span><br><span class="line"></span><br><span class="line">   SERVERS IN PRODUCTION USE! PLEASE READ EACH STEP CAREFULLY!</span><br><span class="line"></span><br><span class="line">In order to log into MariaDB to secure it, we&#x27;ll need the current</span><br><span class="line">password for the root user. If you&#x27;ve just installed MariaDB, and</span><br><span class="line">you haven&#x27;t set the root password yet, the password will be blank,</span><br><span class="line">so you should just press enter here.</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">Enter current password for root (enter for none):     #默认按Enter键</span><br><span class="line">OK, successfully used password, moving on...</span><br><span class="line"></span><br><span class="line">Setting the root password ensures that nobody can log into the MariaDB</span><br><span class="line">root user without the proper authorisation.</span><br><span class="line"></span><br><span class="line">Set root password? [Y/n] y</span><br><span class="line"></span><br><span class="line">New password:                 #输入数据库root密码000000</span><br><span class="line">Re-enter new password:             #再次输入密码000000</span><br><span class="line">Password updated successfully!</span><br><span class="line">Reloading privilege tables..</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">By default, a MariaDB installation has an anonymous user, allowing anyone</span><br><span class="line">to log into MariaDB without having to have a user account created for</span><br><span class="line">them. This is intended only for testing, and to make the installation</span><br><span class="line">go a bit smoother. You should remove them before moving into a</span><br><span class="line">production environment.</span><br><span class="line"></span><br><span class="line">Remove anonymous users? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"> </span><br><span class="line">Normally, root should only be allowed to connect from &#x27;localhost&#x27;. This</span><br><span class="line">ensures that someone cannot guess at the root password from the network.</span><br><span class="line"> </span><br><span class="line">Disallow root login remotely? [Y/n] n</span><br><span class="line"> ... skipping.</span><br><span class="line"></span><br><span class="line">By default, MariaDB comes with a database named &#x27;test&#x27; that anyone can</span><br><span class="line">access. This is also intended only for testing, and should be removed</span><br><span class="line">before moving into a production environment.</span><br><span class="line"></span><br><span class="line">Remove test database and access to it? [Y/n] y</span><br><span class="line"> \- Dropping test database...</span><br><span class="line"> ... Success!</span><br><span class="line"> \- Removing privileges on test database...</span><br><span class="line"> ... Success!</span><br><span class="line"></span><br><span class="line">Reloading the privilege tables will ensure that all changes made so far</span><br><span class="line">will take effect immediately.</span><br><span class="line"></span><br><span class="line">Reload privilege tables now? [Y/n] y</span><br><span class="line"> ... Success!</span><br><span class="line"> </span><br><span class="line">Cleaning up...</span><br><span class="line"></span><br><span class="line">All done! If you&#x27;ve completed all of the above steps, your MariaDB</span><br><span class="line">installation should now be secure.</span><br><span class="line"></span><br><span class="line">Thanks for using MariaDB!</span><br></pre></td></tr></table></figure>

<p><strong>（6）配置mysql1主节点</strong></p>
<p>修改mysql1节点的数据库配置文件，在配置文件&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf中的[mysqld]增添如下内容。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# cat /etc/my.cnf.d/server.cnf</span><br><span class="line">... ...</span><br><span class="line">[mysqld]</span><br><span class="line">log_bin = mysql-bin            #记录操作日志</span><br><span class="line">binlog_ignore_db = mysql         #不同步MySQL系统数据库</span><br><span class="line">server_id = 12              #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例如172.30.11.12，server_id就写12</span><br></pre></td></tr></table></figure>

<p>重启数据库服务，并进入数据库，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# systemctl restart mariadb</span><br><span class="line">[root@mysql1 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 9</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"> </span><br><span class="line">MariaDB [(none)]&gt; </span><br></pre></td></tr></table></figure>

<p>在mysql1节点，授权在任何客户端机器上可以以root用户登录到数据库，然后在主节点上创建一个user用户连接节点mysql2，并赋予从节点同步主节点数据库的权限。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; grant all privileges on *.* to root@&#x27;%&#x27; identified by &quot;000000&quot;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; grant replication slave on *.* to &#x27;user&#x27;@&#x27;mysql2&#x27; identified by &#x27;000000&#x27;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><strong>（7）配置mysql2从节点</strong></p>
<p>修改mysql2节点的数据库配置文件，在配置文件&#x2F;etc&#x2F;my.cnf.d&#x2F;server.cnf中的[mysqld]增添如下内容。</p>
<p><code>[root@mysql2 ~]# cat /etc/my.cnf.d/server.cnf</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">... ...</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">log_bin = mysql-bin            #记录操作日志</span><br><span class="line">binlog_ignore_db = mysql         #不同步MySQL系统数据库</span><br><span class="line">server_id = 13              #数据库集群中的每个节点id都要不同，一般使用IP地址的最后段的数字，例如172.30.11.13，server_id就写13</span><br><span class="line"></span><br><span class="line">... ...</span><br></pre></td></tr></table></figure>

<p>修改完配置文件后，重启数据库服务，并在从节点mysql2上登录MariaDB数据库，配置从节点连接主节点的连接信息。master_host为主节点主机名mysql1，master_user为上一步中创建的用户user，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql2 ~]# systemctl restart mariadb</span><br><span class="line">[root@mysql2 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 9</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; change master to master_host=&#x27;mysql1&#x27;,master_user=&#x27;user&#x27;,master_password=&#x27;000000&#x27;;</span><br><span class="line"></span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br></pre></td></tr></table></figure>

<p>配置完毕主从数据库之间的连接信息之后，开启从节点服务。使用show slave status\G命令，并查看从节点服务状态，如果Slave_IO_Running和Slave_SQL_Running的状态都为YES，则从节点服务开启成功。命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [(none)]&gt; start slave;</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; show slave status\G</span><br><span class="line"></span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">        Slave_IO_State: Waiting for master to send event</span><br><span class="line">          Master_Host: mysql1</span><br><span class="line">          Master_User: user</span><br><span class="line">         Master_Port: 3306</span><br><span class="line">         Connect_Retry: 60</span><br><span class="line">........</span><br><span class="line">      Slave_IO_Running: Yes</span><br><span class="line">       Slave_SQL_Running: Yes</span><br><span class="line">.........</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到Slave_IO_Running和Slave_SQL_Running的状态都是Yes，配置数据库主从集群成功。</p>
<p><strong>（8）主节点创建数据库</strong></p>
<p>先在主节点mysql1中创建库test，并在库test中创建表company，插入表数据，创建完成后，查看表company数据，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql1 ~]# mysql -uroot -p000000</span><br><span class="line"></span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line"></span><br><span class="line">Your MariaDB connection id is 11</span><br><span class="line"></span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"> </span><br><span class="line">MariaDB [(none)]&gt; create database test;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">MariaDB [(none)]&gt; use test;</span><br><span class="line">Database changed</span><br><span class="line">MariaDB [test]&gt; create table company(id int not null primary key,name varchar(50),addr varchar(255));</span><br><span class="line">Query OK, 0 rows affected (0.01 sec)</span><br><span class="line"> </span><br><span class="line">MariaDB [test]&gt; insert into company values(1,&quot;alibaba&quot;,&quot;china&quot;);</span><br><span class="line"></span><br><span class="line">Query OK, 1 row affected (0.01 sec)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">MariaDB [test]&gt; select * from company;</span><br><span class="line"></span><br><span class="line">+----+---------+-------+</span><br><span class="line">| id | name  | addr |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| 1 | alibaba | china |</span><br><span class="line">+----+---------+-------+</span><br><span class="line"></span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p><strong>（9）从节点验证复制功能</strong></p>
<p>登录mysql2节点的数据库，查看数据库列表。找到test数据库，查询表，并查询内容验证从数据库的复制功能，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mysql2 ~]# mysql -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 12</span><br><span class="line">Server version: 10.3.23-MariaDB-log MariaDB Server</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line"> </span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"> </span><br><span class="line">MariaDB [(none)]&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database      |</span><br><span class="line">+--------------------+</span><br><span class="line"></span><br><span class="line">| information_schema |</span><br><span class="line">| mysql       |</span><br><span class="line">| performance_schema |</span><br><span class="line">| test        |</span><br><span class="line">+--------------------+</span><br><span class="line">4 rows in set (0.00 sec) </span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt; use test;</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line">Database changed</span><br><span class="line">MariaDB [test]&gt; show tables;</span><br><span class="line">+----------------+</span><br><span class="line">| Tables_in_test |</span><br><span class="line">+----------------+</span><br><span class="line">| company    |</span><br><span class="line">+----------------+</span><br><span class="line">1 row in set (0.00 sec)</span><br><span class="line">MariaDB [test]&gt; select * from company;</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| id | name  | addr |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| 1 | alibaba | china |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<p>可以查看到主数据库中刚刚创建的库、表、信息，验证从数据库的复制功能成功。</p>
<h3 id="【题目7】数据库读写分离"><a href="#【题目7】数据库读写分离" class="headerlink" title="【题目7】数据库读写分离"></a>【题目7】数据库读写分离</h3><p>用上个案例安装完成的主从数据库作为基础进行实验，使用OpenStack平台再创建一台云主机作为mycat数据库中间件，逻辑库USERDB对应数据库database为test（在部署主从数据库时已创建）；设置数据库写入节点为主节点mysql1；设置数据库读取节点为从节点mysql2。</p>
<p><strong>（1）修改主机名</strong></p>
<p><code>hostnamectl set-hostname mycat</code></p>
<p><code>bash</code></p>
<p><code>exit</code></p>
<p><strong>（2）安装JDK环境</strong></p>
<p>在mycat节点安装Java JDK环境，具体操作步骤如下：</p>
<p>使用提供的mariadb-10.3.23-repo.tar.gz包上传至mycat节点的&#x2F;root目录下，解压并配置成本地yum源，命令如下：</p>
<p><code>curl -O http://172.19.25.11/mariadb-10.3.23-repo.tar.gz</code></p>
<p><code>tar -zxvf mariadb-10.3.23-repo.tar.gz -C /opt</code></p>
<p><code>rm -rf /etc/yum.repos.d/*</code></p>
<p><code>vi /etc/yum.repos.d/local.repo</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mariadb]</span><br><span class="line">name=mariadb</span><br><span class="line">baseurl=file:///opt/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p>配置完yum源之后，进行安装Java JDK环境，命令如下：</p>
<p><code>yum install -y java-1.8.0-openjdk java-1.8.0-openjdk-devel</code></p>
<p>安装完之后，可以使用命令查看Java JDK环境，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mycat ~]# java -version</span><br><span class="line">openjdk version &quot;1.8.0_262&quot;</span><br><span class="line">OpenJDK Runtime Environment (build 1.8.0_262-b10)</span><br><span class="line">OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode)</span><br></pre></td></tr></table></figure>

<p><strong>（3）安装Mycat服务</strong></p>
<p>将Mycat服务的二进制软件包Mycat-server-1.6-RELEASE-20161028204710-linux.tar.gz上传到Mycat虚拟机的&#x2F;root目录下，并将软件包解压到&#x2F;use&#x2F;local目录中。赋予解压后的Mycat目录权限。</p>
<p><code>curl -O http://172.19.25.11/Mycat-server-1.6.6.1-release-20181031195535-linux.tar.gz</code></p>
<p><code>tar -zxvf Mycat-server-1.6.6.1-release-20181031195535-linux.tar.gz -C /usr/local/</code></p>
<p><code>chmod -R 777 /usr/local/mycat/</code></p>
<p><strong>（4）编辑Mycat的逻辑库配置文件</strong></p>
<p>配置Mycat服务读写分离的schema.xml配置文件在&#x2F;usr&#x2F;local&#x2F;mycat&#x2F;conf&#x2F;目录下，可以在文件中定义一个逻辑库，使用户可以通过Mycat服务管理该逻辑库对应的MariaDB数据库。在这里定义一个逻辑库schema，name为USERDB；该逻辑库USERDB对应数据库database为test（在部署主从数据库时已创建）；设置数据库写入节点为主节点mysql1；设置数据库读取节点为从节点mysql2。（可以直接删除原来schema.xml的内容，替换为如下。）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mycat ~]# cat /usr/local/mycat/conf/schema.xml </span><br><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;</span><br><span class="line">&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;</span><br><span class="line">        &lt;schema name=&quot;USERDB&quot; checkSQLschema=&quot;true&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;&gt;&lt;/schema&gt;</span><br><span class="line">        &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;test&quot; /&gt;</span><br><span class="line">        &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;3&quot; writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;</span><br><span class="line">                &lt;heartbeat&gt;select user()&lt;/heartbeat&gt;</span><br><span class="line">                &lt;writeHost host=&quot;hostM1&quot; url=&quot;192.168.20.107:3306&quot; user=&quot;root&quot; password=&quot;000000&quot;&gt;</span><br><span class="line">                &lt;readHost host=&quot;hostS2&quot; url=&quot;192.168.20.109:3306&quot; user=&quot;root&quot; password=&quot;000000&quot; /&gt;</span><br><span class="line">                &lt;/writeHost&gt;</span><br><span class="line">                &lt;/dataHost&gt; </span><br><span class="line">&lt;/mycat:schema&gt;</span><br></pre></td></tr></table></figure>

<p><strong>（5）修改配置文件权限</strong></p>
<p>修改schema.xml的用户权限，命令如下：</p>
<p> <code>chown root:root /usr/local/mycat/conf/schema.xml</code></p>
<p><strong>（6）编辑mycat的访问用户</strong></p>
<p>修改&#x2F;usr&#x2F;local&#x2F;mycat&#x2F;conf&#x2F;目录下的server.xml文件，修改root用户的访问密码与数据库，密码设置为000000，访问Mycat的逻辑库为USERDB，命令如下。</p>
<p><code>cat /usr/local/mycat/conf/server.xml</code> </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">在配置文件的最后部分，</span><br><span class="line"></span><br><span class="line">&lt;user name=&quot;root&quot;&gt;</span><br><span class="line">    &lt;property name=&quot;password&quot;&gt;000000&lt;/property&gt;</span><br><span class="line">    &lt;property name=&quot;schemas&quot;&gt;USERDB&lt;/property&gt;</span><br><span class="line">&lt;user&gt;    </span><br><span class="line">然后删除如下几行：</span><br><span class="line">&lt;user name=&quot;user&quot;&gt;</span><br><span class="line">    &lt;property name=&quot;password&quot;&gt;user&lt;/property&gt;</span><br><span class="line">    &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt;</span><br><span class="line">    &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt;</span><br><span class="line">&lt;/user&gt;</span><br></pre></td></tr></table></figure>

<p><strong>（7）启动Mycat服务</strong></p>
<p>通过命令启动Mycat数据库中间件服务，启动后使用netstat -ntpl命令查看虚拟机端口开放情况，如果有开放8066和9066端口，则表示Mycat服务开启成功。端口查询情况如图2-1所示。</p>
<p><code>bash /usr/local/mycat/bin/mycat restart</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mycat ~]# netstat -ntpl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span><br><span class="line">tcp        0      0 127.0.0.1:32000         0.0.0.0:*               LISTEN      11538/java          </span><br><span class="line">tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN      503/rpcbind         </span><br><span class="line">tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1194/sshd           </span><br><span class="line">tcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      964/master          </span><br><span class="line">tcp6       0      0 :::1984                 :::*                    LISTEN      11538/java          </span><br><span class="line">tcp6       0      0 :::8066                 :::*                    LISTEN      11538/java          </span><br><span class="line">tcp6       0      0 :::36199                :::*                    LISTEN      11538/java          </span><br><span class="line">tcp6       0      0 :::9066                 :::*                    LISTEN      11538/java          </span><br><span class="line">tcp6       0      0 :::111                  :::*                    LISTEN      503/rpcbind         </span><br><span class="line">tcp6       0      0 :::38037                :::*                    LISTEN      11538/java          </span><br><span class="line">tcp6       0      0 :::22                   :::*                    LISTEN      1194/sshd           </span><br><span class="line">tcp6       0      0 ::1:25                  :::*                    LISTEN      964/master  </span><br></pre></td></tr></table></figure>

<p><strong>（8）用Mycat服务查询数据库信息</strong></p>
<p>先在Mycat虚拟机上使用Yum安装mariadb-client服务。</p>
<p><code>yum install -y MariaDB-client</code></p>
<p>在Mycat虚拟机上使用mysql命令查看Mycat服务的逻辑库USERDB，因为Mycat的逻辑库USERDB对应数据库test（在部署主从数据库时已安装），所以可以查看库中已经创建的表company。命令如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mycat ~]# mysql -h127.0.0.1 -P8066 -uroot -p000000</span><br><span class="line">Welcome to the MariaDB monitor. Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 2</span><br><span class="line">Server version: 5.6.29-mycat-1.6-RELEASE-20161028204710 MyCat Server (OpenCloundDB)</span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line">MySQL [(none)]&gt; show databases;</span><br><span class="line">+----------+</span><br><span class="line">| DATABASE |</span><br><span class="line">+----------+</span><br><span class="line">| USERDB  |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (0.001 sec)</span><br><span class="line">MySQL [(none)]&gt; use USERDB</span><br><span class="line">Reading table information for completion of table and column names</span><br><span class="line">You can turn off this feature to get a quicker startup with -A</span><br><span class="line">Database changed</span><br><span class="line">MySQL [USERDB]&gt; show tables;</span><br><span class="line">+----------------+</span><br><span class="line">| Tables_in_test |</span><br><span class="line">+----------------+</span><br><span class="line">| company    |</span><br><span class="line">+----------------+</span><br><span class="line">1 row in set (0.003 sec)</span><br><span class="line">MySQL [USERDB]&gt; select * from company;</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| id | name  | addr |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">| 1 | alibaba | china |</span><br><span class="line">+----+---------+-------+</span><br><span class="line">1 row in set (0.005 sec)</span><br></pre></td></tr></table></figure>

<p><strong>（9）用Mycat服务添加表数据</strong></p>
<p>在Mycat虚拟机上使用mysql命令对表company添加一条数据(2,“basketball”,“usa”)，添加完毕后查看表信息。命令如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MySQL [USERDB]&gt; insert into company values(2,&quot;bastetball&quot;,&quot;usa&quot;);</span><br><span class="line">Query OK, 1 row affected (0.050 sec)</span><br><span class="line">MySQL [USERDB]&gt; select * from company;</span><br><span class="line">+----+------------+-------+</span><br><span class="line">| id | name    | addr |</span><br><span class="line">+----+------------+-------+</span><br><span class="line">| 1 | alibaba  | china |</span><br><span class="line">| 2 | bastetball | usa  |</span><br><span class="line">+----+------------+-------+</span><br><span class="line">2 rows in set (0.003 sec)</span><br></pre></td></tr></table></figure>

<p><strong>(10）验证Mycat服务对数据库读写操作分离</strong></p>
<p>在Mycat虚拟机节点使用mysql命令，通过9066端口查询对数据库读写操作的分离信息。可以看到所有的写入操作WRITE_LOAD数都在mysql1主数据库节点上，所有的读取操作READ_LOAD数都在mysql2主数据库节点上。由此可见，数据库读写操作已经分离到mysql1和mysql2节点上了。命令如下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@mycat ~]# mysql -h127.0.0.1 -P9066 -uroot -p000000 -e  &#x27;show @@datasource;&#x27;</span><br><span class="line">+----------+--------+-------+----------------+------+------+--------+------+------+---------+-----------+------------+</span><br><span class="line">| DATANODE | NAME   | TYPE  | HOST           | PORT | W/R  | ACTIVE | IDLE | SIZE | EXECUTE | READ_LOAD | WRITE_LOAD |</span><br><span class="line">+----------+--------+-------+----------------+------+------+--------+------+------+---------+-----------+------------+</span><br><span class="line">| dn1      | hostM1 | mysql | 192.168.20.107 | 3306 | W    |      0 |   10 | 1000 |      17 |         0 |          0 |</span><br><span class="line">| dn1      | hostS2 | mysql | 192.168.20.109 | 3306 | R    |      0 |    4 | 1000 |      10 |         0 |          0 |</span><br><span class="line">+----------+--------+-------+----------------+------+------+--------+------+------+---------+-----------+------------+</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="【题目9】zun的运维"><a href="#【题目9】zun的运维" class="headerlink" title="【题目9】zun的运维"></a>【题目9】zun的运维</h3><p>使用OpenStack私有云平台，使用脚本完成Zun服务的安装，安装完成后，上传CentOS7_1804.tar镜像到私有云平台，命名为centos7.5-docker。然后使用该镜像启动一个名为chinaskill-container的容器。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>compute</strong></p>
<p> <code>iaas-install-swift-compute.sh</code> </p>
<p><code>iaas-install-cinder-compute.sh</code></p>
<p><code>iaas-install-zun-compute.sh</code> </p>
<p><strong>controller</strong></p>
<p><code>iaas-install-swift-controller.sh</code> </p>
<p><code>iaas-install-cinder-controller.sh</code> </p>
<p><code>iaas-install-zun-controller.sh</code></p>
<p><strong>创建镜像</strong></p>
<p><code>openstack image create centos7-docker --public --container-format docker --disk-format raw &lt; /opt/iaas/images/CentOS7_1804.tar</code></p>
<p><strong>创建容器</strong></p>
<p><code>zun  run -n chinaskill-container --image-driver glance centos7.5-docker</code></p>
]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算-私有云</title>
    <url>/2022/04/22/%E4%BA%91%E8%AE%A1%E7%AE%97-%E7%A7%81%E6%9C%89%E4%BA%91/</url>
    <content><![CDATA[<h1 id="OpenStack-私有云"><a href="#OpenStack-私有云" class="headerlink" title="OpenStack 私有云"></a>OpenStack 私有云</h1><h2 id="OpenStack搭建任务当前任务共10道题目"><a href="#OpenStack搭建任务当前任务共10道题目" class="headerlink" title="OpenStack搭建任务当前任务共10道题目"></a><strong>OpenStack搭建任务</strong>当前任务共10道题目<span id="more"></span></h2><p><strong>1.基础安装</strong></p>
<p><strong>controller</strong></p>
<p><code>yum install -y iaas-xiandian</code></p>
<p>改脚本</p>
<p><code>iaas-pre-host.sh</code> </p>
<p><strong>compute</strong></p>
<p><code>yum install -y iaas-xiandian</code></p>
<p>改脚本</p>
<p><code>iaas-pre-host.sh</code> </p>
<p><strong>2、【实操题】数据库安装（3分）</strong></p>
<p>在controller节点上使用iaas-install-mysql.sh 脚本安装Mariadb、Memcached、RabbitMQ等服务。安装服务完毕后，完成下列题目。</p>
<p>1.登录数据库服务，设置数据库的最大连接数为5000。</p>
<p>2.登录数据库服务，设置数据库的参数，将最大允许的packet设置为30M。</p>
<p><code>iaas-install-mysql.sh</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/my.cnf</span><br><span class="line">#</span><br><span class="line"># This group is read both both by the client and the server</span><br><span class="line"># use it for options that affect everything</span><br><span class="line">#</span><br><span class="line">[client-server]</span><br><span class="line">#</span><br><span class="line"># This group is read by the server</span><br><span class="line">#</span><br><span class="line">[mysqld]</span><br><span class="line"># Disabling symbolic-links is recommended to prevent assorted security risks</span><br><span class="line">symbolic-links=0</span><br><span class="line">default-storage-engine = innodb</span><br><span class="line">innodb_file_per_table</span><br><span class="line">collation-server = utf8_general_ci</span><br><span class="line">init-connect = &#x27;SET NAMES utf8&#x27;</span><br><span class="line">character-set-server = utf8</span><br><span class="line">max_connections=5000 //最大连接数</span><br><span class="line">max_allowed_packet=30M //最大允许的packet设置为30M</span><br><span class="line">#</span><br><span class="line"># include all files from the config directory</span><br><span class="line">#</span><br><span class="line">!includedir /etc/my.cnf.d</span><br></pre></td></tr></table></figure>

<p><strong>3、【实操题】key安装</strong></p>
<p><code>iaas-install-keystone.sh</code> </p>
<p><strong>4、【实操题】Glance安装（2分）</strong></p>
<p>在controller节点上使用iaas-install-glance.sh脚本安装glance 服务。使用命令将提供的cirros-0.3.4-x86_64-disk.img镜像上传至平台，命名为cirros，并设置最小启动需要的硬盘为10G。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><code>curl -O http://172.19.25.11/cirros-0.3.4-x86_64-disk.img</code></p>
<p><code>source /etc/keystone/admin-openrc.sh</code> </p>
<p><code>glance image-create --name cirros --disk-format qcow2 --container-format bare --min-disk 10 --progress &lt; cirros-0.3.4-x86_64-disk.img</code> </p>
<p><strong>5、【实操题】Nova安装（2分）</strong></p>
<p>在controller节点和compute节点上分别使用iaas-install-nova-compute.sh脚本、iaas-install-nova-compute.sh脚本安装Nova 服务。安装完成后，修改相关参数对openstack平台进行调优操作，相应的调优操作有：</p>
<p>（1）设置cpu超售比例为4倍；</p>
<p>（2）设置内存超售比例为1.5倍；</p>
<p>（3）预留2048mb内存，这部分内存不能被虚拟机使用；</p>
<p>（4）预留10240mb磁盘，这部分磁盘不能被虚拟机使用；</p>
<p><strong>controller</strong></p>
<p><code>iaas-install-nova-controller.sh</code></p>
<p><strong>compute</strong></p>
<p><code>iaas-install-nova-compute.sh</code></p>
<p><strong>（1）设置cpu超售比例为4倍；</strong></p>
<p><code>sed -i &#39;s/#cpu_allocation_ratio=0.0/cpu_allocation_ratio=4.0/g&#39; /etc/nova/nova.conf</code></p>
<p><strong>（2）设置内存超售比例为1.5倍；</strong></p>
<p><code>sed -i &#39;s/#ram_allocation_ratio=0.0/ram_allocation_ratio=1.5/g&#39; /etc/nova/nova.conf</code></p>
<p><strong>（3）预留2048mb内存，这部分内存不能被虚拟机使用；</strong></p>
<p><code>sed -i &#39;s/#reserved_host_memory_mb=512/reserved_host_memory_mb=2048/g&#39; /etc/nova/nova.conf</code></p>
<p><strong>（4）预留10240mb磁盘，这部分磁盘不能被虚拟机使用；</strong></p>
<p><code>sed -i &#39;s/#reserved_host_disk_mb=0/reserved_host_disk_mb=10240/g&#39; /etc/nova/nova.conf</code></p>
<p><strong>重启</strong></p>
<p><code>systemctl restart *nova*</code></p>
<p><strong>6、【实操题】Neutron安装（2分）</strong></p>
<p>在controller节点和compute节点上分别使用iaas-install-neutron-controller.sh 和iaas-install-neutron-compute.sh脚本安装neutron服务并用命令创建一个网络</p>
<p><strong>compute</strong></p>
<p><code>iaas-install-neutron-compute.sh</code></p>
<p><strong>controller</strong></p>
<p><code>iaas-install-neutron-controller.sh</code> </p>
<p><strong>在controller执行命令</strong></p>
<p><code>openstack network create --no-share --external --provider-physical-network provider --provider-network-type vlan extnet</code></p>
<p><code>openstack subnet create --network extnet --subnet-range 10.10.1.0/24 --gateway 10.10.1.1 subextnet</code></p>
<p><strong>7、【实操题】dashboard安装（1分）</strong></p>
<p><strong>controller</strong></p>
<p><code>iaas-install-dashboard.sh</code></p>
<p><strong>8、【实操题】Swift安装（2分）</strong></p>
<p>在控制节点和计算节点上分别使用iaas-install-swift-controller.sh和iaas-install-swift-compute.sh脚本安装Swift服务。安装完成后，使用命令创建一个名叫examcontainer的容器。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>compute</strong></p>
<p><code>iaas-install-swift-compute.sh</code> </p>
<p><strong>controller</strong></p>
<p><code>iaas-install-swift-controller.sh</code></p>
<p><code>swift post examcontainer</code></p>
<p><strong>9、【实操题】Cinder创建硬盘（2分）</strong></p>
<p>在控制节点和计算节点分别使用iaas-install-cinder-controller.sh、iaas-install-cinder-compute.sh脚本安装Cinder服务，使用cinder命令创建一个名字叫blockvolume，大小为2G的云硬盘。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>compute</strong></p>
<p><code>iaas-install-cinder-compute.sh</code></p>
<p><strong>controller</strong></p>
<p><code>iaas-install-cinder-controller.sh</code></p>
<p><code>cinder create --name blockvolume 2</code></p>
<p><strong>10、【实操题】Heat安装（1分）</strong></p>
<p>在控制节点上使用iaas-install-heat.sh脚本安装Heat服务。完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>controller</strong></p>
<p><code>iaas-install-heat.sh</code></p>
<h3 id="三、OpenStack运维任务当前任务共8道题目"><a href="#三、OpenStack运维任务当前任务共8道题目" class="headerlink" title="三、OpenStack运维任务当前任务共8道题目"></a>三、OpenStack运维任务当前任务共8道题目</h3><p><strong>1、【实操题】Heat模板管理（2分）</strong></p>
<p>在自行搭建的OpenStack私有云平台上，在&#x2F;root目录下编写Heat模板heat-image.yaml，编写模板内容通过使用swift外部存储方式创建镜像heat-image，限制镜像最低磁盘使用为20G，最低内存使用为2G。完成后提交控制节点的用户名、密码和IP地址到答题框。（在提交信息前请准备好yaml模板执行的环境）</p>
<p><strong>不会</strong></p>
<p><strong>2、【实操题】Heat模板管理（1.5分）</strong></p>
<p>在自行搭建的OpenStack私有云平台或赛项提供的all-in-one平台上，在&#x2F;root目录下编写Heat模板create_net.yaml，创建名为Heat-Network网络，选择不共享；创建子网名为Heat-Subnet，子网网段设置为10.20.2.0&#x2F;24，开启DHCP服务，地址池为10.20.2.20-10.20.2.100。完成后提交控制节点的用户名、密码和IP地址到答题框。（在提交信息前请准备好yaml模板执行的环境）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat create_net.yaml </span><br><span class="line">heat_template_version: 2018-03-02</span><br><span class="line">resources:</span><br><span class="line">  network:</span><br><span class="line">    type: OS::Neutron::Net</span><br><span class="line">    properties:</span><br><span class="line">      name: &quot;heat-network&quot;</span><br><span class="line">      admin_state_up: true</span><br><span class="line">      shared: false</span><br><span class="line">  subnet:</span><br><span class="line">    type: OS::Neutron::Subnet</span><br><span class="line">    properties:</span><br><span class="line">      name: &quot;heat-subnet&quot;</span><br><span class="line">      cidr: 10.10.2.0/24</span><br><span class="line">      gateway_ip: 10.10.2.1</span><br><span class="line">      enable_dhcp: true</span><br><span class="line">      allocation_pools:</span><br><span class="line">        - start: 10.10.2.20</span><br><span class="line">          end: 10.10.2.100</span><br><span class="line">      network_id: &#123;get_resource: &quot;network&quot;&#125;</span><br></pre></td></tr></table></figure>

<p><code>heat stack-create -f  create_net.yaml heta-network</code></p>
<p><strong>3、【实操题】OpenStack参数调优（2分）</strong></p>
<p>OpenStack各服务内部通信都是通过RPC来交互，各agent都需要去连接RabbitMQ；随着各服务agent增多，MQ的连接数会随之增多，最终可能会到达上限，成为瓶颈。使用自行搭建的OpenStack私有云平台，分别通过用户级别、系统级别、配置文件来设置RabbitMQ服务的最大连接数为10240，配置完成后提交修改节点的用户名、密码和IP地址到答题框。</p>
<p>用户级别：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/security/limits.conf`</span><br><span class="line">//添加两行</span><br><span class="line">user             soft    nofile          10240</span><br><span class="line">user             hard    nofile          10240</span><br></pre></td></tr></table></figure>

<p>系统级别：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/sysctl.conf </span><br><span class="line">fs.file-max=10240</span><br></pre></td></tr></table></figure>

<p>配置文件：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /usr/lib/systemd/system/rabbitmq-server.service </span><br><span class="line"># systemd unit example</span><br><span class="line">[Unit]</span><br><span class="line">Description=RabbitMQ broker</span><br><span class="line">After=network.target epmd@0.0.0.0.socket</span><br><span class="line">Wants=network.target epmd@0.0.0.0.socket</span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">User=rabbitmq</span><br><span class="line">Group=rabbitmq</span><br><span class="line">NotifyAccess=all</span><br><span class="line">TimeoutStartSec=3600</span><br><span class="line">LimitNOFILE=10240 //添加这一条</span><br><span class="line"># Note:</span><br><span class="line"># You *may* wish to add the following to automatically restart RabbitMQ</span><br><span class="line"># in the event of a failure. systemd service restarts are not a</span><br><span class="line"># replacement for service monitoring. Please see</span><br><span class="line"># http://www.rabbitmq.com/monitoring.html</span><br><span class="line">#</span><br><span class="line"># Restart=on-failure</span><br><span class="line"># RestartSec=10</span><br><span class="line">WorkingDirectory=/var/lib/rabbitmq</span><br><span class="line">ExecStart=/usr/lib/rabbitmq/bin/rabbitmq-server</span><br><span class="line">ExecStop=/usr/lib/rabbitmq/bin/rabbitmqctl stop</span><br><span class="line">ExecStop=/bin/sh -c &quot;while ps -p $MAINPID &gt;/dev/null 2&gt;&amp;1; do sleep 1; done&quot;</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<p><code>systemctl daemon-reload</code></p>
<p><code>systemctl restart rabbitmq-server</code></p>
<p><strong>查看 rabbitmq 状态，查到 total_limit,10140</strong></p>
<p><code>rabbitmqctl status</code></p>
<p><strong>4、【实操题】KVM I&#x2F;O优化（1.5分）</strong></p>
<p>使用自行搭建的OpenStack私有云平台，优化KVM的I&#x2F;O调度算法，将默认的模式修改为none模式。配置完成后提交控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# echo none &gt; /sys/block/vda/queue/scheduler </span><br><span class="line">[root@controller ~]# cat /sys/block/vda/queue/scheduler </span><br><span class="line">[none] mq-deadline kyber </span><br></pre></td></tr></table></figure>

<p><strong>5、【实操题】开放镜像（2分）</strong></p>
<p>使用自行搭建的OpenStack私有云平台。使用提供的cirros-0.3.4-x86_64-disk.img镜像文件（镜像文件在提供的HTTP服务中）在admin项目中创建名为glance-cirros的镜像，通过命令将glance-cirros镜像指定demo项目可以进行共享使用。配置完成后提交controller控制节点的用户名、密码和IP地址到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# source /etc/keystone/admin-openrc.sh </span><br><span class="line">[root@controller ~]# glance image-create --name glance-cirros --disk-format qcow2 --container-format bare &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| Property         | Value                                |</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6     |</span><br><span class="line">| container_format | bare                                 |</span><br><span class="line">| created_at       | 2022-04-22T05:43:46Z                 |</span><br><span class="line">| disk_format      | qcow2                                |</span><br><span class="line">| id               | b18ba8b5-a016-4a0d-9115-9748d23084d4 |</span><br><span class="line">| min_disk         | 0                                    |</span><br><span class="line">| min_ram          | 0                                    |</span><br><span class="line">| name             | glance-cirros                        |</span><br><span class="line">| owner            | e0ab86e9a7e947d38ef4b86dfa1e3942     |</span><br><span class="line">| protected        | False                                |</span><br><span class="line">| size             | 13287936                             |</span><br><span class="line">| status           | active                               |</span><br><span class="line">| tags             | []                                   |</span><br><span class="line">| updated_at       | 2022-04-22T05:43:47Z                 |</span><br><span class="line">| virtual_size     | None                                 |</span><br><span class="line">| visibility       | shared                               |</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack project list</span><br><span class="line">+----------------------------------+------------------------------------------------------------------+</span><br><span class="line">| ID                               | Name                                                             |</span><br><span class="line">+----------------------------------+------------------------------------------------------------------+</span><br><span class="line">| 88585c35bd8d483abf671329fa9b1a7a | service                                                          |</span><br><span class="line">| 89de021d5c934f3cb70e397d0d6eb3c7 | e0ab86e9a7e947d38ef4b86dfa1e3942-e2eb5f10-5316-4ba8-857d-23b75de |</span><br><span class="line">| b4bc592766104dd3912e4532c53ebaa1 | demo                                                             |</span><br><span class="line">| e0ab86e9a7e947d38ef4b86dfa1e3942 | admin                                                            |</span><br><span class="line">+----------------------------------+------------------------------------------------------------------+</span><br><span class="line">[root@controller ~]# glance member-create b18ba8b5-a016-4a0d-9115-9748d23084d4            b4bc592766104dd3912e4532c53ebaa1</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">| Image ID                             | Member ID                        | Status  |</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">| b18ba8b5-a016-4a0d-9115-9748d23084d4 | b4bc592766104dd3912e4532c53ebaa1 | pending |</span><br><span class="line">+--------------------------------------+----------------------------------+---------+</span><br><span class="line">[root@controller ~]# glance member-update b18ba8b5-a016-4a0d-9115-9748d23084d4 b4bc592766104dd3912e4532c53ebaa1 accepted</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br><span class="line">| Image ID                             | Member ID                        | Status   |</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br><span class="line">| b18ba8b5-a016-4a0d-9115-9748d23084d4 | b4bc592766104dd3912e4532c53ebaa1 | accepted |</span><br><span class="line">+--------------------------------------+----------------------------------+----------+</span><br></pre></td></tr></table></figure>

<p><strong>6、【实操题】修改glance存储后端（2分）</strong></p>
<p>在提供的OpenStack私有云平台，创建一台云主机（镜像使用CentOS7.5，flavor使用带临时磁盘50G的），配置该主机为nfs的server端，将该云主机中的&#x2F;mnt&#x2F;test目录进行共享（目录不存在可自行创建）。然后配置controller节点为nfs的client端，要求将&#x2F;mnt&#x2F;test目录作为glance后端存储的挂载目录。配置完成后提交controller控制节点的用户名、密码和IP地址到答题框。</p>
<p><strong>nfs服务端：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@nfs ~]# umount /mnt</span><br><span class="line">[root@nfs ~]# lsblk</span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  100G  0 disk </span><br><span class="line">└─vda1 253:1    0  100G  0 part /</span><br><span class="line">vdb    253:16   0   50G  0 disk </span><br><span class="line">[root@nfs ~]# </span><br><span class="line">[root@nfs ~]# mkfs.ext4 /dev/vdb</span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">文件系统标签=</span><br><span class="line">OS type: Linux</span><br><span class="line">块大小=4096 (log=2)</span><br><span class="line">分块大小=4096 (log=2)</span><br><span class="line">Stride=0 blocks, Stripe width=0 blocks</span><br><span class="line">3276800 inodes, 13107200 blocks</span><br><span class="line">655360 blocks (5.00%) reserved for the super user</span><br><span class="line">第一个数据块=0</span><br><span class="line">Maximum filesystem blocks=2162163712</span><br><span class="line">400 block groups</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">8192 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, </span><br><span class="line">        4096000, 7962624, 11239424</span><br><span class="line">Allocating group tables: 完成                            </span><br><span class="line">正在写入inode表: 完成                            </span><br><span class="line">Creating journal (32768 blocks): 完成</span><br><span class="line">Writing superblocks and filesystem accounting information: 完成   </span><br><span class="line">[root@nfs ~]# mount /dev/vdb /mnt</span><br><span class="line">[root@nfs ~]# lsblk</span><br><span class="line">NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  100G  0 disk </span><br><span class="line">└─vda1 253:1    0  100G  0 part /</span><br><span class="line">vdb    253:16   0   50G  0 disk /mnt</span><br></pre></td></tr></table></figure>

<p><code>yum install -y nfs-utils rpcbind</code></p>
<p> <code>mkdir /mnt/test</code></p>
<p><code>echo /mnt/test *(rw,async,no_root_squash) &gt; /etc/exports</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@nfs ~]# systemctl restart rpcbind</span><br><span class="line">[root@nfs ~]# systemctl restart nfs</span><br><span class="line">[root@nfs ~]# systemctl enable rpcbind</span><br><span class="line">[root@nfs ~]# systemctl enable nfs</span><br></pre></td></tr></table></figure>

<p><strong>nfs客户端：</strong></p>
<p><code>echo 192.168.20.106:/mnt/test /var/lib/glance/images nfs default netdev 0 0 &gt; /etc/fstab</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# systemctl restart rpcbind</span><br><span class="line">[root@controller ~]# systemctl restart nfs</span><br><span class="line">[root@controller ~]# systemctl enable rpcbind</span><br><span class="line">[root@controller ~]# systemctl enable nfs</span><br><span class="line">[root@controller ~]# showmount -e 192.168.20.106</span><br><span class="line">Export list for 192.168.20.106:</span><br><span class="line">/mnt/test *</span><br><span class="line">[root@controller ~]# mount -t nfs 192.168.20.106:/mnt/test /var/lib/glance/images/</span><br><span class="line">[root@controller ~]# chown -R glance:glance /var/lib/glance/images/</span><br><span class="line">[root@controller ~]# ll /var/lib/glance/</span><br><span class="line">总用量 4</span><br><span class="line">drwxr-xr-x 2 glance glance 4096 4月  25 06:18 images</span><br><span class="line">[root@controller ~]# mount </span><br><span class="line">...............................................................</span><br><span class="line">192.168.20.106:/mnt/test on /var/lib/glance/images type nfs4 (rw,relatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=192.168.20.105,local_lock=none,addr=192.168.20.106)</span><br></pre></td></tr></table></figure>

<p><strong>7、【实操题】Raid管理（1分）</strong></p>
<p>在提供的OpenStack私有云平台，创建一台云主机，flavor使用带有50G临时磁盘的，然后在云主机上对云硬盘进行操作。要求分出4个大小为5G的分区，使用这4个分区，创建名为&#x2F;dev&#x2F;md5、raid级别为5的磁盘阵列加一个热备盘（&#x2F;dev&#x2F;vdb4为热备盘）。完成后提交云主机的用户名、密码和IP地址到答题框。</p>
<p><code>yum install -y mdadm</code></p>
<p><code>umount /mnt</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@raid ~]# fdisk /dev/vdb</span><br><span class="line">Welcome to fdisk (util-linux 2.23.2).</span><br><span class="line">Changes will remain in memory only, until you decide to write them.</span><br><span class="line">Be careful before using the write command.</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (0 primary, 0 extended, 4 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">Partition number (1-4, default 1): </span><br><span class="line">First sector (2048-104857599, default 2048): </span><br><span class="line">Using default value 2048</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (2048-104857599, default 104857599): +5G</span><br><span class="line">Partition 1 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (1 primary, 0 extended, 3 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">Partition number (2-4, default 2): </span><br><span class="line">First sector (10487808-104857599, default 10487808): </span><br><span class="line">Using default value 10487808</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (10487808-104857599, default 104857599): +5G</span><br><span class="line">Partition 2 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (2 primary, 0 extended, 2 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default p): </span><br><span class="line">Using default response p</span><br><span class="line">Partition number (3,4, default 3): </span><br><span class="line">First sector (20973568-104857599, default 20973568): </span><br><span class="line">Using default value 20973568</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (20973568-104857599, default 104857599): +5G  </span><br><span class="line">Partition 3 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): n</span><br><span class="line">Partition type:</span><br><span class="line">   p   primary (3 primary, 0 extended, 1 free)</span><br><span class="line">   e   extended</span><br><span class="line">Select (default e): p</span><br><span class="line">Selected partition 4</span><br><span class="line">First sector (31459328-104857599, default 31459328): </span><br><span class="line">Using default value 31459328</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (31459328-104857599, default 104857599): +5G</span><br><span class="line">Partition 4 of type Linux and of size 5 GiB is set</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 1</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 2</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 3</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): t</span><br><span class="line">Partition number (1-4, default 4): 4</span><br><span class="line">Hex code (type L to list all codes): fd</span><br><span class="line">Changed type of partition &#x27;Linux&#x27; to &#x27;Linux raid autodetect&#x27;</span><br><span class="line">Command (m for help): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br></pre></td></tr></table></figure>

<p><code>mdadm -C /dev/md5 -l 5 -n 3 -x 1 /dev/vdb1 /dev/vdb2 /dev/vdb3 /dev/vdb4</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@raid ~]# mdadm -D /dev/md5 </span><br><span class="line">/dev/md5:</span><br><span class="line">           Version : 1.2</span><br><span class="line">     Creation Time : Fri Apr 22 07:34:32 2022</span><br><span class="line">        Raid Level : raid5</span><br><span class="line">        Array Size : 10475520 (9.99 GiB 10.73 GB)</span><br><span class="line">     Used Dev Size : 5237760 (5.00 GiB 5.36 GB)</span><br><span class="line">      Raid Devices : 3</span><br><span class="line">     Total Devices : 4</span><br><span class="line">       Persistence : Superblock is persistent</span><br><span class="line">   Update Time : Fri Apr 22 07:40:00 2022</span><br><span class="line">         State : clean </span><br><span class="line">Active Devices : 3</span><br><span class="line">   Working Devices : 4</span><br><span class="line">    Failed Devices : 0</span><br><span class="line">     Spare Devices : 1</span><br><span class="line">        Layout : left-symmetric</span><br><span class="line">    Chunk Size : 512K</span><br><span class="line">Consistency Policy : resync</span><br><span class="line">          Name : raid.novalocal:5  (local to host raid.novalocal)</span><br><span class="line">          UUID : a7ee7f6c:33942c54:654cf6c9:880cc731</span><br><span class="line">        Events : 20</span><br><span class="line">Number   Major   Minor   RaidDevice State</span><br><span class="line">   0     253       17        0      active sync   /dev/vdb1</span><br><span class="line">   1     253       18        1      active sync   /dev/vdb2</span><br><span class="line">   4     253       19        2      active sync   /dev/vdb3</span><br><span class="line">   3     253       20        -      spare   /dev/vdb4</span><br></pre></td></tr></table></figure>

<p><strong>8、【实操题】redis主从（1分）</strong></p>
<p>使用提供的OpenStack私有云平台，申请两台CentOS7.5系统的云主机，使用提供的http源，在两个节点自行安装redis服务并启动，配置redis的访问需要密码，密码设置为123456。然后将这两个redis节点配置为redis的主从架构。配置完成后提交redis主节点的用户名、密码和IP地址到答题框。</p>
<p>主节点：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf </span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">requirepass 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<p>从节点：</p>
<p><code>yum install -y redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vi /etc/redis.conf</span><br><span class="line">bind 0.0.0.0</span><br><span class="line">protected-mode no</span><br><span class="line">slaveof 192.168.20.129 6379</span><br><span class="line">masterauth 123456</span><br></pre></td></tr></table></figure>

<p><code>systemctl restart redis</code></p>
<p><code>systemctl enable  redis</code></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@redis-2 ~]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; info</span><br><span class="line">.................</span><br><span class="line"># Replication</span><br><span class="line">role:slave</span><br><span class="line">master_host:192.168.20.129</span><br><span class="line">master_port:6379</span><br><span class="line">master_link_status:up</span><br><span class="line">master_last_io_seconds_ago:7</span><br><span class="line">master_sync_in_progress:0</span><br><span class="line">slave_repl_offset:589</span><br><span class="line">slave_priority:100</span><br><span class="line">slave_read_only:1</span><br><span class="line">connected_slaves:0</span><br><span class="line">master_repl_offset:0</span><br><span class="line">repl_backlog_active:0</span><br><span class="line">repl_backlog_size:1048576</span><br><span class="line">repl_backlog_first_byte_offset:0</span><br><span class="line">repl_backlog_histlen:0</span><br><span class="line">....................</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>云计算准备任务</title>
    <url>/2022/03/30/%E4%BA%91%E8%AE%A1%E7%AE%97%E5%87%86%E5%A4%87%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h1 id="连接云主机教程"><a href="#连接云主机教程" class="headerlink" title="连接云主机教程"></a>连接云主机教程</h1><p><strong>1.连接    NETGEAR    WIFI</strong></p>
<p><strong>2.设置ip</strong><span id="more"></span></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ip:172.19.25.1xx(xx大于20)</span><br><span class="line"></span><br><span class="line">mask:255.255.255.0</span><br><span class="line"></span><br><span class="line">gateway:172.19.25.1</span><br><span class="line"></span><br><span class="line">dns:172.18.0.2</span><br></pre></td></tr></table></figure>

<p><strong>3.配置静态路由</strong></p>
<p>以管理员模式进入cmd</p>
<p>输入</p>
<p><code>route add 192.168.10.0 mask 255.255.255.0 172.19.25.2 -p</code></p>
<p><code>route add 192.168.20.0 mask 255.255.255.0 172.19.25.2 -p</code></p>
<p><strong>4.进入openstack</strong></p>
<p>浏览器输入192.168.10.10&#x2F;dashboard</p>
<p>domian:demo</p>
<p>user:admin</p>
<p>pass:000000</p>
<p>项目&gt;资源管理&gt;云主机</p>
<p>选择cs1—cs11的一台云主机</p>
<p>用securecrt连接 ex-net ip</p>
]]></content>
      <tags>
        <tag>云计算</tag>
      </tags>
  </entry>
  <entry>
    <title>搭建博客艰苦历程</title>
    <url>/2022/03/28/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%E8%89%B0%E8%8B%A6%E5%8E%86%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="搭建博客艰苦历程"><a href="#搭建博客艰苦历程" class="headerlink" title="搭建博客艰苦历程"></a>搭建博客艰苦历程</h1><h2 id="现在前面的话"><a href="#现在前面的话" class="headerlink" title="现在前面的话"></a>现在前面的话</h2><p>今天刷朋友圈看见了某大佬分享了自己的Blog，看他的Blog把我给看麻了┭┮﹏┭┮，<span id="more"></span>我就想着也搭一个Blog，其实我很早就有了这个想法，但是我  <del>学业繁忙</del>  太懒了就一直没有开始搭建。今天我要下定决心搭建完这个博客！</p>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="安装git"><a href="#安装git" class="headerlink" title="安装git"></a>安装git</h3><h6 id="Git-for-Windows"><a href="#Git-for-Windows" class="headerlink" title="Git for Windows"></a><a href="https://gitforwindows.org/">Git for Windows</a></h6><h3 id="安装node-js"><a href="#安装node-js" class="headerlink" title="安装node.js"></a>安装node.js</h3><p><a href="https://nodejs.org/en/download/">Download | Node.js (nodejs.org)</a>    </p>
<p>下载LTS版就可以了</p>
<p>安装完后打开git安装目录下的git-bash.exe</p>
<p><code>node -v</code></p>
<p><code>npm -v</code></p>
<p>可以检测有没有安装成功，如果成功显示出版本号就是安装成功反之就是失败。</p>
<h3 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h3><p>打开git-bash.exe后可以创建一个文件夹</p>
<p><code>mkdir blog</code></p>
<p>进入该文件夹</p>
<p><code>cd blog</code></p>
<p>输入命令</p>
<p><code>npm install -g hexo-cli</code></p>
<p>hexo -v可以查看版本</p>
<p>初始化hexo</p>
<p><code>hexo init myblog</code></p>
<p>然后进入myblog</p>
<p><code>cd myblog</code></p>
<p><code>npm install</code></p>
<p>新建完成后，指定文件夹目录下有：</p>
<ul>
<li>node_modules: 依赖包</li>
<li>public：存放生成的页面</li>
<li>scaffolds：生成文章的一些模板</li>
<li>source：用来存放你的文章</li>
<li>themes：主题</li>
<li>** _config.yml: 博客的配置文件**</li>
</ul>
<p><code>hexo g</code></p>
<p><code>hexo s</code></p>
<p>这样就打开了hexo服务，启动浏览器输入localhost:4000就可以看到你的Blog啦！</p>
<p>当然，这只能自娱自乐，哈哈哈哈哈。要想让大家都能看到你的Blog就继续往下看啦！</p>
<h3 id="创建自己的Github个人仓库"><a href="#创建自己的Github个人仓库" class="headerlink" title="创建自己的Github个人仓库"></a>创建自己的Github个人仓库</h3><p>当然你得有一个个人账户去注册一个吧！</p>
<p>访问<a href="https://github.com/">GitHub</a></p>
<p>点击右上角的sign up就可以开始注册</p>
<p>注册完登录后，在GitHub.com中看到一个New repository，新建仓库</p>
<p>创建一个和你用户名相同的仓库，后面加.github.io，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是xxxx.github.io，其中xxx就是你注册GitHub的用户名。</p>
<p>点击create repository。</p>
<h3 id="生成SSH添加到GitHub"><a href="#生成SSH添加到GitHub" class="headerlink" title="生成SSH添加到GitHub"></a>生成SSH添加到GitHub</h3><p>回到git-bash中</p>
<p><code>git config --global user.name &quot;yourname&quot;</code></p>
<p><code>git config --global user.email &quot;youremail&quot;</code></p>
<p>这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。</p>
<p>可以用以下两条，检查一下你有没有输对</p>
<p><code>git config user.name</code></p>
<p><code>git config user.email</code></p>
<p>然后创建SSH,无脑回车</p>
<p><code>ssh-keygen -t rsa -C &quot;youremail&quot;</code></p>
<p>youremail是你GitHub的邮箱，不要输错！！！</p>
<p>这个时候它会告诉你已经生成了.ssh的文件夹。在你的电脑中找到这个文件夹。</p>
<p>应该在c:&#x2F;&#x2F;用户&#x2F;用户名&#x2F;.ssh&#x2F;下</p>
<p>ssh，简单来讲，就是一个秘钥，其中，id_rsa是你这台电脑的私人秘钥，不能给别人看的，id_rsa.pub是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。</p>
<p>而后在GitHub的setting中，找到SSH keys的设置选项，点击New SSH key，把你的id_rsa.pub里面的信息复制进去。</p>
<p><code>ssh -T git@github.com</code></p>
<p>这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件 <code>_config.yml</code>，翻到最后，修改为</p>
<p><code>deploy:</code>  </p>
<p><code>type: git</code>  </p>
<p><code>repo: https://github.com/YourgithubName/YourgithubName.github.io.git</code>  </p>
<p><code>branch: master</code></p>
<p>YourgithubName就是你的GitHub账户</p>
<p>这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。</p>
<p><code>npm install hexo-deployer-git --save</code></p>
<p>然后</p>
<p><code>hexo clean</code></p>
<p><code>hexo generate</code> </p>
<p><code>hexo deploy</code></p>
<p>其中 <code>hexo clean</code>清除了你之前生成的东西，也可以不加。<br><code>hexo generate</code> 顾名思义，生成静态文章，可以用 <code>hexo g</code>缩写<br><code>hexo deploy</code> 部署文章，可以用<code>hexo d</code>缩写</p>
<p>过一会儿就可以在<code>http://yourname.github.io</code> 这个网站看到你的Blog了！！</p>
<h3 id="设置个人域名"><a href="#设置个人域名" class="headerlink" title="设置个人域名"></a>设置个人域名</h3><p>现在你的个人网站是<code>http://yourname.github.io</code>，如果你想个性一点就要设置你的个人域名了（要钱）！</p>
<p>注册一个阿里云账户,在<a href="https://wanwang.aliyun.com/?spm=5176.8142029.digitalization.2.e9396d3e46JCc5">阿里云</a>上买一个域名，我买的是 <a href="">jjjjy.icu</a>，各个后缀的价格不太一样，比如最广泛的.com就比较贵，看个人喜好咯。</p>
<p>你需要先进行实名认证，然后购买属于你的域名。</p>
<p>购买后点击右上角的控制台</p>
<p>点击域名&gt;点击全部域名&gt;点击解析&gt;点击添加记录</p>
<p>记录类型：CNAME</p>
<p>主机记录：@</p>
<p>解析线路：默认</p>
<p>记录值：yourname.github.io</p>
<p>点击确认</p>
<p>可以再添加一条将@改为www，其余不变。</p>
<p>登录<a href="https://github.com/">GitHub</a>，选择你创建的仓库，点击settings，点击pages，设置Custom domain，输入你购买的域名</p>
<p>然后在你的博客文件source中创建一个名为CNAME文件，不要后缀。写上你的域名。</p>
<p>最后，在gitbash中，输入</p>
<p><code>hexo clean</code></p>
<p><code>hexo g</code></p>
<p><code>hexo d</code></p>
<p>过不了多久，再打开你的浏览器，输入你自己的域名，就可以看到搭建的网站啦！</p>
<p>接下来你就可以正式开始写文章了。</p>
<p><code>hexo new 标题</code></p>
<p>然后在source&#x2F;_post中打开后缀为.md的文件，就可以开始编辑了。当你写完的时候，再</p>
<p><code>hexo clean</code></p>
<p><code>hexo g</code></p>
<p><code>hexo d</code></p>
<p>就可以看到更新了。</p>
<h3 id="hexo基本配置"><a href="#hexo基本配置" class="headerlink" title="hexo基本配置"></a>hexo基本配置</h3><p>在文件根目录下的<code>_config.yml</code>，就是整个hexo框架的配置文件了。可以在里面修改大部分的配置。详细可参考<a href="https://hexo.io/zh-cn/docs/configuration">官方的配置</a>描述。</p>
]]></content>
  </entry>
  <entry>
    <title>金砖国赛样题三</title>
    <url>/2022/09/19/%E9%87%91%E7%A0%96%E5%9B%BD%E8%B5%9B%E6%A0%B7%E9%A2%98%E4%B8%89/</url>
    <content><![CDATA[<h1 id="2022金砖国家职业技能大赛样题"><a href="#2022金砖国家职业技能大赛样题" class="headerlink" title="2022金砖国家职业技能大赛样题"></a>2022金砖国家职业技能大赛样题</h1><h2 id="C-场次题目：企业级应用的自动化部署和运维"><a href="#C-场次题目：企业级应用的自动化部署和运维" class="headerlink" title="C 场次题目：企业级应用的自动化部署和运维"></a>C 场次题目：企业级应用的自动化部署和运维<span id="more"></span></h2><h3 id="任务-1-企业级应用的自动化部署（15-分）"><a href="#任务-1-企业级应用的自动化部署（15-分）" class="headerlink" title="任务 1 企业级应用的自动化部署（15 分）"></a>任务 1 企业级应用的自动化部署（15 分）</h3><ol>
<li>ansible 自动化运维工具的安装【3 分】</li>
</ol>
<p>请使用提供的软件包在 master 节点安装 ansible，安装完成后使用 ansible –version 命令验证是否安装成功。为所有节点添加 test 用户，设置用户密 码为 000000。为 test 用户设置免密 sudo，配置 ssh 免密登录，使 master 节点 能够免密登录所有节点的 test 用户。 将 ansible –version 命令和回显粘贴到答题框。</p>
<p><strong>安装ansible</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install  -y ansible</span><br><span class="line">[root@master ~]#  ansible --version</span><br><span class="line">ansible 2.9.10</span><br><span class="line">  config file = /etc/ansible/ansible.cfg</span><br><span class="line">  configured module search path = [u&#x27;/root/.ansible/plugins/modules&#x27;, u&#x27;/usr/share/ansible/plugins/modules&#x27;]</span><br><span class="line">  ansible python module location = /usr/lib/python2.7/site-packages/ansible</span><br><span class="line">  executable location = /usr/bin/ansible</span><br><span class="line">  python version = 2.7.5 (default, Apr 11 2018, 07:36:10) [GCC 4.8.5 20150623 (Red Hat 4.8.5-28)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>为所有节点添加 test 用户，设置用户密 码为 000000为所有节点添加 test 用户，设置用户密 码为 000000</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# useradd test</span><br><span class="line">[root@master ~]# passwd test</span><br></pre></td></tr></table></figure>

<p><strong>为 test 用户设置免密 sudo</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# chmod u+w /etc/sudoers</span><br><span class="line">[root@master ~]# vi /etc/sudoers</span><br><span class="line">test	ALL=(ALL)	NOPASSWD: ALL    #添加这一行</span><br></pre></td></tr></table></figure>

<p><strong>配置 ssh 免密登录</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# ssh-keygen</span><br><span class="line">[root@master ~]# ssh-copy-id test@master</span><br><span class="line">[root@master ~]# ssh-copy-id test@node1</span><br><span class="line">[root@master ~]# ssh-copy-id test@node2</span><br></pre></td></tr></table></figure>

<p>2.ansible 自动化运维工具的初始化【3 分】 </p>
<p>创建 &#x2F;root&#x2F;ansible 目录作为工作目录，在该目录内创建 ansible.cfg 文 件并完成以下配置，清单文件位置为 &#x2F;root&#x2F;ansible&#x2F;inventory，登录用户为 t est，登录时不需要输入密码。设置并行主机数量为 2，允许 test 用户免密提权 到 root。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir ansible</span><br><span class="line">cd ansible</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat ansible.cfg</span><br><span class="line">[defaults]</span><br><span class="line">inventory      = /root/ansible/inventory    #清单文件</span><br><span class="line">forks          = 2</span><br><span class="line">ask_pass      = False</span><br><span class="line">remote_user = test   #登录用户</span><br><span class="line">[privilege_escalation]</span><br><span class="line">become=True</span><br><span class="line">become_method=sudo</span><br><span class="line">become_user=root</span><br><span class="line">become_ask_pass=False</span><br></pre></td></tr></table></figure>

<p>3.主机清单的编写【2 分】 </p>
<p>编写主机清单文件，创建 master 用户组，master 用户组内添加 master 主 机；创建 node 用户组，node 组内添加 node1 和 node2 主机，主机名不得使用 I P 地址。 完成后执行 ansible-inventory –list 、ansible all -m ping 和 ansib le all -a “id” 命令，将这三条命令返回结果粘贴到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# cat inventory </span><br><span class="line">[master]</span><br><span class="line">master</span><br><span class="line"></span><br><span class="line">[node]</span><br><span class="line">node1</span><br><span class="line">node2</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# ansible-inventory --list</span><br><span class="line">[WARNING]: Found both group and host with same name: master</span><br><span class="line">&#123;</span><br><span class="line">    &quot;_meta&quot;: &#123;</span><br><span class="line">        &quot;hostvars&quot;: &#123;&#125;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;all&quot;: &#123;</span><br><span class="line">        &quot;children&quot;: [</span><br><span class="line">            &quot;master&quot;, </span><br><span class="line">            &quot;node&quot;, </span><br><span class="line">            &quot;ungrouped&quot;</span><br><span class="line">        ]</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;master&quot;: &#123;</span><br><span class="line">        &quot;hosts&quot;: [</span><br><span class="line">            &quot;master&quot;</span><br><span class="line">        ]</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;node&quot;: &#123;</span><br><span class="line">        &quot;hosts&quot;: [</span><br><span class="line">            &quot;node1&quot;, </span><br><span class="line">            &quot;node2&quot;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">[root@master ansible]# ansible all -m ping</span><br><span class="line">[WARNING]: Found both group and host with same name: master</span><br><span class="line">node1 | SUCCESS =&gt; &#123;</span><br><span class="line">    &quot;ansible_facts&quot;: &#123;</span><br><span class="line">        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;ping&quot;: &quot;pong&quot;</span><br><span class="line">&#125;</span><br><span class="line">node2 | SUCCESS =&gt; &#123;</span><br><span class="line">    &quot;ansible_facts&quot;: &#123;</span><br><span class="line">        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;ping&quot;: &quot;pong&quot;</span><br><span class="line">&#125;</span><br><span class="line">master | SUCCESS =&gt; &#123;</span><br><span class="line">    &quot;ansible_facts&quot;: &#123;</span><br><span class="line">        &quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;changed&quot;: false, </span><br><span class="line">    &quot;ping&quot;: &quot;pong&quot;</span><br><span class="line">&#125;</span><br><span class="line">[root@master ansible]# ansible all -a &quot;id&quot;</span><br><span class="line">[WARNING]: Found both group and host with same name: master</span><br><span class="line">node1 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">uid=0(root) gid=0(root) groups=0(root)</span><br><span class="line">node2 | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">uid=0(root) gid=0(root) groups=0(root)</span><br><span class="line">master | CHANGED | rc=0 &gt;&gt;</span><br><span class="line">uid=0(root) gid=0(root) groups=0(root)</span><br></pre></td></tr></table></figure>

<p>4.使用自动化工具对 master 节点进行初始化【2 分】 </p>
<p>请编写 prometheus.yml 控制 master 主机组，使用对应模块将 SELinux 临时 状态和开机启动状态也设置为 disabled。请使用 ansible 对应模块安装时间同 步服务，使用文本编辑模块将该服务的作用域设置为 0.0.0.0&#x2F;0，并设置状态为 启动和开机自动启动。首先将提供的 prometheus-2.37.0.linux-amd64.tar.gz 使用文件拷贝模块将该压缩包拷贝到目标主机的&#x2F;usr&#x2F;local&#x2F; 下，使用 shell 模 块解压该压缩包。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# cat prometheus.yml</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">  - name: install prometheus</span><br><span class="line">    hosts: master</span><br><span class="line">    tasks:</span><br><span class="line">      - name: disable selinux</span><br><span class="line">        selinux:</span><br><span class="line">          state: disabled</span><br><span class="line">      - name: echo chronyd</span><br><span class="line">        shell: echo &quot;allow 0.0.0.0/0&quot; &gt;&gt; /etc/chrony.conf</span><br><span class="line">      - name: enable chronyd</span><br><span class="line">        service: </span><br><span class="line">          name: chronyd</span><br><span class="line">          state: started</span><br><span class="line">          enabled: true</span><br><span class="line">      - name: copy prometheus</span><br><span class="line">        copy:</span><br><span class="line">          src: /root/ansible/prometheus-2.37.0.linux-amd64.tar.gz</span><br><span class="line">          dest: /usr/local/ </span><br><span class="line">      - name: tar prometheus</span><br><span class="line">        shell: tar -zxvf /usr/local/prometheus-2.37.0.linux-amd64.tar.gz -C /usr/local/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# ansible-playbook prometheus.yml</span><br></pre></td></tr></table></figure>

<p>5.使用自动化运维工具完成企业级应用的部署【5 分】 </p>
<p>编写prometheus.yml.j2模板文件，将所有node节点信息添加到该文件中， 但是被管节点的主机名信息必须使用变量 IP 地址可以手动输入。完成后请创建 node_exporter.yml 文件，编写第一个 play，将该 play 命名为 node，该 play 控制的主机组为 node。使用 ansible 模块将 node_exporter-1.3.1.linux-amd6 4.tar.gz 发送到 node 主机组的 &#x2F;usr&#x2F;local&#x2F; 下，使用一个 shell 模块解压该 压缩包，并启动该服务。随后编写第二个 play，将第二个 play 命名为 master， 第二个 play 控制 master 节点。首先使用 ansible 模块将 prometheus.yml.j2 文 件传输到 master 节点，然后使用 script 模块将 prometheus 启动。使用对应模 块将 grafana-8.1.2-1.x86_64.rpm 包发送到被控节点的 &#x2F;mnt&#x2F; 目录下，然后使用对应模块将该软件包安装，安装完成后设置 grafana 服务启动并设置开机自动 启动。使用浏览器登录 prometheus 查看 prometheus 是否成功监控所有 node 节 点。 请将浏览器反馈的结果截图、prometheus.yml.j2 文件的内容、node_expor ter.yml 文件内容及运行结果提交到答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# cat prometheus.yml.j2 </span><br><span class="line"></span><br><span class="line"># my global config</span><br><span class="line"></span><br><span class="line">global:</span><br><span class="line">  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.</span><br><span class="line">  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.</span><br><span class="line"></span><br><span class="line">  # scrape_timeout is set to the global default (10s).</span><br><span class="line"></span><br><span class="line"># Alertmanager configuration</span><br><span class="line"></span><br><span class="line">alerting:</span><br><span class="line">  alertmanagers:</span><br><span class="line">    - static_configs:</span><br><span class="line">        - targets:</span><br><span class="line">          # - alertmanager:9093</span><br><span class="line"></span><br><span class="line"># Load rules once and periodically evaluate them according to the global &#x27;evaluation_interval&#x27;.</span><br><span class="line"></span><br><span class="line">rule_files:</span><br><span class="line"></span><br><span class="line">  # - &quot;first_rules.yml&quot;</span><br><span class="line"></span><br><span class="line">  # - &quot;second_rules.yml&quot;</span><br><span class="line"></span><br><span class="line"># A scrape configuration containing exactly one endpoint to scrape:</span><br><span class="line"></span><br><span class="line"># Here it&#x27;s Prometheus itself.</span><br><span class="line"></span><br><span class="line">scrape_configs:</span><br><span class="line"></span><br><span class="line">  # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.</span><br><span class="line"></span><br><span class="line">  - job_name: &quot;prometheus&quot;</span><br><span class="line"></span><br><span class="line">    # metrics_path defaults to &#x27;/metrics&#x27;</span><br><span class="line"></span><br><span class="line">    # scheme defaults to &#x27;http&#x27;.</span><br><span class="line"></span><br><span class="line">    static_configs:</span><br><span class="line"></span><br><span class="line">      - targets: [&quot;192.168.20.115:9090&quot;]</span><br><span class="line"></span><br><span class="line">  - job_name: &quot;node&quot;</span><br><span class="line">    static_configs:</span><br><span class="line">    &#123;% for node in groups[&#x27;node&#x27;] %&#125;</span><br><span class="line">      - targets: [&quot;&#123;&#123;node&#125;&#125;:9100&quot;]</span><br><span class="line">    &#123;% endfor %&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# cat node_exporter.yml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">- name: node</span><br><span class="line">  hosts: node</span><br><span class="line">  tasks:</span><br><span class="line">    - name: copy node_exporter</span><br><span class="line">      copy:</span><br><span class="line">        src: /root//ansible/node_exporter-1.3.1.linux-amd64.tar.gz</span><br><span class="line">        dest: /usr/local/</span><br><span class="line"></span><br><span class="line">    - name: install node_exporter</span><br><span class="line">      shell: tar -zxvf /usr/local/node_exporter-1.3.1.linux-amd64.tar.gz -C /usr/local/</span><br><span class="line">    - name: service</span><br><span class="line">      template:</span><br><span class="line">        src: ./node_exporter.service.j2</span><br><span class="line">        dest: /usr/lib/systemd/system/node_exporter.service</span><br><span class="line">    - name: enable node_exporter</span><br><span class="line">      service:</span><br><span class="line">        name: node_exporter</span><br><span class="line">        state: started</span><br><span class="line">        enabled: true</span><br><span class="line"></span><br><span class="line">- name: master</span><br><span class="line">  hosts: master</span><br><span class="line">  tasks:</span><br><span class="line">    - name: copy</span><br><span class="line">      template:</span><br><span class="line">        src: ./prometheus.yml.j2</span><br><span class="line">        dest: /usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml</span><br><span class="line">    - name: service</span><br><span class="line">      template:</span><br><span class="line">        src: ./prometheus.service.j2</span><br><span class="line">        dest: /usr/lib/systemd/system/prometheus.service</span><br><span class="line"></span><br><span class="line">    - name: enable prometheus</span><br><span class="line">      service:</span><br><span class="line">        name: prometheus</span><br><span class="line">        state: started</span><br><span class="line">        enabled: true</span><br><span class="line"></span><br><span class="line">    - name: copy grafana-8.1.2-1.x86_64.rpm</span><br><span class="line">      copy:</span><br><span class="line">        src: /root/ansible/grafana-8.1.2-1.x86_64.rpm</span><br><span class="line">        dest: /mnt/</span><br><span class="line">    - name: install grafana-8.1.2-1.x86_64.rpm</span><br><span class="line">      shell: yum install -y /mnt/*.rpm</span><br><span class="line">    - name: enable grafana-8.1.2-1.x86_64.rpm</span><br><span class="line">      service:</span><br><span class="line">        name: grafana-server</span><br><span class="line">        state: started</span><br><span class="line">        enabled: true</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# cat prometheus.service.j2 </span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=Prometheus Server</span><br><span class="line">Documentation=https://prometheus.io</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/usr/local/prometheus-2.37.0.linux-amd64/prometheus \</span><br><span class="line">--config.file=/usr/local/prometheus-2.37.0.linux-amd64/prometheus.yml \</span><br><span class="line">--storage.tsdb.path=/usr/local/prometheus-2.37.0.linux-amd64/data/ \</span><br><span class="line">--storage.tsdb.retention=15d \</span><br><span class="line">--web.enable-lifecycle</span><br><span class="line"></span><br><span class="line">ExecReload=/bin/kill -HUP $MAINPID</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# cat node_exporter.service.j2 </span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description=node_exporter</span><br><span class="line">Documentation=https://prometheus.io/</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=simple</span><br><span class="line">ExecStart=/usr/local/node_exporter-1.3.1.linux-amd64/node_exporter \</span><br><span class="line">--collector.ntp \</span><br><span class="line">--collector.mountstats \</span><br><span class="line">--collector.systemd \</span><br><span class="line">--collector.tcpstat</span><br><span class="line"></span><br><span class="line">ExecReload=/bin/kill -HUP $MAINPID</span><br><span class="line">Restart=on-failure</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ansible]# ansible-playbook node_exporter.yml </span><br></pre></td></tr></table></figure>

<h3 id="任务-2-企业级应用的运维（12-分）"><a href="#任务-2-企业级应用的运维（12-分）" class="headerlink" title="任务 2 企业级应用的运维（12 分）"></a>任务 2 企业级应用的运维（12 分）</h3><p>1.使用 prometheus 监控 mysqld 服务【3 分】 </p>
<p>将提供的 mysqld_exporter-0.14.0.linux-amd64.tar.gz 发送到 agent 虚 拟机 &#x2F;usr&#x2F;local&#x2F; 目录下解压并安装 mariadb 服务。进入 mariadb 数据库中创 建 mysqld_monitor 用户并授权，然后创建 mariadb 配置文件，内容为数据库用 户名密码。启动 mysqld_exporter 组件确保 9104 端口启动。回到 prometheus 节 点修改 prometheus.yml 文件并添加 mysql 被监控信息。重启 prometheus，随后 web 界面刷新并查看 mysqld 被控信息。</p>
]]></content>
      <tags>
        <tag>应用部署和运维</tag>
      </tags>
  </entry>
  <entry>
    <title>金砖国赛样题</title>
    <url>/2022/09/02/%E9%87%91%E7%A0%96%E5%9B%BD%E8%B5%9B%E6%A0%B7%E9%A2%98%E4%B8%80/</url>
    <content><![CDATA[<h1 id="2022金砖国家职业技能大赛样题"><a href="#2022金砖国家职业技能大赛样题" class="headerlink" title="2022金砖国家职业技能大赛样题"></a>2022金砖国家职业技能大赛样题</h1><h2 id="A-场次题目：OpenStack-平台部署与运维"><a href="#A-场次题目：OpenStack-平台部署与运维" class="headerlink" title="A 场次题目：OpenStack 平台部署与运维"></a><strong>A 场次题目：OpenStack 平台部署与运维</strong><span id="more"></span></h2><h3 id="任务-1-私有云平台环境初始化（5-分）"><a href="#任务-1-私有云平台环境初始化（5-分）" class="headerlink" title="任务 1 私有云平台环境初始化（5 分）"></a>任务 1 私有云平台环境初始化（5 分）</h3><p>1.初始化操作系统 使用提供的用户名密码，登录竞赛云平台。根据表 1 中的 IP 地址规划，设 置各服务器节点的 IP 地址，确保网络正常通信，设置控制节点主机名为 Contro ller，计算节点主机名为 Compute，并修改 hosts 文件将 IP 地址映射为主机名， 关闭防火墙并设置为开机不启动，设置 SELinux 为 Permissive 模式并设置永久 关闭。请查看控制节点和计算节点主机名，使用命令查看 SELinux 状态，使用 head 命令、tail 命令和 cut 命令提取出永久关闭 SELinux 的关键信息。 将以上命令及返回结果提交到答题框。【2 分】 </p>
<p><strong>配置hosts</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.20.105	controller</span><br><span class="line">192.168.20.103	compute</span><br></pre></td></tr></table></figure>

<p><strong>关闭防火墙并设置为开机不启动</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# systemctl stop firewalld</span><br><span class="line">[root@controller ~]# systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<p><strong>设置 SELinux 为 Permissive 模式并设置永久 关闭</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# setenforce 0</span><br><span class="line">[root@controller ~]# sed -i &#x27;s/SELINUX=permissive/SELINUX=disabled/g&#x27; /etc/selinux/config</span><br></pre></td></tr></table></figure>

<p><strong>使用命令查看 SELinux 状态</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# getenforce </span><br><span class="line">Disabled</span><br></pre></td></tr></table></figure>

<p>2.挂载安装光盘镜像 将提供的 CentOS-7-x86_64-DVD-1804.iso 和 chinaskills_cloud_iaas.iso 光盘镜像上传到 Controller 节点 &#x2F;root 目录下，然后在 &#x2F;opt 目录下使用一条 命令创建&#x2F;centos 目录和&#x2F;iaas 目录，并将镜像文件 CentOS-7-x86_64-DVD-1804. iso 挂载到 &#x2F;centos 目录下，将镜像文件 chinaskills_cloud_iaas.iso 挂载到 &#x2F;iaas 目录下。 请将以上命令及返回结果返回到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# curl -O http://172.19.25.11/middle/CentOS-7-x86_64-DVD-1804.iso</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100 4263M  100 4263M    0     0   111M      0  0:00:38  0:00:38 --:--:--  111M</span><br><span class="line">[root@controller ~]# curl -O http://172.19.25.11/middle/chinaskills_cloud_iaas_v1.0.1.iso</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100 3622M  100 3622M    0     0   111M      0  0:00:32  0:00:32 --:--:--  109M</span><br><span class="line">[root@controller ~]# ls</span><br><span class="line">CentOS-7-x86_64-DVD-1804.iso  chinaskills_cloud_iaas_v1.0.1.iso</span><br><span class="line">[root@controller ~]# mount CentOS-7-x86_64-DVD-1804.iso /mnt/</span><br><span class="line">mount: /dev/loop0 is write-protected, mounting read-only</span><br><span class="line">[root@controller ~]# mkdir -p /opt/centos</span><br><span class="line">[root@controller ~]# mkdir -p /opt/iaas</span><br><span class="line">[root@controller ~]# cp -rf /mnt/* /opt/centos/</span><br><span class="line">[root@controller ~]# umount /mnt/</span><br><span class="line">[root@controller ~]# mount chinaskills_cloud_iaas_v1.0.1.iso /mnt/</span><br><span class="line">mount: /dev/loop0 is write-protected, mounting read-only</span><br><span class="line">[root@controller ~]# cp -rf /mnt/* /opt/iaas/</span><br><span class="line">[root@controller ~]# umount /mnt/</span><br></pre></td></tr></table></figure>

<p>3.搭建文件共享服务器 在 Controller 节点上安装 vsftp 服务器，设置开机自启动，请将以上命令 及返回结果提交到答题框。【0.5 分】</p>
<p><strong>配置yum源</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/yum.repos.d/local.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=file:///opt/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[iaas]</span><br><span class="line">name=iaas</span><br><span class="line">baseurl=file:///opt/iaas/iaas-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p><strong>安装vsftpd</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# yum install -y vsftpd</span><br></pre></td></tr></table></figure>

<p><strong>设置开机自启动</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# systemctl enable vsftpd</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/vsftpd.service to /usr/lib/systemd/system/vsftpd.service.</span><br></pre></td></tr></table></figure>

<p>4.设置 yum 源 将 ftp 仓库设置为 &#x2F;opt&#x2F;，为 controller 节点设置本地 yum 源，yum 源文 件名为 local.repo；为 compute 配置 ftp 源，yum 源文件名称为 ftp.repo，其 中 ftp 服务器地址为 controller 节点 IP。 请将两个节点的 yum 源文件内容提交到答题框。【0.5 分】</p>
<p><strong>设置 yum 源 将 ftp 仓库设置为 &#x2F;opt&#x2F;</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# echo anon_root=/opt/ &gt;&gt; /etc/vsftpd/vsftpd.conf </span><br><span class="line">[root@controller ~]# systemctl restart vsftpd</span><br></pre></td></tr></table></figure>

<p><strong>为 compute 配置 ftp 源，yum 源文件名称为 ftp.repo</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# cat /etc/yum.repos.d/ftp.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=ftp://controller/centos</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[iaas]</span><br><span class="line">name=iaas</span><br><span class="line">baseurl=ftp://controller/iaas/iaas-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p>5.部署时间同步服务器 在 Controller 节点上部署 chrony 服务器，允许其他节点同步时间，启动服 务并设置为开机启动；在 compute 节点上指定 controller 节点为上游 NTP 服务 器，重启服务并设为开机启动。 请在控制节点上使用 chronyc 命令同步控制节点的系统时间。【1 分】</p>
<p><strong>controller配置</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/chrony.conf</span><br><span class="line"></span><br><span class="line"># Use public servers from the pool.ntp.org project.</span><br><span class="line"></span><br><span class="line"># Please consider joining the pool (http://www.pool.ntp.org/join.html).</span><br><span class="line"></span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line">server controller iburst</span><br><span class="line"></span><br><span class="line"># Record the rate at which the system clock gains/losses time.</span><br><span class="line"></span><br><span class="line">driftfile /var/lib/chrony/drift</span><br><span class="line"></span><br><span class="line"># Allow the system clock to be stepped in the first three updates</span><br><span class="line"></span><br><span class="line"># if its offset is larger than 1 second.</span><br><span class="line"></span><br><span class="line">makestep 1.0 3</span><br><span class="line"></span><br><span class="line"># Enable kernel synchronization of the real-time clock (RTC).</span><br><span class="line"></span><br><span class="line">rtcsync</span><br><span class="line"></span><br><span class="line"># Enable hardware timestamping on all interfaces that support it.</span><br><span class="line"></span><br><span class="line">#hwtimestamp *</span><br><span class="line"></span><br><span class="line"># Increase the minimum number of selectable sources required to adjust</span><br><span class="line"></span><br><span class="line"># the system clock.</span><br><span class="line"></span><br><span class="line">#minsources 2</span><br><span class="line"></span><br><span class="line"># Allow NTP client access from local network.</span><br><span class="line"></span><br><span class="line">#allow 192.168.0.0/16</span><br><span class="line"></span><br><span class="line"># Serve time even if not synchronized to a time source.</span><br><span class="line"></span><br><span class="line">#local stratum 10</span><br><span class="line"></span><br><span class="line"># Specify file containing keys for NTP authentication.</span><br><span class="line"></span><br><span class="line">#keyfile /etc/chrony.keys</span><br><span class="line"></span><br><span class="line"># Specify directory for log files.</span><br><span class="line"></span><br><span class="line">logdir /var/log/chrony</span><br><span class="line"></span><br><span class="line"># Select which information is logged.</span><br><span class="line"></span><br><span class="line">#log measurements statistics tracking</span><br><span class="line">allow 192.168.20.0/24</span><br><span class="line">local stratum 10</span><br></pre></td></tr></table></figure>

<p><strong>启动服 务并设置为开机启动</strong></p>
<p>[root@controller ~]# systemctl restart chronyd<br>[root@controller ~]# systemctl enable chronyd</p>
<p><strong>compute节点配置</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# cat /etc/chrony.conf </span><br><span class="line"></span><br><span class="line"># Use public servers from the pool.ntp.org project.</span><br><span class="line"></span><br><span class="line"># Please consider joining the pool (http://www.pool.ntp.org/join.html).</span><br><span class="line"></span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line">server controller iburst</span><br><span class="line"></span><br><span class="line"># Record the rate at which the system clock gains/losses time.</span><br><span class="line"></span><br><span class="line">driftfile /var/lib/chrony/drift</span><br><span class="line"></span><br><span class="line"># Allow the system clock to be stepped in the first three updates</span><br><span class="line"></span><br><span class="line"># if its offset is larger than 1 second.</span><br><span class="line"></span><br><span class="line">makestep 1.0 3</span><br><span class="line"></span><br><span class="line"># Enable kernel synchronization of the real-time clock (RTC).</span><br><span class="line"></span><br><span class="line">rtcsync</span><br><span class="line"></span><br><span class="line"># Enable hardware timestamping on all interfaces that support it.</span><br><span class="line"></span><br><span class="line">#hwtimestamp *</span><br><span class="line"></span><br><span class="line"># Increase the minimum number of selectable sources required to adjust</span><br><span class="line"></span><br><span class="line"># the system clock.</span><br><span class="line"></span><br><span class="line">#minsources 2</span><br><span class="line"></span><br><span class="line"># Allow NTP client access from local network.</span><br><span class="line"></span><br><span class="line">#allow 192.168.0.0/16</span><br><span class="line"></span><br><span class="line"># Serve time even if not synchronized to a time source.</span><br><span class="line"></span><br><span class="line">#local stratum 10</span><br><span class="line"></span><br><span class="line"># Specify file containing keys for NTP authentication.</span><br><span class="line"></span><br><span class="line">#keyfile /etc/chrony.keys</span><br><span class="line"></span><br><span class="line"># Specify directory for log files.</span><br><span class="line"></span><br><span class="line">logdir /var/log/chrony</span><br><span class="line"></span><br><span class="line"># Select which information is logged.</span><br><span class="line"></span><br><span class="line">#log measurements statistics tracking</span><br></pre></td></tr></table></figure>

<p><strong>启动服 务并设置为开机启动</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# systemctl restart chronyd</span><br><span class="line">[root@compute ~]# systemctl enable chronyd</span><br></pre></td></tr></table></figure>

<h3 id="任务-2-OpenStack-搭建任务（10-分）"><a href="#任务-2-OpenStack-搭建任务（10-分）" class="headerlink" title="任务 2 OpenStack 搭建任务（10 分）"></a>任务 2 OpenStack 搭建任务（10 分）</h3><p>1.修改变量文件 在控制节点和计算节点上分别安装 iaas-xiandian 软件包，修改配置脚本文件中基本变量（配置脚本文件为&#x2F;etc&#x2F;xiandian&#x2F;openrc.sh）。修改完成后使用 命令生效该变量文件，然后执行 echo $INTERFACE_IP 命令。 请将命令和返回结果提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# yum install -y iaas-xiandian</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# yum install -y iaas-xiandian</span><br></pre></td></tr></table></figure>

<p><strong>修改脚本 注意两个节点都要</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/xiandian/openrc.sh </span><br><span class="line">#--------------------system Config--------------------##</span><br><span class="line">#Controller Server Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP=192.168.20.105</span><br><span class="line"></span><br><span class="line">#Controller HOST Password. example:000000 </span><br><span class="line">HOST_PASS=000000</span><br><span class="line"></span><br><span class="line">#Controller Server hostname. example:controller</span><br><span class="line">HOST_NAME=controller</span><br><span class="line"></span><br><span class="line">#Compute Node Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP_NODE=192.168.20.103</span><br><span class="line"></span><br><span class="line">#Compute HOST Password. example:000000 </span><br><span class="line">HOST_PASS_NODE=000000</span><br><span class="line"></span><br><span class="line">#Compute Node hostname. example:compute</span><br><span class="line">HOST_NAME_NODE=compute</span><br><span class="line"></span><br><span class="line">#--------------------Chrony Config-------------------##</span><br><span class="line">#Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)</span><br><span class="line">network_segment_IP=192.168.20.0/24</span><br><span class="line"></span><br><span class="line">#--------------------Rabbit Config ------------------##</span><br><span class="line">#user for rabbit. example:openstack</span><br><span class="line">RABBIT_USER=openstack</span><br><span class="line"></span><br><span class="line">#Password for rabbit user .example:000000</span><br><span class="line">RABBIT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------MySQL Config---------------------##</span><br><span class="line">#Password for MySQL root user . exmaple:000000</span><br><span class="line">DB_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Keystone Config------------------##</span><br><span class="line">#Password for Keystore admin user. exmaple:000000</span><br><span class="line">DOMAIN_NAME=demo</span><br><span class="line">ADMIN_PASS=000000</span><br><span class="line">DEMO_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql keystore user. exmaple:000000</span><br><span class="line">KEYSTONE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Glance Config--------------------##</span><br><span class="line">#Password for Mysql glance user. exmaple:000000</span><br><span class="line">GLANCE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore glance user. exmaple:000000</span><br><span class="line">GLANCE_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Nova Config----------------------##</span><br><span class="line">#Password for Mysql nova user. exmaple:000000</span><br><span class="line">NOVA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore nova user. exmaple:000000</span><br><span class="line">NOVA_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Neturon Config-------------------##</span><br><span class="line">#Password for Mysql neutron user. exmaple:000000</span><br><span class="line">NEUTRON_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore neutron user. exmaple:000000</span><br><span class="line">NEUTRON_PASS=000000</span><br><span class="line"></span><br><span class="line">#metadata secret for neutron. exmaple:000000</span><br><span class="line">METADATA_SECRET=000000</span><br><span class="line"></span><br><span class="line">#Tunnel Network Interface. example:x.x.x.x</span><br><span class="line">INTERFACE_IP=192.168.20.105    #（192.168.20.103）</span><br><span class="line"></span><br><span class="line">#External Network Interface. example:eth1</span><br><span class="line">INTERFACE_NAME=eth1</span><br><span class="line"></span><br><span class="line">#External Network The Physical Adapter. example:provider</span><br><span class="line">Physical_NAME=provider</span><br><span class="line"></span><br><span class="line">#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101</span><br><span class="line">minvlan=101</span><br><span class="line"></span><br><span class="line">#Last Vlan ID in VLAN RANGE for VLAN Network. example:200</span><br><span class="line">maxvlan=200</span><br><span class="line"></span><br><span class="line">#--------------------Cinder Config--------------------##</span><br><span class="line">#Password for Mysql cinder user. exmaple:000000</span><br><span class="line">CINDER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore cinder user. exmaple:000000</span><br><span class="line">CINDER_PASS=000000</span><br><span class="line"></span><br><span class="line">#Cinder Block Disk. example:md126p3</span><br><span class="line">BLOCK_DISK=vdb1</span><br><span class="line"></span><br><span class="line">#--------------------Swift Config---------------------##</span><br><span class="line">#Password for Keystore swift user. exmaple:000000</span><br><span class="line">SWIFT_PASS=000000</span><br><span class="line"></span><br><span class="line">#The NODE Object Disk for Swift. example:md126p4.</span><br><span class="line">OBJECT_DISK=vdb2</span><br><span class="line"></span><br><span class="line">#The NODE IP for Swift Storage Network. example:x.x.x.x.</span><br><span class="line">STORAGE_LOCAL_NET_IP=000000</span><br><span class="line"></span><br><span class="line">#--------------------Heat Config----------------------##</span><br><span class="line">#Password for Mysql heat user. exmaple:000000</span><br><span class="line">HEAT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore heat user. exmaple:000000</span><br><span class="line">HEAT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Zun Config-----------------------##</span><br><span class="line">#Password for Mysql Zun user. exmaple:000000</span><br><span class="line">ZUN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Zun user. exmaple:000000</span><br><span class="line">ZUN_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql Kuryr user. exmaple:000000</span><br><span class="line">KURYR_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Kuryr user. exmaple:000000</span><br><span class="line">KURYR_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Ceilometer Config----------------##</span><br><span class="line">#Password for Gnocchi ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------AODH Config----------------##</span><br><span class="line">#Password for Mysql AODH user. exmaple:000000</span><br><span class="line">AODH_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore AODH user. exmaple:000000</span><br><span class="line">AODH_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Barbican Config----------------##</span><br><span class="line">#Password for Mysql Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_PASS=000000</span><br></pre></td></tr></table></figure>

<p><strong>生效该变量文件，然后执行 echo $INTERFACE_IP 命令</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# source /etc/xiandian/openrc.sh </span><br><span class="line">[root@controller ~]# echo $INTERFACE_IP</span><br><span class="line">192.168.20.105</span><br></pre></td></tr></table></figure>

<p><strong>执行脚本</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-pre-host.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-pre-host.sh </span><br></pre></td></tr></table></figure>

<p>2.搭建数据库组件 使用提供的脚本框架 iaas-install-mysql.sh 填充脚本，在 controller  节点上安装 mariadb、mencached、rabbitmq 等服务并完成相关配置。完成后修 改配置文件将 mencached 最大连接数修改为 2048。 请将修改后的配置文件提交到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-mysql.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/sysconfig/memcached </span><br><span class="line">PORT=&quot;11211&quot;</span><br><span class="line">USER=&quot;memcached&quot;</span><br><span class="line">MAXCONN=&quot;2048&quot;</span><br><span class="line">CACHESIZE=&quot;64&quot;</span><br><span class="line">OPTIONS=&quot;-l 127.0.0.1,::1,controller&quot;</span><br></pre></td></tr></table></figure>

<p>3.搭建认证服务组件 使用提供的脚本框架 iaas-install-keystone.sh 填充脚本，在 controlle r 节点上安装 keystone 服务并完成相关配置。完成后使用 openstack 命令请求 一个 token。 请将以上命令和返回结果提交到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-keystone.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack token issue</span><br><span class="line">+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Field      | Value                                                                                                                                                                                   |</span><br><span class="line">+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| expires    | 2022-09-02T22:55:39+0000                                                                                                                                                                |</span><br><span class="line">| id         | gAAAAABjEnvb7SeEOXtFuO4RiJ7fRCcIK6Eh_LTwtH9uHjdOblLrJFOjbhiw3ukzo7Tey8jSQGwO2XIwBXrxU4AoMseIaFvzVQAiUdtWZjt5mKlXrGkbtUJTR5bhn9ktgqWHC5DsQtlxmmBLJq8-SUL5RG1CKGmknq-hOOFSsFaBQfbSH2fDeWQ |</span><br><span class="line">| project_id | 4350b89460a148d7bf1b2ae63296a6bd                                                                                                                                                        |</span><br><span class="line">| user_id    | 4aec1580c77d4222964b7947d3239a88                                                                                                                                                        |</span><br><span class="line">+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>4.搭建镜像服务组件 使用提供的脚本框架 iaas-install-glance.sh 填充脚本，在 controller  节点上安装 glance 服务并完成相关配置。完成后请将 cirros-0.3.4-x86_64-d isk.img 上传到控制节点的 &#x2F;root 目录下，然后使用 openstack 命令将该镜像 上传到 openstack 平台镜像命名为 cirros。 请将镜像上传的操作命令和返回结果提交到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-glance.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image create cirros --disk-format qcow2 --container bare &lt; cirros-0.3.4-x86_64-disk.img </span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| Field            | Value                                                |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br><span class="line">| checksum         | ee1eca47dc88f4879d8a229cc70a07c6                     |</span><br><span class="line">| container_format | bare                                                 |</span><br><span class="line">| created_at       | 2022-09-02T21:59:43Z                                 |</span><br><span class="line">| disk_format      | qcow2                                                |</span><br><span class="line">| file             | /v2/images/cbfdbd87-71df-4be7-8eb2-6904f781239c/file |</span><br><span class="line">| id               | cbfdbd87-71df-4be7-8eb2-6904f781239c                 |</span><br><span class="line">| min_disk         | 0                                                    |</span><br><span class="line">| min_ram          | 0                                                    |</span><br><span class="line">| name             | cirros                                               |</span><br><span class="line">| owner            | 4350b89460a148d7bf1b2ae63296a6bd                     |</span><br><span class="line">| protected        | False                                                |</span><br><span class="line">| schema           | /v2/schemas/image                                    |</span><br><span class="line">| size             | 13287936                                             |</span><br><span class="line">| status           | active                                               |</span><br><span class="line">| tags             |                                                      |</span><br><span class="line">| updated_at       | 2022-09-02T21:59:44Z                                 |</span><br><span class="line">| virtual_size     | None                                                 |</span><br><span class="line">| visibility       | shared                                               |</span><br><span class="line">+------------------+------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>5.搭建计算服务组件 使用提供的脚本框架 iaas-install-nova-controller.sh 和 iaas-install -nova-compute.sh 填充脚本，在 controller 和 compute 节点上安装 nova 服 务并完成配置。完成后请将控制节点的计算资源也加入集群。然后使用 openstack 命令列出能提供计算资源的节点。 将列出计算资源的命令和返回结果提交到答题框。【1.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-nova-controller.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# iaas-install-nova-compute.sh </span><br></pre></td></tr></table></figure>

<p><strong>修改openrc.sh</strong></p>
<p><strong>HOST_IP_NODE&#x3D;192.168.20.105</strong></p>
<p><strong>HOST_NAME_NODE&#x3D;controller</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cat /etc/xiandian/openrc.sh </span><br><span class="line">#--------------------system Config--------------------##</span><br><span class="line">#Controller Server Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP=192.168.20.105</span><br><span class="line"></span><br><span class="line">#Controller HOST Password. example:000000 </span><br><span class="line">HOST_PASS=000000</span><br><span class="line"></span><br><span class="line">#Controller Server hostname. example:controller</span><br><span class="line">HOST_NAME=controller</span><br><span class="line"></span><br><span class="line">#Compute Node Manager IP. example:x.x.x.x</span><br><span class="line">HOST_IP_NODE=192.168.20.105</span><br><span class="line"></span><br><span class="line">#Compute HOST Password. example:000000 </span><br><span class="line">HOST_PASS_NODE=000000</span><br><span class="line"></span><br><span class="line">#Compute Node hostname. example:compute</span><br><span class="line">HOST_NAME_NODE=controller</span><br><span class="line"></span><br><span class="line">#--------------------Chrony Config-------------------##</span><br><span class="line">#Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)</span><br><span class="line">network_segment_IP=192.168.20.0/24</span><br><span class="line"></span><br><span class="line">#--------------------Rabbit Config ------------------##</span><br><span class="line">#user for rabbit. example:openstack</span><br><span class="line">RABBIT_USER=openstack</span><br><span class="line"></span><br><span class="line">#Password for rabbit user .example:000000</span><br><span class="line">RABBIT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------MySQL Config---------------------##</span><br><span class="line">#Password for MySQL root user . exmaple:000000</span><br><span class="line">DB_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Keystone Config------------------##</span><br><span class="line">#Password for Keystore admin user. exmaple:000000</span><br><span class="line">DOMAIN_NAME=demo</span><br><span class="line">ADMIN_PASS=000000</span><br><span class="line">DEMO_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql keystore user. exmaple:000000</span><br><span class="line">KEYSTONE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Glance Config--------------------##</span><br><span class="line">#Password for Mysql glance user. exmaple:000000</span><br><span class="line">GLANCE_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore glance user. exmaple:000000</span><br><span class="line">GLANCE_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Nova Config----------------------##</span><br><span class="line">#Password for Mysql nova user. exmaple:000000</span><br><span class="line">NOVA_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore nova user. exmaple:000000</span><br><span class="line">NOVA_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Neturon Config-------------------##</span><br><span class="line">#Password for Mysql neutron user. exmaple:000000</span><br><span class="line">NEUTRON_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore neutron user. exmaple:000000</span><br><span class="line">NEUTRON_PASS=000000</span><br><span class="line"></span><br><span class="line">#metadata secret for neutron. exmaple:000000</span><br><span class="line">METADATA_SECRET=000000</span><br><span class="line"></span><br><span class="line">#Tunnel Network Interface. example:x.x.x.x</span><br><span class="line">INTERFACE_IP=192.168.20.105    #（192.168.20.103）</span><br><span class="line"></span><br><span class="line">#External Network Interface. example:eth1</span><br><span class="line">INTERFACE_NAME=eth1</span><br><span class="line"></span><br><span class="line">#External Network The Physical Adapter. example:provider</span><br><span class="line">Physical_NAME=provider</span><br><span class="line"></span><br><span class="line">#First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101</span><br><span class="line">minvlan=101</span><br><span class="line"></span><br><span class="line">#Last Vlan ID in VLAN RANGE for VLAN Network. example:200</span><br><span class="line">maxvlan=200</span><br><span class="line"></span><br><span class="line">#--------------------Cinder Config--------------------##</span><br><span class="line">#Password for Mysql cinder user. exmaple:000000</span><br><span class="line">CINDER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore cinder user. exmaple:000000</span><br><span class="line">CINDER_PASS=000000</span><br><span class="line"></span><br><span class="line">#Cinder Block Disk. example:md126p3</span><br><span class="line">BLOCK_DISK=vdb1</span><br><span class="line"></span><br><span class="line">#--------------------Swift Config---------------------##</span><br><span class="line">#Password for Keystore swift user. exmaple:000000</span><br><span class="line">SWIFT_PASS=000000</span><br><span class="line"></span><br><span class="line">#The NODE Object Disk for Swift. example:md126p4.</span><br><span class="line">OBJECT_DISK=vdb2</span><br><span class="line"></span><br><span class="line">#The NODE IP for Swift Storage Network. example:x.x.x.x.</span><br><span class="line">STORAGE_LOCAL_NET_IP=000000</span><br><span class="line"></span><br><span class="line">#--------------------Heat Config----------------------##</span><br><span class="line">#Password for Mysql heat user. exmaple:000000</span><br><span class="line">HEAT_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore heat user. exmaple:000000</span><br><span class="line">HEAT_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Zun Config-----------------------##</span><br><span class="line">#Password for Mysql Zun user. exmaple:000000</span><br><span class="line">ZUN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Zun user. exmaple:000000</span><br><span class="line">ZUN_PASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Mysql Kuryr user. exmaple:000000</span><br><span class="line">KURYR_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Kuryr user. exmaple:000000</span><br><span class="line">KURYR_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Ceilometer Config----------------##</span><br><span class="line">#Password for Gnocchi ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore ceilometer user. exmaple:000000</span><br><span class="line">CEILOMETER_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------AODH Config----------------##</span><br><span class="line">#Password for Mysql AODH user. exmaple:000000</span><br><span class="line">AODH_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore AODH user. exmaple:000000</span><br><span class="line">AODH_PASS=000000</span><br><span class="line"></span><br><span class="line">#--------------------Barbican Config----------------##</span><br><span class="line">#Password for Mysql Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_DBPASS=000000</span><br><span class="line"></span><br><span class="line">#Password for Keystore Barbican user. exmaple:000000</span><br><span class="line">BARBICAN_PASS=000000</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-nova-compute.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"> [root@controller ~]# openstack compute service list --service nova-compute</span><br><span class="line">+----+--------------+------------+------+---------+-------+----------------------------+</span><br><span class="line">| ID | Binary       | Host       | Zone | Status  | State | Updated At                 |</span><br><span class="line">+----+--------------+------------+------+---------+-------+----------------------------+</span><br><span class="line">|  6 | nova-compute | compute    | nova | enabled | up    | 2022-09-02T22:17:25.000000 |</span><br><span class="line">|  7 | nova-compute | controller | nova | enabled | up    | 2022-09-02T22:17:18.000000 |</span><br><span class="line">+----+--------------+------------+------+---------+-------+----------------------------+</span><br></pre></td></tr></table></figure>

<p>6.搭建网络组件并初始化网络 使用提供的脚本框架 iaas-install-neutron-controller.sh 和 iaas-insta ll-neutron-compute.sh，填充脚本，在 controller 和 compute 节点上安装 neutron 服务并完成配置。创建云主机外部网络 ext-net，子网为 ext-subnet， 云主机浮动 IP 可用网段为 172.18.x.100<del>172.18.x.200，网关为 172.18.x.1。 创建云主机内部网络 int-net1，子网为 int-subnet1，云主机子网 IP 可用网段 为 10.0.0.100</del>10.0.0.200，网关为 10.0.0.1；创建云主机内部网络int-net2， 子网为 int-subnet2，云主机子网 IP 可用网段为 10.0.1.100 ~ 10.0.1.200， 网关为 10.0.1.1。添加名为 ext-router 的路由器，添加网关在 ext-net 网络， 添加内部端口到 int-net1 网络，完成内部网络 int-net1 和外部网络的连通。 请使用 openstack 命令完成以下任务，完成后将命令和返回结果提交到答题框。【4 分】 </p>
<p><strong>云主机外部网络 ext-net创建</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack network create ext-net --share --external --provider-physical-network provider --provider-network-type vlan </span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| Field                     | Value                                |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| admin_state_up            | UP                                   |</span><br><span class="line">| availability_zone_hints   |                                      |</span><br><span class="line">| availability_zones        |                                      |</span><br><span class="line">| created_at                | 2022-09-03T02:12:17Z                 |</span><br><span class="line">| description               |                                      |</span><br><span class="line">| dns_domain                | None                                 |</span><br><span class="line">| id                        | 2ba422fb-1ad9-4509-b9a4-643b82112ce6 |</span><br><span class="line">| ipv4_address_scope        | None                                 |</span><br><span class="line">| ipv6_address_scope        | None                                 |</span><br><span class="line">| is_default                | False                                |</span><br><span class="line">| is_vlan_transparent       | None                                 |</span><br><span class="line">| mtu                       | 1500                                 |</span><br><span class="line">| name                      | ext-net                              |</span><br><span class="line">| port_security_enabled     | True                                 |</span><br><span class="line">| project_id                | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| provider:network_type     | vlan                                 |</span><br><span class="line">| provider:physical_network | provider                             |</span><br><span class="line">| provider:segmentation_id  | 182                                  |</span><br><span class="line">| qos_policy_id             | None                                 |</span><br><span class="line">| revision_number           | 5                                    |</span><br><span class="line">| router:external           | External                             |</span><br><span class="line">| segments                  | None                                 |</span><br><span class="line">| shared                    | True                                 |</span><br><span class="line">| status                    | ACTIVE                               |</span><br><span class="line">| subnets                   |                                      |</span><br><span class="line">| tags                      |                                      |</span><br><span class="line">| updated_at                | 2022-09-03T02:12:18Z                 |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack subnet create --network ext-net --subnet-range 172.18.7.0/24 --gateway 172.18.7.1 --allocation-pool start=172.18.7.100,end=172.18.7.200 ext-subnet</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| Field             | Value                                |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| allocation_pools  | 172.18.7.100-172.18.7.200            |</span><br><span class="line">| cidr              | 172.18.7.0/24                        |</span><br><span class="line">| created_at        | 2022-09-03T02:16:52Z                 |</span><br><span class="line">| description       |                                      |</span><br><span class="line">| dns_nameservers   |                                      |</span><br><span class="line">| enable_dhcp       | True                                 |</span><br><span class="line">| gateway_ip        | 172.18.7.1                           |</span><br><span class="line">| host_routes       |                                      |</span><br><span class="line">| id                | 72ec3fbe-05b5-4282-a01a-7f22b1d432ca |</span><br><span class="line">| ip_version        | 4                                    |</span><br><span class="line">| ipv6_address_mode | None                                 |</span><br><span class="line">| ipv6_ra_mode      | None                                 |</span><br><span class="line">| name              | ext-subnet                           |</span><br><span class="line">| network_id        | 2ba422fb-1ad9-4509-b9a4-643b82112ce6 |</span><br><span class="line">| project_id        | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| revision_number   | 0                                    |</span><br><span class="line">| segment_id        | None                                 |</span><br><span class="line">| service_types     |                                      |</span><br><span class="line">| subnetpool_id     | None                                 |</span><br><span class="line">| tags              |                                      |</span><br><span class="line">| updated_at        | 2022-09-03T02:16:52Z                 |</span><br><span class="line">+-------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>创建云主机内部网络 int-net1</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack network create int-net1</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| Field                     | Value                                |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| admin_state_up            | UP                                   |</span><br><span class="line">| availability_zone_hints   |                                      |</span><br><span class="line">| availability_zones        |                                      |</span><br><span class="line">| created_at                | 2022-09-03T02:20:07Z                 |</span><br><span class="line">| description               |                                      |</span><br><span class="line">| dns_domain                | None                                 |</span><br><span class="line">| id                        | 89062681-1008-4083-8dfb-a5c6763eec7f |</span><br><span class="line">| ipv4_address_scope        | None                                 |</span><br><span class="line">| ipv6_address_scope        | None                                 |</span><br><span class="line">| is_default                | False                                |</span><br><span class="line">| is_vlan_transparent       | None                                 |</span><br><span class="line">| mtu                       | 1450                                 |</span><br><span class="line">| name                      | int-net1                             |</span><br><span class="line">| port_security_enabled     | True                                 |</span><br><span class="line">| project_id                | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| provider:network_type     | vxlan                                |</span><br><span class="line">| provider:physical_network | None                                 |</span><br><span class="line">| provider:segmentation_id  | 170                                  |</span><br><span class="line">| qos_policy_id             | None                                 |</span><br><span class="line">| revision_number           | 2                                    |</span><br><span class="line">| router:external           | Internal                             |</span><br><span class="line">| segments                  | None                                 |</span><br><span class="line">| shared                    | False                                |</span><br><span class="line">| status                    | ACTIVE                               |</span><br><span class="line">| subnets                   |                                      |</span><br><span class="line">| tags                      |                                      |</span><br><span class="line">| updated_at                | 2022-09-03T02:20:07Z                 |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack subnet create --network int-net1 --subnet-range 10.0.0.0/24 --gateway 10.0.0.1 --allocation-pool start=10.0.0.100,end=10.0.0.200 int-subnet1</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| Field             | Value                                |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| allocation_pools  | 10.0.0.100-10.0.0.200                |</span><br><span class="line">| cidr              | 10.0.0.0/24                          |</span><br><span class="line">| created_at        | 2022-09-03T02:22:08Z                 |</span><br><span class="line">| description       |                                      |</span><br><span class="line">| dns_nameservers   |                                      |</span><br><span class="line">| enable_dhcp       | True                                 |</span><br><span class="line">| gateway_ip        | 10.0.0.1                             |</span><br><span class="line">| host_routes       |                                      |</span><br><span class="line">| id                | 95eefbdd-5669-4840-a1e6-2bcd67ae1208 |</span><br><span class="line">| ip_version        | 4                                    |</span><br><span class="line">| ipv6_address_mode | None                                 |</span><br><span class="line">| ipv6_ra_mode      | None                                 |</span><br><span class="line">| name              | int-subnet1                          |</span><br><span class="line">| network_id        | 89062681-1008-4083-8dfb-a5c6763eec7f |</span><br><span class="line">| project_id        | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| revision_number   | 0                                    |</span><br><span class="line">| segment_id        | None                                 |</span><br><span class="line">| service_types     |                                      |</span><br><span class="line">| subnetpool_id     | None                                 |</span><br><span class="line">| tags              |                                      |</span><br><span class="line">| updated_at        | 2022-09-03T02:22:08Z                 |</span><br><span class="line">+-------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>创建云主机内部网络 int-net2</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack network create int-net2</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| Field                     | Value                                |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| admin_state_up            | UP                                   |</span><br><span class="line">| availability_zone_hints   |                                      |</span><br><span class="line">| availability_zones        |                                      |</span><br><span class="line">| created_at                | 2022-09-03T02:22:49Z                 |</span><br><span class="line">| description               |                                      |</span><br><span class="line">| dns_domain                | None                                 |</span><br><span class="line">| id                        | e74600ae-a303-4804-b82b-c4401d308246 |</span><br><span class="line">| ipv4_address_scope        | None                                 |</span><br><span class="line">| ipv6_address_scope        | None                                 |</span><br><span class="line">| is_default                | False                                |</span><br><span class="line">| is_vlan_transparent       | None                                 |</span><br><span class="line">| mtu                       | 1450                                 |</span><br><span class="line">| name                      | int-net2                             |</span><br><span class="line">| port_security_enabled     | True                                 |</span><br><span class="line">| project_id                | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| provider:network_type     | vxlan                                |</span><br><span class="line">| provider:physical_network | None                                 |</span><br><span class="line">| provider:segmentation_id  | 165                                  |</span><br><span class="line">| qos_policy_id             | None                                 |</span><br><span class="line">| revision_number           | 2                                    |</span><br><span class="line">| router:external           | Internal                             |</span><br><span class="line">| segments                  | None                                 |</span><br><span class="line">| shared                    | False                                |</span><br><span class="line">| status                    | ACTIVE                               |</span><br><span class="line">| subnets                   |                                      |</span><br><span class="line">| tags                      |                                      |</span><br><span class="line">| updated_at                | 2022-09-03T02:22:49Z                 |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack subnet create --network int-net2 --subnet-range 10.0.1.0/24 --gateway 10.0.1.1 --allocation-pool start=10.0.1.100,end=10.0.1.200 int-subnet2</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| Field             | Value                                |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| allocation_pools  | 10.0.1.100-10.0.1.200                |</span><br><span class="line">| cidr              | 10.0.1.0/24                          |</span><br><span class="line">| created_at        | 2022-09-03T02:24:19Z                 |</span><br><span class="line">| description       |                                      |</span><br><span class="line">| dns_nameservers   |                                      |</span><br><span class="line">| enable_dhcp       | True                                 |</span><br><span class="line">| gateway_ip        | 10.0.1.1                             |</span><br><span class="line">| host_routes       |                                      |</span><br><span class="line">| id                | 4a736169-b4e4-4c88-bd7b-b8ea02443fdf |</span><br><span class="line">| ip_version        | 4                                    |</span><br><span class="line">| ipv6_address_mode | None                                 |</span><br><span class="line">| ipv6_ra_mode      | None                                 |</span><br><span class="line">| name              | int-subnet2                          |</span><br><span class="line">| network_id        | e74600ae-a303-4804-b82b-c4401d308246 |</span><br><span class="line">| project_id        | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| revision_number   | 0                                    |</span><br><span class="line">| segment_id        | None                                 |</span><br><span class="line">| service_types     |                                      |</span><br><span class="line">| subnetpool_id     | None                                 |</span><br><span class="line">| tags              |                                      |</span><br><span class="line">| updated_at        | 2022-09-03T02:24:19Z                 |</span><br><span class="line">+-------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>添加名为 ext-router 的路由器</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack router create ext-router</span><br><span class="line">+-------------------------+--------------------------------------+</span><br><span class="line">| Field                   | Value                                |</span><br><span class="line">+-------------------------+--------------------------------------+</span><br><span class="line">| admin_state_up          | UP                                   |</span><br><span class="line">| availability_zone_hints |                                      |</span><br><span class="line">| availability_zones      |                                      |</span><br><span class="line">| created_at              | 2022-09-03T02:27:25Z                 |</span><br><span class="line">| description             |                                      |</span><br><span class="line">| distributed             | False                                |</span><br><span class="line">| external_gateway_info   | None                                 |</span><br><span class="line">| flavor_id               | None                                 |</span><br><span class="line">| ha                      | False                                |</span><br><span class="line">| id                      | b42861e5-5dd5-4c48-b225-212a9d567030 |</span><br><span class="line">| name                    | ext-router                           |</span><br><span class="line">| project_id              | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| revision_number         | 1                                    |</span><br><span class="line">| routes                  |                                      |</span><br><span class="line">| status                  | ACTIVE                               |</span><br><span class="line">| tags                    |                                      |</span><br><span class="line">| updated_at              | 2022-09-03T02:27:25Z                 |</span><br><span class="line">+-------------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>添加内部端口到 int-net1 网络，完成内部网络 int-net1 和外部网络的连通</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack router add subnet  ext-router int-subnet1</span><br><span class="line">[root@controller ~]# openstack router set ext-router --external-gateway ext-net</span><br></pre></td></tr></table></figure>

<p>7.搭建图形化界面 使用提供的脚本框架 iaas-install-dashboard.sh，填充脚本，在 control ler 节点上安装 dashboard 服务并完成相关配置。 请使用 curl 指令获取 dashboard 首页信息，将获取到的首页信息提交到答 题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# iaas-install-dashboard.sh </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# curl -i http://192.168.20.105/dashboard</span><br><span class="line">HTTP/1.1 302 Found</span><br><span class="line">Date: Sat, 03 Sep 2022 02:57:56 GMT</span><br><span class="line">Server: Apache/2.4.6 (CentOS) mod_wsgi/3.4 Python/2.7.5</span><br><span class="line">Content-Language: en</span><br><span class="line">Vary: Accept-Language,Cookie</span><br><span class="line">X-Frame-Options: SAMEORIGIN</span><br><span class="line">Content-Length: 0</span><br><span class="line">Location: http://192.168.20.105/dashboard/auth/login/?next=/dashboard/</span><br><span class="line">Content-Type: text/html; charset=utf-8</span><br></pre></td></tr></table></figure>

<h3 id="任务-3-OpenStack-运维任务（15-分）"><a href="#任务-3-OpenStack-运维任务（15-分）" class="headerlink" title="任务 3 OpenStack 运维任务（15 分）"></a>任务 3 OpenStack 运维任务（15 分）</h3><p>1.用户管理 在 keystone 中创建用户 testuser，密码为 password。创建好之后，使用 命令修改 testuser 密码为 000000，并查看 testuser 的详细信息。添加将该 用户添加到 admin 项目并赋予普通用户权限，完成后测试登录。 使用 testuser 用登录系统完成后截图并提交到答题框。【1 分】</p>
<p><strong>创建用户 testuser，密码为 password</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack user create --password password testuser --domain demo</span><br><span class="line">+---------------------+----------------------------------+</span><br><span class="line">| Field               | Value                            |</span><br><span class="line">+---------------------+----------------------------------+</span><br><span class="line">| domain_id           | 79342087fd364c72a664c0ed6590154d |</span><br><span class="line">| enabled             | True                             |</span><br><span class="line">| id                  | fc4ae8a457ec486dbb5c07b977d17833 |</span><br><span class="line">| name                | testuser                         |</span><br><span class="line">| options             | &#123;&#125;                               |</span><br><span class="line">| password_expires_at | None                             |</span><br><span class="line">+---------------------+----------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>使用 命令修改 testuser 密码为 000000，并查看 testuser 的详细信息</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack user set testuser --password 000000</span><br><span class="line">[root@controller ~]# openstack user show testuser</span><br><span class="line">+---------------------+----------------------------------+</span><br><span class="line">| Field               | Value                            |</span><br><span class="line">+---------------------+----------------------------------+</span><br><span class="line">| domain_id           | 79342087fd364c72a664c0ed6590154d |</span><br><span class="line">| enabled             | True                             |</span><br><span class="line">| id                  | fc4ae8a457ec486dbb5c07b977d17833 |</span><br><span class="line">| name                | testuser                         |</span><br><span class="line">| options             | &#123;&#125;                               |</span><br><span class="line">| password_expires_at | None                             |</span><br><span class="line">+---------------------+----------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>添加将该 用户添加到 admin 项目并赋予普通用户权限</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack user set testuser --project admin</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack role add user --user testuser --domain demo</span><br></pre></td></tr></table></figure>

<p>2.服务查询 使用命令列出服务目录和端点，查看 glance 服务的端点。将以上命令和返 回结果提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack endpoint list --service glance</span><br><span class="line">+----------------------------------+-----------+--------------+--------------+---------+-----------+------------------------+</span><br><span class="line">| ID                               | Region    | Service Name | Service Type | Enabled | Interface | URL                    |</span><br><span class="line">+----------------------------------+-----------+--------------+--------------+---------+-----------+------------------------+</span><br><span class="line">| 0388bc7714f04cd89e3ca579f43a67e4 | RegionOne | glance       | image        | True    | admin     | http://controller:9292 |</span><br><span class="line">| 0500f3b7152747f6853542a3723766dd | RegionOne | glance       | image        | True    | public    | http://controller:9292 |</span><br><span class="line">| 7047188a53f04073acdb1a60722fe2d7 | RegionOne | glance       | image        | True    | internal  | http://controller:9292 |</span><br><span class="line">+----------------------------------+-----------+--------------+--------------+---------+-----------+------------------------+</span><br></pre></td></tr></table></figure>

<p>3.镜像管理 登录 controller 节点，使用 glance 相关命令，上传镜像，源使用 CentO S_6.5_x86_64_XD.qcow2，名字为 testone，然后使用 openstack 命令修改这个 镜像名改为 examimage，然后给这个镜像打一个标签，标签名字为 lastone 改 完后使用 openstack 命令查看镜像列表。 将以上命令和返回结果提交到答题框。【2 分】</p>
<p><strong>使用 glance 相关命令，上传镜像</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# glance image-create --name testone --disk-format qcow2 --container-format bare &lt; /opt/iaas/images/CentOS_6.5_x86_64_XD.qcow2 </span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| Property         | Value                                |</span><br><span class="line">+------------------+--------------------------------------+</span><br><span class="line">| checksum         | 3e565ace16066679ea363dde5411ed25     |</span><br><span class="line">| container_format | bare                                 |</span><br><span class="line">| created_at       | 2022-09-03T03:18:23Z                 |</span><br><span class="line">| disk_format      | qcow2                                |</span><br><span class="line">| id               | df61b71f-4077-4374-a143-331b73377f18 |</span><br><span class="line">| min_disk         | 0                                    |</span><br><span class="line">| min_ram          | 0                                    |</span><br><span class="line">| name             | testone                              |</span><br><span class="line">| owner            | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| protected        | False                                |</span><br><span class="line">| size             | 283181056                            |</span><br><span class="line">| status           | active                               |</span><br><span class="line">| tags             | []                                   |</span><br><span class="line">| updated_at       | 2022-09-03T03:18:25Z                 |</span><br><span class="line">| virtual_size     | None                                 |</span><br><span class="line">| visibility       | shared                               |</span><br><span class="line">+------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>使用 openstack 命令修改这个 镜像名改为 examimage</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image set testone --name examimage</span><br></pre></td></tr></table></figure>

<p><strong>给这个镜像打一个标签</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image set examimage --tag lastone</span><br></pre></td></tr></table></figure>

<p><strong>使用 openstack 命令查看镜像列表</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack image list</span><br><span class="line">+--------------------------------------+-----------+--------+</span><br><span class="line">| ID                                   | Name      | Status |</span><br><span class="line">+--------------------------------------+-----------+--------+</span><br><span class="line">| cbfdbd87-71df-4be7-8eb2-6904f781239c | cirros    | active |</span><br><span class="line">| df61b71f-4077-4374-a143-331b73377f18 | examimage | active |</span><br><span class="line">+--------------------------------------+-----------+--------+</span><br></pre></td></tr></table></figure>

<p>4.后端配置文件管理 进入到glance 后端存储目录中，使用 qemu 命令查看任意的一个镜像信息。 使用 du 命令查看 nova 主配置文件大小。 将以上命令和返回结果提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# qemu-img info /var/lib/glance/images/cbfdbd87-71df-4be7-8eb2-6904f781239c </span><br><span class="line">image: /var/lib/glance/images/cbfdbd87-71df-4be7-8eb2-6904f781239c</span><br><span class="line">file format: qcow2</span><br><span class="line">virtual size: 39M (41126400 bytes)</span><br><span class="line">disk size: 13M</span><br><span class="line">cluster_size: 65536</span><br><span class="line">Format specific information:</span><br><span class="line">    compat: 0.10</span><br><span class="line">    refcount bits: 16</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# du -h /etc/nova/nova.conf </span><br><span class="line">364K	/etc/nova/nova.conf</span><br></pre></td></tr></table></figure>

<p>5.存储服务管理 创建一个卷类型，然后创建一块带这个卷类型标识的云硬盘，查询该云硬盘 的详细信息。将该云硬盘挂载到虚拟机中，将该云硬盘格式化为 xfs。创建一个 文件文件名为工位号内容为工位号，然后将该云硬盘卸载，使用 openstack 命令 将该云硬盘修改为只读状态，再次挂载后查看是否存在原始文件，然后再次向该 云硬盘中创建一个文件，文件名为工位号_02。 将返回结果及解题过程提交到答题框。【2 分】</p>
<p><strong>创建一个卷类型，然后创建一块带这个卷类型标识的云硬盘，查询该云硬盘 的详细信息</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack volume type create lvm</span><br><span class="line">+-------------+--------------------------------------+</span><br><span class="line">| Field       | Value                                |</span><br><span class="line">+-------------+--------------------------------------+</span><br><span class="line">| description | None                                 |</span><br><span class="line">| id          | d0b4c01e-9454-453e-8b54-599d94108d19 |</span><br><span class="line">| is_public   | True                                 |</span><br><span class="line">| name        | lvm                                  |</span><br><span class="line">+-------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack volume create --type lvm --size 10 blcok</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| Field               | Value                                |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| attachments         | []                                   |</span><br><span class="line">| availability_zone   | nova                                 |</span><br><span class="line">| bootable            | false                                |</span><br><span class="line">| consistencygroup_id | None                                 |</span><br><span class="line">| created_at          | 2022-09-03T03:34:47.000000           |</span><br><span class="line">| description         | None                                 |</span><br><span class="line">| encrypted           | False                                |</span><br><span class="line">| id                  | 0a576f20-1ecf-4b7a-999c-37daa899bf8e |</span><br><span class="line">| migration_status    | None                                 |</span><br><span class="line">| multiattach         | False                                |</span><br><span class="line">| name                | blcok                                |</span><br><span class="line">| properties          |                                      |</span><br><span class="line">| replication_status  | None                                 |</span><br><span class="line">| size                | 10                                   |</span><br><span class="line">| snapshot_id         | None                                 |</span><br><span class="line">| source_volid        | None                                 |</span><br><span class="line">| status              | creating                             |</span><br><span class="line">| type                | lvm                                  |</span><br><span class="line">| updated_at          | None                                 |</span><br><span class="line">| user_id             | dcf33a463e07489c817f0467f13e968b     |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack volume show blcok</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| Field                          | Value                                |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br><span class="line">| attachments                    | []                                   |</span><br><span class="line">| availability_zone              | nova                                 |</span><br><span class="line">| bootable                       | false                                |</span><br><span class="line">| consistencygroup_id            | None                                 |</span><br><span class="line">| created_at                     | 2022-09-03T03:34:47.000000           |</span><br><span class="line">| description                    | None                                 |</span><br><span class="line">| encrypted                      | False                                |</span><br><span class="line">| id                             | 0a576f20-1ecf-4b7a-999c-37daa899bf8e |</span><br><span class="line">| migration_status               | None                                 |</span><br><span class="line">| multiattach                    | False                                |</span><br><span class="line">| name                           | blcok                                |</span><br><span class="line">| os-vol-host-attr:host          | compute@lvm#LVM                      |</span><br><span class="line">| os-vol-mig-status-attr:migstat | None                                 |</span><br><span class="line">| os-vol-mig-status-attr:name_id | None                                 |</span><br><span class="line">| os-vol-tenant-attr:tenant_id   | 6c6ebc8aab7144eeb3c0f56ae76568e4     |</span><br><span class="line">| properties                     |                                      |</span><br><span class="line">| replication_status             | None                                 |</span><br><span class="line">| size                           | 10                                   |</span><br><span class="line">| snapshot_id                    | None                                 |</span><br><span class="line">| source_volid                   | None                                 |</span><br><span class="line">| status                         | available                            |</span><br><span class="line">| type                           | lvm                                  |</span><br><span class="line">| updated_at                     | 2022-09-03T03:34:48.000000           |</span><br><span class="line">| user_id                        | dcf33a463e07489c817f0467f13e968b     |</span><br><span class="line">+--------------------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p><strong>将该云硬盘挂载到虚拟机中，将该云硬盘格式化为 xfs</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server add volume controller blcok</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# mkfs.xfs /dev/vdb</span><br><span class="line">meta-data=/dev/vdb               isize=512    agcount=4, agsize=655360 blks</span><br><span class="line">         =                       sectsz=512   attr=2, projid32bit=1</span><br><span class="line">         =                       crc=1        finobt=0, sparse=0</span><br><span class="line">data     =                       bsize=4096   blocks=2621440, imaxpct=25</span><br><span class="line">         =                       sunit=0      swidth=0 blks</span><br><span class="line">naming   =version 2              bsize=4096   ascii-ci=0 ftype=1</span><br><span class="line">log      =internal log           bsize=4096   blocks=2560, version=2</span><br><span class="line">         =                       sectsz=512   sunit=0 blks, lazy-count=1</span><br><span class="line">realtime =none                   extsz=4096   blocks=0, rtextents=0</span><br></pre></td></tr></table></figure>

<p><strong>创建一个 文件文件名为工位号内容为工位号，然后将该云硬盘卸载，使用 openstack 命令 将该云硬盘修改为只读状态</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# mount /dev/vdb /mnt/</span><br><span class="line">[root@controller ~]# cd /mnt/</span><br><span class="line">[root@controller mnt]# touch 777</span><br><span class="line">[root@controller ~]# umount /mnt/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server remove volume controller blcok</span><br><span class="line">[root@controller ~]# openstack volume set blcok --read-only</span><br></pre></td></tr></table></figure>

<p><strong>再次挂载后查看是否存在原始文件，然后再次向该 云硬盘中创建一个文件，文件名为工位号_02</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server add volume controller blcok</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# mount /dev/vdb /mnt/</span><br><span class="line">mount: /dev/vdb is write-protected, mounting read-only</span><br><span class="line">[root@controller ~]# ls /mnt/</span><br><span class="line">777</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# cd /mnt/</span><br><span class="line">[root@controller mnt]# ls</span><br><span class="line">777</span><br><span class="line">[root@controller mnt]# touch 777_2</span><br><span class="line">touch: cannot touch ‘777_2’: Read-only file system</span><br></pre></td></tr></table></figure>

<p>6.存储服务管理 使用命令创建一个 5GB 的云硬盘，名称为 disk-2，将云硬盘挂载到云虚拟 机内，然后格式化为 ext4，挂载到虚拟机的 &#x2F;mnt&#x2F; 目录下，使用 df -h 将命令 和返回信息提交到答题框。将该云硬盘使用命令卸载，使用命令将该云硬盘扩容 到 10GB，使用命令将云硬盘挂载到云主机上，将命令及返回信息提交到答题框。 进入云主机使用命令扩容文件系统，扩容后再次挂载到 &#x2F;mnt&#x2F;。 使用 df -hT 命令并将命令和返回信息提交到答题框。【2 分】</p>
<p><strong>使用命令创建一个 5GB 的云硬盘，名称为 disk-2，将云硬盘挂载到云虚拟 机内，然后格式化为 ext4，挂载到虚拟机的 &#x2F;mnt&#x2F; 目录下，使用 df -h 将命令 和返回信息提交到答题框</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack volume create disk-2 --size 5</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| Field               | Value                                |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| attachments         | []                                   |</span><br><span class="line">| availability_zone   | nova                                 |</span><br><span class="line">| bootable            | false                                |</span><br><span class="line">| consistencygroup_id | None                                 |</span><br><span class="line">| created_at          | 2022-09-03T03:50:20.000000           |</span><br><span class="line">| description         | None                                 |</span><br><span class="line">| encrypted           | False                                |</span><br><span class="line">| id                  | eb48e3aa-3661-456a-90c0-bad30491a3eb |</span><br><span class="line">| migration_status    | None                                 |</span><br><span class="line">| multiattach         | False                                |</span><br><span class="line">| name                | disk-2                               |</span><br><span class="line">| properties          |                                      |</span><br><span class="line">| replication_status  | None                                 |</span><br><span class="line">| size                | 5                                    |</span><br><span class="line">| snapshot_id         | None                                 |</span><br><span class="line">| source_volid        | None                                 |</span><br><span class="line">| status              | creating                             |</span><br><span class="line">| type                | None                                 |</span><br><span class="line">| updated_at          | None                                 |</span><br><span class="line">| user_id             | dcf33a463e07489c817f0467f13e968b     |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack server add volume controller disk-2</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# mkfs.ext4 /dev/vdb</span><br><span class="line">mke2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Filesystem label=</span><br><span class="line">OS type: Linux</span><br><span class="line">Block size=4096 (log=2)</span><br><span class="line">Fragment size=4096 (log=2)</span><br><span class="line">Stride=0 blocks, Stripe width=0 blocks</span><br><span class="line">327680 inodes, 1310720 blocks</span><br><span class="line">65536 blocks (5.00%) reserved for the super user</span><br><span class="line">First data block=0</span><br><span class="line">Maximum filesystem blocks=1342177280</span><br><span class="line">40 block groups</span><br><span class="line">32768 blocks per group, 32768 fragments per group</span><br><span class="line">8192 inodes per group</span><br><span class="line">Superblock backups stored on blocks: </span><br><span class="line">	32768, 98304, 163840, 229376, 294912, 819200, 884736</span><br><span class="line"></span><br><span class="line">Allocating group tables: done                            </span><br><span class="line">Writing inode tables: done                            </span><br><span class="line">Creating journal (32768 blocks): done</span><br><span class="line">Writing superblocks and filesystem accounting information: done </span><br><span class="line"></span><br><span class="line">[root@controller ~]# mount /dev/vdb /mnt/</span><br><span class="line">[root@controller ~]# df -h /mnt/</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vdb        4.8G   20M  4.6G   1% /mnt</span><br></pre></td></tr></table></figure>

<p><strong>将该云硬盘使用命令卸载，使用命令将该云硬盘扩容 到 10GB，使用命令将云硬盘挂载到云主机上，将命令及返回信息提交到答题框</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# umount /mnt/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server remove volume controller disk-2</span><br><span class="line">[root@controller ~]# openstack volume set disk-2 --size 10</span><br><span class="line">[root@controller ~]# openstack server add volume controller disk-2</span><br></pre></td></tr></table></figure>

<p><strong>进入云主机使用命令扩容文件系统，扩容后再次挂载到 &#x2F;mnt&#x2F;。 使用 df -hT 命令并将命令和返回信息提交到答题框</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# mount /dev/vdb  /mnt/</span><br><span class="line">[root@controller ~]# df -hT /dev/vdb</span><br><span class="line">Filesystem     Type  Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vdb       ext4  9.8G   37M  9.2G   1% /mnt</span><br></pre></td></tr></table></figure>

<p>7.对象存储管理 使用 swift 相关命令，创建一个容器，然后往这个容器中上传一个文件（文 件可以自行创建），上传完毕后，使用命令查看容器。 将以上命令和返回结果提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# swift post chinaskill</span><br><span class="line">[root@controller ~]# swift upload chinaskill cirros-0.3.4-x86_64-disk.img </span><br><span class="line">[root@controller ~]# swift list chinaskill</span><br></pre></td></tr></table></figure>

<p>8.安全组管理 使用命令创建名称为 group_web 的安全组该安全组的描述为工位号，为该安 全组添加一条规则允许任意 ip 地址访问 web 流量，完成后查看该安全组的详细 信息。 将以上命令和返回结果提交到答题框。【2 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack security group create group_web --description 777</span><br><span class="line">+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Field           | Value                                                                                                                                                 |</span><br><span class="line">+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| created_at      | 2022-09-03T04:12:13Z                                                                                                                                  |</span><br><span class="line">| description     | 777                                                                                                                                                   |</span><br><span class="line">| id              | f31a44fe-5e5c-47d2-9925-e2a1a8b6cb92                                                                                                                  |</span><br><span class="line">| name            | group_web                                                                                                                                             |</span><br><span class="line">| project_id      | 4350b89460a148d7bf1b2ae63296a6bd                                                                                                                      |</span><br><span class="line">| revision_number | 2                                                                                                                                                     |</span><br><span class="line">| rules           | created_at=&#x27;2022-09-03T04:12:13Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv6&#x27;, id=&#x27;4acce720-f30c-472e-a5d8-4e133c9c3e3d&#x27;, updated_at=&#x27;2022-09-03T04:12:13Z&#x27; |</span><br><span class="line">|                 | created_at=&#x27;2022-09-03T04:12:13Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;966133fd-099f-4b18-a2a0-0023fa9a8226&#x27;, updated_at=&#x27;2022-09-03T04:12:13Z&#x27; |</span><br><span class="line">| updated_at      | 2022-09-03T04:12:13Z                                                                                                                                  |</span><br><span class="line">+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack security group rule create group_web --ingress --ethertype IPv4 --protocol tcp --dst-port 80:80 </span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| Field             | Value                                |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| created_at        | 2022-09-03T04:22:45Z                 |</span><br><span class="line">| description       |                                      |</span><br><span class="line">| direction         | ingress                              |</span><br><span class="line">| ether_type        | IPv4                                 |</span><br><span class="line">| id                | 35ebf6fa-9668-4155-bf90-09623279c17f |</span><br><span class="line">| name              | None                                 |</span><br><span class="line">| port_range_max    | 80                                   |</span><br><span class="line">| port_range_min    | 80                                   |</span><br><span class="line">| project_id        | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| protocol          | tcp                                  |</span><br><span class="line">| remote_group_id   | None                                 |</span><br><span class="line">| remote_ip_prefix  | 0.0.0.0/0                            |</span><br><span class="line">| revision_number   | 0                                    |</span><br><span class="line">| security_group_id | f31a44fe-5e5c-47d2-9925-e2a1a8b6cb92 |</span><br><span class="line">| updated_at        | 2022-09-03T04:22:45Z                 |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack security group rule create group_web --ingress --ethertype IPv4 --protocol tcp --dst-port 443:443 </span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| Field             | Value                                |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">| created_at        | 2022-09-03T04:23:09Z                 |</span><br><span class="line">| description       |                                      |</span><br><span class="line">| direction         | ingress                              |</span><br><span class="line">| ether_type        | IPv4                                 |</span><br><span class="line">| id                | 24780db0-b8f7-4630-a5b0-ec576be11909 |</span><br><span class="line">| name              | None                                 |</span><br><span class="line">| port_range_max    | 443                                  |</span><br><span class="line">| port_range_min    | 443                                  |</span><br><span class="line">| project_id        | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| protocol          | tcp                                  |</span><br><span class="line">| remote_group_id   | None                                 |</span><br><span class="line">| remote_ip_prefix  | 0.0.0.0/0                            |</span><br><span class="line">| revision_number   | 0                                    |</span><br><span class="line">| security_group_id | f31a44fe-5e5c-47d2-9925-e2a1a8b6cb92 |</span><br><span class="line">| updated_at        | 2022-09-03T04:23:09Z                 |</span><br><span class="line">+-------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack security group show group_web</span><br><span class="line">+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| Field           | Value                                                                                                                                                                                                                                            |</span><br><span class="line">+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br><span class="line">| created_at      | 2022-09-03T04:12:13Z                                                                                                                                                                                                                             |</span><br><span class="line">| description     | 777                                                                                                                                                                                                                                              |</span><br><span class="line">| id              | f31a44fe-5e5c-47d2-9925-e2a1a8b6cb92                                                                                                                                                                                                             |</span><br><span class="line">| name            | group_web                                                                                                                                                                                                                                        |</span><br><span class="line">| project_id      | 4350b89460a148d7bf1b2ae63296a6bd                                                                                                                                                                                                                 |</span><br><span class="line">| revision_number | 4                                                                                                                                                                                                                                                |</span><br><span class="line">| rules           | created_at=&#x27;2022-09-03T04:23:09Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;24780db0-b8f7-4630-a5b0-ec576be11909&#x27;, port_range_max=&#x27;443&#x27;, port_range_min=&#x27;443&#x27;, protocol=&#x27;tcp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2022-09-03T04:23:09Z&#x27; |</span><br><span class="line">|                 | created_at=&#x27;2022-09-03T04:22:45Z&#x27;, direction=&#x27;ingress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;35ebf6fa-9668-4155-bf90-09623279c17f&#x27;, port_range_max=&#x27;80&#x27;, port_range_min=&#x27;80&#x27;, protocol=&#x27;tcp&#x27;, remote_ip_prefix=&#x27;0.0.0.0/0&#x27;, updated_at=&#x27;2022-09-03T04:22:45Z&#x27;   |</span><br><span class="line">|                 | created_at=&#x27;2022-09-03T04:12:13Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv6&#x27;, id=&#x27;4acce720-f30c-472e-a5d8-4e133c9c3e3d&#x27;, updated_at=&#x27;2022-09-03T04:12:13Z&#x27;                                                                                            |</span><br><span class="line">|                 | created_at=&#x27;2022-09-03T04:12:13Z&#x27;, direction=&#x27;egress&#x27;, ethertype=&#x27;IPv4&#x27;, id=&#x27;966133fd-099f-4b18-a2a0-0023fa9a8226&#x27;, updated_at=&#x27;2022-09-03T04:12:13Z&#x27;                                                                                            |</span><br><span class="line">| updated_at      | 2022-09-03T04:23:09Z                                                                                                                                                                                                                             |</span><br><span class="line">+-----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<p>9.网络管理 使用命令将int-net1网络设置为共享，然后查看int-net1网络的详细信息。 将命令和返回信息提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack network set int-net1 --share</span><br><span class="line">[root@controller ~]# openstack network show int-net1</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| Field                     | Value                                |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br><span class="line">| admin_state_up            | UP                                   |</span><br><span class="line">| availability_zone_hints   |                                      |</span><br><span class="line">| availability_zones        | nova                                 |</span><br><span class="line">| created_at                | 2022-09-03T02:20:07Z                 |</span><br><span class="line">| description               |                                      |</span><br><span class="line">| dns_domain                | None                                 |</span><br><span class="line">| id                        | 89062681-1008-4083-8dfb-a5c6763eec7f |</span><br><span class="line">| ipv4_address_scope        | None                                 |</span><br><span class="line">| ipv6_address_scope        | None                                 |</span><br><span class="line">| is_default                | None                                 |</span><br><span class="line">| is_vlan_transparent       | None                                 |</span><br><span class="line">| mtu                       | 1450                                 |</span><br><span class="line">| name                      | int-net1                             |</span><br><span class="line">| port_security_enabled     | True                                 |</span><br><span class="line">| project_id                | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| provider:network_type     | vxlan                                |</span><br><span class="line">| provider:physical_network | None                                 |</span><br><span class="line">| provider:segmentation_id  | 170                                  |</span><br><span class="line">| qos_policy_id             | None                                 |</span><br><span class="line">| revision_number           | 4                                    |</span><br><span class="line">| router:external           | Internal                             |</span><br><span class="line">| segments                  | None                                 |</span><br><span class="line">| shared                    | True                                 |</span><br><span class="line">| status                    | ACTIVE                               |</span><br><span class="line">| subnets                   | 95eefbdd-5669-4840-a1e6-2bcd67ae1208 |</span><br><span class="line">| tags                      |                                      |</span><br><span class="line">| updated_at                | 2022-09-03T04:25:28Z                 |</span><br><span class="line">+---------------------------+--------------------------------------+</span><br></pre></td></tr></table></figure>

<p>10.网络管理 使用 dashboard 界面使用 centos7.5 镜像创建一台云主机，云主机命名为 test-01，使用命令查看浮动 IP 地址池，使用命令创建一个浮动 IP，然后将浮动 IP 绑定到云主机上。 将命令和返回信息提交到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack floating ip create ext-net --subnet ext-subnet</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| Field               | Value                                |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">| created_at          | 2022-09-03T04:32:48Z                 |</span><br><span class="line">| description         |                                      |</span><br><span class="line">| fixed_ip_address    | None                                 |</span><br><span class="line">| floating_ip_address | 172.18.7.101                         |</span><br><span class="line">| floating_network_id | 2ba422fb-1ad9-4509-b9a4-643b82112ce6 |</span><br><span class="line">| id                  | d896a909-e912-4bd3-9545-4193eb6636b6 |</span><br><span class="line">| name                | 172.18.7.101                         |</span><br><span class="line">| port_id             | None                                 |</span><br><span class="line">| project_id          | 4350b89460a148d7bf1b2ae63296a6bd     |</span><br><span class="line">| qos_policy_id       | None                                 |</span><br><span class="line">| revision_number     | 0                                    |</span><br><span class="line">| router_id           | None                                 |</span><br><span class="line">| status              | DOWN                                 |</span><br><span class="line">| subnet_id           | 72ec3fbe-05b5-4282-a01a-7f22b1d432ca |</span><br><span class="line">| updated_at          | 2022-09-03T04:32:48Z                 |</span><br><span class="line">+---------------------+--------------------------------------+</span><br><span class="line">[root@controller ~]# openstack port list</span><br><span class="line">+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------+--------+</span><br><span class="line">| ID                                   | Name | MAC Address       | Fixed IP Addresses                                                          | Status |</span><br><span class="line">+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------+--------+</span><br><span class="line">| 165a8658-212f-4d5c-9f57-34e6bfc191fb |      | fa:16:3e:af:b7:9b | ip_address=&#x27;172.18.7.102&#x27;, subnet_id=&#x27;72ec3fbe-05b5-4282-a01a-7f22b1d432ca&#x27; | ACTIVE |</span><br><span class="line">| 268a1196-3f7d-46f7-a253-c5dd35e02d36 |      | fa:16:3e:da:05:46 | ip_address=&#x27;10.0.0.106&#x27;, subnet_id=&#x27;95eefbdd-5669-4840-a1e6-2bcd67ae1208&#x27;   | DOWN   |</span><br><span class="line">| 2ce34897-ec52-4078-8cb0-698b86ab8150 |      | fa:16:3e:42:a4:6a | ip_address=&#x27;172.18.7.100&#x27;, subnet_id=&#x27;72ec3fbe-05b5-4282-a01a-7f22b1d432ca&#x27; | ACTIVE |</span><br><span class="line">| 7bae72cd-cd60-4507-8390-6ee41294232d |      | fa:16:3e:24:e7:d5 | ip_address=&#x27;172.18.7.101&#x27;, subnet_id=&#x27;72ec3fbe-05b5-4282-a01a-7f22b1d432ca&#x27; | N/A    |</span><br><span class="line">| 7db84b25-4b39-4865-9ac0-86331f2f0723 |      | fa:16:3e:b9:a4:ba | ip_address=&#x27;10.0.0.1&#x27;, subnet_id=&#x27;95eefbdd-5669-4840-a1e6-2bcd67ae1208&#x27;     | ACTIVE |</span><br><span class="line">| 853035c4-c428-4f3c-b855-bf9fa6909c7e |      | fa:16:3e:f7:6c:8b | ip_address=&#x27;10.0.0.104&#x27;, subnet_id=&#x27;95eefbdd-5669-4840-a1e6-2bcd67ae1208&#x27;   | ACTIVE |</span><br><span class="line">| e77ba178-5500-4f8d-b4e9-de61e42dc387 |      | fa:16:3e:27:0d:fd | ip_address=&#x27;10.0.1.100&#x27;, subnet_id=&#x27;4a736169-b4e4-4c88-bd7b-b8ea02443fdf&#x27;   | ACTIVE |</span><br><span class="line">| ef84f74d-acef-4eb6-893b-d1dc4996f7f9 |      | fa:16:3e:62:af:e0 | ip_address=&#x27;10.0.0.100&#x27;, subnet_id=&#x27;95eefbdd-5669-4840-a1e6-2bcd67ae1208&#x27;   | ACTIVE |</span><br><span class="line">+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------+--------+</span><br><span class="line">[root@controller ~]# openstack floating ip list</span><br><span class="line">+--------------------------------------+---------------------+------------------+------+--------------------------------------+----------------------------------+</span><br><span class="line">| ID                                   | Floating IP Address | Fixed IP Address | Port | Floating Network                     | Project                          |</span><br><span class="line">+--------------------------------------+---------------------+------------------+------+--------------------------------------+----------------------------------+</span><br><span class="line">| d896a909-e912-4bd3-9545-4193eb6636b6 | 172.18.7.101        | None             | None | 2ba422fb-1ad9-4509-b9a4-643b82112ce6 | 4350b89460a148d7bf1b2ae63296a6bd |</span><br><span class="line">+--------------------------------------+---------------------+------------------+------+--------------------------------------+----------------------------------+</span><br><span class="line">[root@controller ~]# openstack floating ip set 172.18.7.101 --port 853035c4-c428-4f3c-b855-bf9fa6909c7e</span><br></pre></td></tr></table></figure>

<p>11.虚拟机管理 使用 openstack 命令利用 centos7.5 镜像创建一台云主机，连接 int-net1 网 络，云主机名称为 test-02。创建成功后使用命令查看云主机详细信息，确定该 云主机是处于计算节点还是控制节点。如果云主机处于控制节点上请将其冷迁移 到计算节点，如果如果云主机处于计算节点上请将其冷迁移到控制节点。 本题全部流程请使用命令完成，请将全部命令和结果粘贴到答题框。【3 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server create test-02 --network int-net1 --flavor flavor1 --image cirros</span><br><span class="line">+-------------------------------------+------------------------------------------------+</span><br><span class="line">| Field                               | Value                                          |</span><br><span class="line">+-------------------------------------+------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                         |</span><br><span class="line">| OS-EXT-AZ:availability_zone         |                                                |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | None                                           |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | None                                           |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       |                                                |</span><br><span class="line">| OS-EXT-STS:power_state              | NOSTATE                                        |</span><br><span class="line">| OS-EXT-STS:task_state               | scheduling                                     |</span><br><span class="line">| OS-EXT-STS:vm_state                 | building                                       |</span><br><span class="line">| OS-SRV-USG:launched_at              | None                                           |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                           |</span><br><span class="line">| accessIPv4                          |                                                |</span><br><span class="line">| accessIPv6                          |                                                |</span><br><span class="line">| addresses                           |                                                |</span><br><span class="line">| adminPass                           | 4jH8K2fgLce9                                   |</span><br><span class="line">| config_drive                        |                                                |</span><br><span class="line">| created                             | 2022-09-03T04:53:28Z                           |</span><br><span class="line">| flavor                              | flavor1 (471938fe-abdd-4ec1-8c59-3f81e703d3c9) |</span><br><span class="line">| hostId                              |                                                |</span><br><span class="line">| id                                  | 7c9cc958-b2a1-4b24-bb0a-eb8de9c1612a           |</span><br><span class="line">| image                               | cirros (cbfdbd87-71df-4be7-8eb2-6904f781239c)  |</span><br><span class="line">| key_name                            | None                                           |</span><br><span class="line">| name                                | test-02                                        |</span><br><span class="line">| progress                            | 0                                              |</span><br><span class="line">| project_id                          | 4350b89460a148d7bf1b2ae63296a6bd               |</span><br><span class="line">| properties                          |                                                |</span><br><span class="line">| security_groups                     | name=&#x27;default&#x27;                                 |</span><br><span class="line">| status                              | BUILD                                          |</span><br><span class="line">| updated                             | 2022-09-03T04:53:28Z                           |</span><br><span class="line">| user_id                             | 4aec1580c77d4222964b7947d3239a88               |</span><br><span class="line">| volumes_attached                    |                                                |</span><br><span class="line">+-------------------------------------+------------------------------------------------+</span><br><span class="line">[root@controller ~]# openstack server show test-02</span><br><span class="line">+-------------------------------------+----------------------------------------------------------+</span><br><span class="line">| Field                               | Value                                                    |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                                   |</span><br><span class="line">| OS-EXT-AZ:availability_zone         | nova                                                     |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | compute                                                  |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | compute                                                  |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       | instance-00000003                                        |</span><br><span class="line">| OS-EXT-STS:power_state              | Running                                                  |</span><br><span class="line">| OS-EXT-STS:task_state               | None                                                     |</span><br><span class="line">| OS-EXT-STS:vm_state                 | active                                                   |</span><br><span class="line">| OS-SRV-USG:launched_at              | 2022-09-03T04:53:35.000000                               |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                                     |</span><br><span class="line">| accessIPv4                          |                                                          |</span><br><span class="line">| accessIPv6                          |                                                          |</span><br><span class="line">| addresses                           | int-net1=10.0.0.103                                      |</span><br><span class="line">| config_drive                        |                                                          |</span><br><span class="line">| created                             | 2022-09-03T04:53:28Z                                     |</span><br><span class="line">| flavor                              | flavor1 (471938fe-abdd-4ec1-8c59-3f81e703d3c9)           |</span><br><span class="line">| hostId                              | 2f20d41597d1aadaae23573c0d04d519d2bbad28e3c962ed11f16545 |</span><br><span class="line">| id                                  | 7c9cc958-b2a1-4b24-bb0a-eb8de9c1612a                     |</span><br><span class="line">| image                               | cirros (cbfdbd87-71df-4be7-8eb2-6904f781239c)            |</span><br><span class="line">| key_name                            | None                                                     |</span><br><span class="line">| name                                | test-02                                                  |</span><br><span class="line">| progress                            | 0                                                        |</span><br><span class="line">| project_id                          | 4350b89460a148d7bf1b2ae63296a6bd                         |</span><br><span class="line">| properties                          |                                                          |</span><br><span class="line">| security_groups                     | name=&#x27;default&#x27;                                           |</span><br><span class="line">| status                              | ACTIVE                                                   |</span><br><span class="line">| updated                             | 2022-09-03T04:53:35Z                                     |</span><br><span class="line">| user_id                             | 4aec1580c77d4222964b7947d3239a88                         |</span><br><span class="line">| volumes_attached                    |                                                          |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------+</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server stop test-02</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@compute ~]# scp -r /var/lib/nova/instances/7c9cc958-b2a1-4b24-bb0a-eb8de9c1612a root@controller:/var/lib/nova/instances/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# chown -R nova:nova /var/lib/nova/instances/7c9cc958-b2a1-4b24-bb0a-eb8de9c1612a</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MariaDB [nova]&gt; update instances set host=&#x27;controller&#x27;,node=&#x27;controller&#x27; where uuid=&#x27;7c9cc958-b2a1-4b24-bb0a-eb8de9c1612a&#x27;;</span><br><span class="line">Query OK, 1 row affected (0.00 sec)</span><br><span class="line">Rows matched: 1  Changed: 1  Warnings: 0</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@controller ~]# openstack server show test-02</span><br><span class="line">+-------------------------------------+----------------------------------------------------------+</span><br><span class="line">| Field                               | Value                                                    |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------+</span><br><span class="line">| OS-DCF:diskConfig                   | MANUAL                                                   |</span><br><span class="line">| OS-EXT-AZ:availability_zone         | nova                                                     |</span><br><span class="line">| OS-EXT-SRV-ATTR:host                | controller                                               |</span><br><span class="line">| OS-EXT-SRV-ATTR:hypervisor_hostname | controller                                               |</span><br><span class="line">| OS-EXT-SRV-ATTR:instance_name       | instance-00000003                                        |</span><br><span class="line">| OS-EXT-STS:power_state              | Shutdown                                                 |</span><br><span class="line">| OS-EXT-STS:task_state               | None                                                     |</span><br><span class="line">| OS-EXT-STS:vm_state                 | stopped                                                  |</span><br><span class="line">| OS-SRV-USG:launched_at              | 2022-09-03T04:53:35.000000                               |</span><br><span class="line">| OS-SRV-USG:terminated_at            | None                                                     |</span><br><span class="line">| accessIPv4                          |                                                          |</span><br><span class="line">| accessIPv6                          |                                                          |</span><br><span class="line">| addresses                           | int-net1=10.0.0.103                                      |</span><br><span class="line">| config_drive                        |                                                          |</span><br><span class="line">| created                             | 2022-09-03T04:53:28Z                                     |</span><br><span class="line">| flavor                              | flavor1 (471938fe-abdd-4ec1-8c59-3f81e703d3c9)           |</span><br><span class="line">| hostId                              | f389bf51bf4a9682dd0667b3a4e643427d68c0f3373c065e3a9e2f15 |</span><br><span class="line">| id                                  | 7c9cc958-b2a1-4b24-bb0a-eb8de9c1612a                     |</span><br><span class="line">| image                               | cirros (cbfdbd87-71df-4be7-8eb2-6904f781239c)            |</span><br><span class="line">| key_name                            | None                                                     |</span><br><span class="line">| name                                | test-02                                                  |</span><br><span class="line">| project_id                          | 4350b89460a148d7bf1b2ae63296a6bd                         |</span><br><span class="line">| properties                          |                                                          |</span><br><span class="line">| security_groups                     | name=&#x27;default&#x27;                                           |</span><br><span class="line">| status                              | SHUTOFF                                                  |</span><br><span class="line">| updated                             | 2022-09-03T05:01:54Z                                     |</span><br><span class="line">| user_id                             | 4aec1580c77d4222964b7947d3239a88                         |</span><br><span class="line">| volumes_attached                    |                                                          |</span><br><span class="line">+-------------------------------------+----------------------------------------------------------+</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>私有云</tag>
      </tags>
  </entry>
  <entry>
    <title>金砖国赛样题二</title>
    <url>/2022/09/06/%E9%87%91%E7%A0%96%E5%9B%BD%E8%B5%9B%E6%A0%B7%E9%A2%98%E4%BA%8C/</url>
    <content><![CDATA[<h1 id="2022金砖国家职业技能大赛样题"><a href="#2022金砖国家职业技能大赛样题" class="headerlink" title="2022金砖国家职业技能大赛样题"></a>2022金砖国家职业技能大赛样题</h1><h2 id="B-场次题目：容器的编排与运维-gt"><a href="#B-场次题目：容器的编排与运维-gt" class="headerlink" title="B 场次题目：容器的编排与运维&gt;"></a>B 场次题目：容器的编排与运维<span id="more"></span>&gt;</h2><h3 id="任务-1-容器云平台环境初始化（5-分）"><a href="#任务-1-容器云平台环境初始化（5-分）" class="headerlink" title="任务 1 容器云平台环境初始化（5 分）"></a>任务 1 容器云平台环境初始化（5 分）</h3><p>1.容器云平台的初始化 根据表 2 的 IP 地址规划，创建云服务器，镜像使用 CentOS_7.5_x86_64_XD. qcow，确保网络正常通信。按照表 2 置主机名节点并关闭 swap，同时永久关闭 s elinux 以及防火墙,并修改 hosts 映射。 请将 master 节点 hosts 文件内容提交到答题框。【1 分】</p>
<p><strong>主机名节点并关闭 swap</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# swapoff -a</span><br></pre></td></tr></table></figure>

<p><strong>永久关闭 s elinux 以及防火墙</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# setenforce 0</span><br><span class="line">setenforce: SELinux is disabled</span><br><span class="line">[root@master ~]# cat /etc/selinux/config </span><br><span class="line"></span><br><span class="line"># This file controls the state of SELinux on the system.</span><br><span class="line"></span><br><span class="line"># SELINUX= can take one of these three values:</span><br><span class="line"></span><br><span class="line">#     enforcing - SELinux security policy is enforced.</span><br><span class="line"></span><br><span class="line">#     permissive - SELinux prints warnings instead of enforcing.</span><br><span class="line"></span><br><span class="line">#     disabled - No SELinux policy is loaded.</span><br><span class="line"></span><br><span class="line">SELINUX=disabled</span><br><span class="line"></span><br><span class="line"># SELINUXTYPE= can take one of three two values:</span><br><span class="line"></span><br><span class="line">#     targeted - Targeted processes are protected,</span><br><span class="line"></span><br><span class="line">#     minimum - Modification of targeted policy. Only selected processes are protected. </span><br><span class="line"></span><br><span class="line">#     mls - Multi Level Security protection.</span><br><span class="line"></span><br><span class="line">SELINUXTYPE=targeted</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>修改 hosts 映射</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.20.115	master</span><br><span class="line">192.168.20.122	node1</span><br><span class="line">192.168.20.112	node2</span><br><span class="line">192.168.20.114	harbor</span><br></pre></td></tr></table></figure>

<p>2．Yum 源数据的持久化挂载 将提供的 CentOS-7-x86_64-DVD-1804.iso 和 chinaskills_cloud_paas.iso 光盘镜像上传到 master 节点 &#x2F;root 目录下，然后在 &#x2F;opt 目录下使用命令创建 &#x2F;centos 目录和 &#x2F;paas 目录，并将镜像文件 CentOS-7-x86_64-DVD-1804.iso 挂 载到&#x2F;centos 目录下，将镜像文件 chinaskills_cloud_paas.iso 挂载到 &#x2F;paas 目录下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# curl -O http://172.19.25.11/middle/chinaskills_cloud_paas.iso</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100 8825M  100 8825M    0     0   111M      0  0:01:19  0:01:19 --:--:--  111M</span><br><span class="line">[root@master ~]# curl -O http://172.19.25.11/middle/CentOS-7-x86_64-DVD-1804.iso</span><br><span class="line">  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current</span><br><span class="line">                                 Dload  Upload   Total   Spent    Left  Speed</span><br><span class="line">100 4263M  100 4263M    0     0   111M      0  0:00:38  0:00:38 --:--:--  106M</span><br><span class="line">[root@master ~]# mount chinaskills_cloud_paas.iso /mnt/</span><br><span class="line">mount: /dev/loop0 is write-protected, mounting read-only</span><br><span class="line">[root@master ~]# mkdir -p /opt/paas</span><br><span class="line">[root@master ~]# mkdir -p /opt/centos</span><br><span class="line">[root@master ~]# cp -rf /mnt/* /opt/paas/</span><br><span class="line">[root@master ~]# umount /mnt/</span><br><span class="line">[root@master ~]# mount CentOS-7-x86_64-DVD-1804.iso /mnt/</span><br><span class="line">mount: /dev/loop0 is write-protected, mounting read-only</span><br><span class="line">[root@master ~]# cp -rf /mnt/* /opt/centos/</span><br><span class="line">[root@master ~]# umount /mnt/</span><br></pre></td></tr></table></figure>

<p>3．Yum 源的编写 为 master 节点设置本地 yum 源，yum 源文件名为 centos.repo，安装 ftp 服 务，将 ftp 仓库设置为 &#x2F;opt&#x2F;，为其他节点配置 ftp 源，yum 源文件名称为 ftp. repo，其中 ftp 服务器地址为 master 节点 IP。 请将其它节点的 yum 源文件内容提交到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat /etc/yum.repos.d/local.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=file:///opt/centos/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[k8s]</span><br><span class="line">name=k8s</span><br><span class="line">baseurl=file:///opt/paas/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line">[root@master ~]# yum install -y vsftpd</span><br><span class="line">[root@master ~]# echo &quot;anon_root=/opt/&quot; &gt;&gt; /etc/vsftpd/vsftpd.conf </span><br><span class="line">[root@master ~]# systemctl restart vsftpd</span><br><span class="line">[root@master ~]# systemctl enable vsftpd</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/vsftpd.service to /usr/lib/systemd/system/vsftpd.service.</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# cat /etc/yum.repos.d/ftp.repo </span><br><span class="line">[centos]</span><br><span class="line">name=centos</span><br><span class="line">baseurl=ftp://master/centos/</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br><span class="line"></span><br><span class="line">[k8s]</span><br><span class="line">name=k8s</span><br><span class="line">baseurl=ftp://master/paas/kubernetes-repo</span><br><span class="line">gpgcheck=0</span><br><span class="line">enabled=1</span><br></pre></td></tr></table></figure>

<p>4．设置时间同步服务器 在 master 节点上部署 chrony 服务器，允许其他节点同步时间，启动服务并 设置为开机启动；在其他节点上指定 master 节点为上游 NTP 服务器，重启服务 并设为开机启动。 请在 master 节点上使用 chronyc 命令同步控制节点的系统时间。</p>
<p><strong>master 节点</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat /etc/chrony.conf </span><br><span class="line"></span><br><span class="line"># Use public servers from the pool.ntp.org project.</span><br><span class="line"></span><br><span class="line"># Please consider joining the pool (http://www.pool.ntp.org/join.html).</span><br><span class="line"></span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line">server	master	iburst</span><br><span class="line"></span><br><span class="line"># Record the rate at which the system clock gains/losses time.</span><br><span class="line"></span><br><span class="line">driftfile /var/lib/chrony/drift</span><br><span class="line"></span><br><span class="line"># Allow the system clock to be stepped in the first three updates</span><br><span class="line"></span><br><span class="line"># if its offset is larger than 1 second.</span><br><span class="line"></span><br><span class="line">makestep 1.0 3</span><br><span class="line"></span><br><span class="line"># Enable kernel synchronization of the real-time clock (RTC).</span><br><span class="line"></span><br><span class="line">rtcsync</span><br><span class="line"></span><br><span class="line"># Enable hardware timestamping on all interfaces that support it.</span><br><span class="line"></span><br><span class="line">#hwtimestamp *</span><br><span class="line"></span><br><span class="line"># Increase the minimum number of selectable sources required to adjust</span><br><span class="line"></span><br><span class="line"># the system clock.</span><br><span class="line"></span><br><span class="line">#minsources 2</span><br><span class="line"></span><br><span class="line"># Allow NTP client access from local network.</span><br><span class="line"></span><br><span class="line">#allow 192.168.0.0/16</span><br><span class="line"></span><br><span class="line"># Serve time even if not synchronized to a time source.</span><br><span class="line"></span><br><span class="line">#local stratum 10</span><br><span class="line"></span><br><span class="line"># Specify file containing keys for NTP authentication.</span><br><span class="line"></span><br><span class="line">#keyfile /etc/chrony.keys</span><br><span class="line"></span><br><span class="line"># Specify directory for log files.</span><br><span class="line"></span><br><span class="line">logdir /var/log/chrony</span><br><span class="line"></span><br><span class="line"># Select which information is logged.</span><br><span class="line"></span><br><span class="line">#log measurements statistics tracking</span><br><span class="line">allow 192.168.20.0/24</span><br><span class="line">local stratum 10</span><br></pre></td></tr></table></figure>

<p><strong>node节点</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# cat /etc/chrony.conf </span><br><span class="line"></span><br><span class="line"># Use public servers from the pool.ntp.org project.</span><br><span class="line"></span><br><span class="line"># Please consider joining the pool (http://www.pool.ntp.org/join.html).</span><br><span class="line"></span><br><span class="line">#server 0.centos.pool.ntp.org iburst</span><br><span class="line">#server 1.centos.pool.ntp.org iburst</span><br><span class="line">#server 2.centos.pool.ntp.org iburst</span><br><span class="line">#server 3.centos.pool.ntp.org iburst</span><br><span class="line">server	master	iburst</span><br><span class="line"></span><br><span class="line"># Record the rate at which the system clock gains/losses time.</span><br><span class="line"></span><br><span class="line">driftfile /var/lib/chrony/drift</span><br><span class="line"></span><br><span class="line"># Allow the system clock to be stepped in the first three updates</span><br><span class="line"></span><br><span class="line"># if its offset is larger than 1 second.</span><br><span class="line"></span><br><span class="line">makestep 1.0 3</span><br><span class="line"></span><br><span class="line"># Enable kernel synchronization of the real-time clock (RTC).</span><br><span class="line"></span><br><span class="line">rtcsync</span><br><span class="line"></span><br><span class="line"># Enable hardware timestamping on all interfaces that support it.</span><br><span class="line"></span><br><span class="line">#hwtimestamp *</span><br><span class="line"></span><br><span class="line"># Increase the minimum number of selectable sources required to adjust</span><br><span class="line"></span><br><span class="line"># the system clock.</span><br><span class="line"></span><br><span class="line">#minsources 2</span><br><span class="line"></span><br><span class="line"># Allow NTP client access from local network.</span><br><span class="line"></span><br><span class="line">#allow 192.168.0.0/16</span><br><span class="line"></span><br><span class="line"># Serve time even if not synchronized to a time source.</span><br><span class="line"></span><br><span class="line">#local stratum 10</span><br><span class="line"></span><br><span class="line"># Specify file containing keys for NTP authentication.</span><br><span class="line"></span><br><span class="line">#keyfile /etc/chrony.keys</span><br><span class="line"></span><br><span class="line"># Specify directory for log files.</span><br><span class="line"></span><br><span class="line">logdir /var/log/chrony</span><br><span class="line"></span><br><span class="line"># Select which information is logged.</span><br><span class="line"></span><br><span class="line">#log measurements statistics tracking</span><br></pre></td></tr></table></figure>

<p>5.设置免密登录 为四台服务器设置免密登录，保证 3 台服务器能够互相免密登录。请使用 s cp 命令将 master 节点的 hosts 文件发送到所有节点的 &#x2F;etc&#x2F;hosts。将以上所 有命令和返回结果提交到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# ssh-keygen </span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">/root/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? </span><br><span class="line">[root@master ~]# ssh-copy-id node1</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line"></span><br><span class="line">/usr/bin/ssh-copy-id: WARNING: All keys were skipped because they already exist on the remote system.</span><br><span class="line">		(if you think this is a mistake, you may want to use -f option)</span><br><span class="line"></span><br><span class="line">[root@master ~]# ssh-copy-id node2</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line"></span><br><span class="line">/usr/bin/ssh-copy-id: WARNING: All keys were skipped because they already exist on the remote system.</span><br><span class="line">		(if you think this is a mistake, you may want to use -f option)</span><br><span class="line"></span><br><span class="line">[root@master ~]# ssh-copy-id harbor</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: &quot;/root/.ssh/id_rsa.pub&quot;</span><br><span class="line">/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed</span><br><span class="line"></span><br><span class="line">/usr/bin/ssh-copy-id: WARNING: All keys were skipped because they already exist on the remote system.</span><br><span class="line">		(if you think this is a mistake, you may want to use -f option)</span><br></pre></td></tr></table></figure>

<h3 id="任务-2-Kubernetes-搭建任务（10-分）"><a href="#任务-2-Kubernetes-搭建任务（10-分）" class="headerlink" title="任务 2 Kubernetes 搭建任务（10 分）"></a>任务 2 Kubernetes 搭建任务（10 分）</h3><p>1.安装 docker 应用 在所有节点上安装 dokcer-ce。并在 harbor 节点安装 harbor 仓库，显现正 常登录 horbor 仓库，登录密码设置为“test_工位号”。请将登录后截图提交到 答题框。【1 分】</p>
<p><strong>所有节点</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# yum install -y yum-utils device-mapper-p* lvm2</span><br><span class="line">[root@master ~]# yum  install -y docker-ce</span><br><span class="line">[root@master ~]# systemctl restart docker</span><br><span class="line">[root@master ~]# systemctl enable docker</span><br></pre></td></tr></table></figure>

<p><strong>harbor节点</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /opt/harbor/</span><br><span class="line">tar -zxvf harbor-offline-installer-v2.1.0.tgz</span><br><span class="line">cd harbor</span><br><span class="line">mv harbor.yml.tmpl harbor.yml</span><br><span class="line">sed -i &quot;5s/reg.mydomain.com/$&#123;IP&#125;/g&quot; harbor.yml</span><br><span class="line">sed -i &quot;13s/^/#/g&quot; harbor.yml</span><br><span class="line">sed -i &quot;15,18s/^/#/g&quot; harbor.yml</span><br><span class="line">./prepare || exit</span><br><span class="line">./install.sh --with-clair || exit</span><br></pre></td></tr></table></figure>

<p>2.搭建 horbor 仓库 修改默认 docker 仓库为 horbor 地址，修改 docker 启动引擎为 systemd。 安装完成后执行 docker verison 命令返回结果以及将 daemon.json 文件内容提 交。【2 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat /etc/docker/daemon.json </span><br><span class="line">&#123;</span><br><span class="line"> &quot;insecure-registries&quot;:[&quot;192.168.20.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;]</span><br><span class="line">&#125;</span><br><span class="line">[root@master ~]# systemctl restart docker</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# docker version</span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           19.03.13</span><br><span class="line"> API version:       1.40</span><br><span class="line"> Go version:        go1.13.15</span><br><span class="line"> Git commit:        4484c46d9d</span><br><span class="line"> Built:             Wed Sep 16 17:03:45 2020</span><br><span class="line"> OS/Arch:           linux/amd64</span><br><span class="line"> Experimental:      false</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          19.03.13</span><br><span class="line">  API version:      1.40 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.13.15</span><br><span class="line">  Git commit:       4484c46d9d</span><br><span class="line">  Built:            Wed Sep 16 17:02:21 2020</span><br><span class="line">  OS/Arch:          linux/amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.3.7</span><br><span class="line">  GitCommit:        8fba4e9a7d01810a393d5d25a3621dc101981175</span><br><span class="line"> runc:</span><br><span class="line">  Version:          1.0.0-rc10</span><br><span class="line">  GitCommit:        dc9208a3303feef5b3839f4323d9beb36df0a9dd</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.18.0</span><br><span class="line">  GitCommit:        fec3683</span><br></pre></td></tr></table></figure>

<p>3.安装 docker-compose 在 master 节点上使用 &#x2F;opt&#x2F;paas&#x2F;docker-compose&#x2F;v1.25.5-docker-compo se-Linux-x86_6 下的文件安装 docker-compose。安装完成后执行 docker-compose version 命令，请将程序返回结果提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# mv /opt/paas/docker-compose/v1.25.5-docker-compose-Linux-x86_64 /usr/local/bin/docker-compose</span><br><span class="line">[root@master ~]# docker-compose version</span><br><span class="line">docker-compose version 1.25.5, build 8a1c60f6</span><br><span class="line">docker-py version: 4.1.0</span><br><span class="line">CPython version: 3.7.5</span><br><span class="line">OpenSSL version: OpenSSL 1.1.0l  10 Sep 2019</span><br></pre></td></tr></table></figure>

<p>4.上传 docker 镜像 在 master 节点使用 &#x2F;opt&#x2F;paas&#x2F; k8s_image_push.sh 将所有镜像上传至 do cker 仓库。完成后将 Harbor 仓库 library 中镜像列表截图，请将以上截图提交 到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# for i in $(ls /opt/paas/images/); do docker load -i /opt/paas/images/$i; done</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master paas]# ./k8s_image_push.sh </span><br></pre></td></tr></table></figure>

<p>5.安装 kubeadm 工具 在 master 及 node 节点安装 Kubeadm 工具并设置开机自动启动，安装完成后 使用 rpm 命令配合 grep 查看 Kubeadm 工具是否正确安装。将 rpm 命令配合 grep 返回结果提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master paas]# yum install -y kubelet-1.18.1 kubeadm-1.18.1 kubectl-1.18.1</span><br><span class="line">[root@master paas]# systemctl enable kubelet</span><br><span class="line">Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /usr/lib/systemd/system/kubelet.service.</span><br><span class="line">[root@master paas]# systemctl restart kubelet</span><br><span class="line">[root@master paas]# rpm -qa | grep kubeadm</span><br><span class="line">kubeadm-1.18.1-0.x86_64</span><br></pre></td></tr></table></figure>

<p>6.计算节点获取必备镜像 在所有 node 节点中使用 docker 命令拉取安装 kubernetes 基础镜像，拉取 完成后使用 docker 命令查看镜像列表。【1 分】</p>
<p>7.kubeadm 安装 master 使用 kubeadm 命令初始化 master 节点，设置 kubernetes 虚拟内部网段地址 为 10.244.0.0&#x2F;16，然后使用 kube-flannel.yaml 完成控制节点初始化设置，完 成后使用命令查看集群状态和所有 pod。 将以上命令和返回结果提交到答题框。【2 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# modprobe br_netfilter</span><br><span class="line">[root@master ~]# echo &quot;net.ipv4.ip_forward = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">[root@master ~]# echo &quot;net.bridge.bridge-nf-call-ip6tables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">[root@master ~]# echo &quot;net.bridge.bridge-nf-call-iptables = 1&quot; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">[root@master ~]# sysctl -p</span><br><span class="line">[root@master ~]# kubeadm init --kubernetes-version=1.18.1 --apiserver-advertise-address=192.168.20.115 --image-repository 192.168.20.114/library --pod-network-cidr=10.244.0.0/16</span><br><span class="line">[root@master ~]# mkdir -p /root/.kube</span><br><span class="line">[root@master ~]# cp -i /etc/kubernetes/admin.conf /root/.kube/config</span><br><span class="line">[root@master ~]# chown $(id -u):$(id -g) /root/.kube/config </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# sed -i &#x27;s/quay.io\/coreos/192.168.20.114\/library/g&#x27; /opt/paas/yaml/flannel/kube-flannel.yaml </span><br><span class="line">[root@master ~]# kubectl apply -f /opt/paas/yaml/flannel/kube-flannel.yaml </span><br><span class="line">podsecuritypolicy.policy/psp.flannel.unprivileged created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/flannel created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/flannel created</span><br><span class="line">serviceaccount/flannel created</span><br><span class="line">configmap/kube-flannel-cfg created</span><br><span class="line">daemonset.apps/kube-flannel-ds created</span><br><span class="line">[root@master ~]# kubectl get pod --all-namespaces</span><br><span class="line">NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   coredns-f8789844f-5v894          1/1     Running   0          7m54s</span><br><span class="line">kube-system   coredns-f8789844f-vfh6t          1/1     Running   0          7m54s</span><br><span class="line">kube-system   etcd-master                      1/1     Running   0          8m7s</span><br><span class="line">kube-system   kube-apiserver-master            1/1     Running   0          8m7s</span><br><span class="line">kube-system   kube-controller-manager-master   1/1     Running   0          8m7s</span><br><span class="line">kube-system   kube-flannel-ds-xvn9p            1/1     Running   0          43s</span><br><span class="line">kube-system   kube-proxy-fjkd2                 1/1     Running   0          7m54s</span><br><span class="line">kube-system   kube-scheduler-master            1/1     Running   0          8m7s</span><br></pre></td></tr></table></figure>

<p>8.安装 kubernetes 网络插件 使用 kube-flannel.yaml 安装 kubernetes 网络插件，安装完成后使用命令 查看节点状态。完成后使用命令查看集群状态。将集群状态查看命令和返回结果 提交到答题框。【0.5 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get nodes</span><br><span class="line">NAME     STATUS   ROLES    AGE   VERSION</span><br><span class="line">master   Ready    master   10m   v1.18.1</span><br><span class="line">[root@master ~]# kubectl cluster-info</span><br><span class="line">Kubernetes master is running at https://192.168.20.115:6443</span><br><span class="line">KubeDNS is running at https://192.168.20.115:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#x27;kubectl cluster-info dump&#x27;.</span><br></pre></td></tr></table></figure>

<p>9.kubernetes 图形化界面的安装 安装 kubernetes dashboard 界面，完成后查看首页然后将 kubernetes dashboard 界面截图提交到答题框。【1 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir dashboard-certs</span><br><span class="line">cd dashboard-certs/</span><br><span class="line">kubectl create namespace kubernetes-dashboard</span><br><span class="line">openssl genrsa -out dashboard.key 2048</span><br><span class="line">openssl req -days 36000 -new -out dashboard.csr -key dashboard.key -subj &#x27;/CN=dashboard-cert&#x27;</span><br><span class="line">openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt</span><br><span class="line">kubectl create secret generic kubernetes-dashboard-certs --from-file=dashboard.key --from-file=dashboard.crt -n kubernetes-dashboard</span><br><span class="line">sed -i &quot;s/kubernetesui/$IP\/library/g&quot; /opt/yaml/dashboard/recommended.yaml </span><br><span class="line">kubectl apply -f /opt/yaml/dashboard/recommended.yaml </span><br><span class="line">kubectl apply -f /opt/yaml/dashboard/dashboard-adminuser.yaml</span><br></pre></td></tr></table></figure>

<p>10.扩展计算节点 在 master 节点上使用 kubeadm 命令查看 token，在所有 node 节点上使用 k ubeadm 命令将 node 节点加入 kubernetes 集群。完成后在 master 节点上查看所 有节点状态。</p>
<p><strong>所有node节点</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">modprobe br_netfilter</span><br><span class="line">cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables = 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables = 1</span><br><span class="line">EOF</span><br><span class="line">sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@node1 ~]# cat /etc/docker/daemon.json </span><br><span class="line">&#123;</span><br><span class="line"> &quot;insecure-registries&quot;:[&quot;192.168.20.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;:[&quot;native.cgroupdriver=systemd&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh master &quot;kubeadm token create --print-join-command&quot; &gt;token.sh</span><br><span class="line">chmod +x token.sh &amp;&amp; source token.sh &amp;&amp; rm -rf token.sh</span><br><span class="line">sleep 20</span><br><span class="line">ssh master &quot;kubectl get nodes&quot;</span><br></pre></td></tr></table></figure>

<h3 id="任务-3-Kubernetes-运维任务（15-分）"><a href="#任务-3-Kubernetes-运维任务（15-分）" class="headerlink" title="任务 3 Kubernetes 运维任务（15 分）"></a>任务 3 Kubernetes 运维任务（15 分）</h3><p>1.使用 dockerfile 构建 dokcer 镜像  以 mysql:5.7 镜像为基础镜像，制作一个 mysql 镜像，可以将提供的 sql 文 件初始化到mysql数据库中，然后使用编写的dockerfile文件将镜像制作出来， 名称为 mysql:latest，并将该镜像上传至前面所搭建的 harbor 仓库中，编写 Y AML 文件，验证数据库内容。 完成后将 dockerfile 文件内容及 harbor 仓库镜像列表、数据库内容提交到 答题框。【1 分】</p>
<p>2.持久化存储 搭建 NFS 共享存储，配置 nfs-provisioner，创建 storageclass，通过 storageclass 动态生成 pvc，大小为 1Gi，修改标准 nfs-deployment.yaml 文件， 编写 storageclass.yaml 和 pvc.yaml 文件，将最终 pvc 状态截图和 yaml 文件提 交至答题框。【2 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# yum install -y nfs-utils rpcbind</span><br><span class="line">[root@master ~]# cat /etc/exports</span><br><span class="line">/root/nfs *(rw,async,no_root_squash)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat rpac.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line"></span><br><span class="line">  namespace: default        #根据实际环境设定namespace,下面类同</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line"></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line"></span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">    # replace with namespace where provisioner is deployed</span><br><span class="line"></span><br><span class="line">    namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line"></span><br><span class="line">  namespace: default</span><br><span class="line">rules:</span><br><span class="line"></span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line"></span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">    # replace with namespace where provisioner is deployed</span><br><span class="line"></span><br><span class="line">    namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat storageclass.yaml </span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-nfs-storage</span><br><span class="line">provisioner: nfs-storage #这里的名称要和provisioner配置文件中的环境变量PROVISIONER_NAME保持一致</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: &quot;false&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat nfs-deployment.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  labels:</span><br><span class="line">    app: nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line"></span><br><span class="line">    namespace: default  #与RBAC文件中的namespace保持一致</span><br><span class="line"></span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nfs-client-provisioner</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-client-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nfs-client-provisioner</span><br><span class="line">      containers:</span><br><span class="line">        - name: nfs-client-provisioner</span><br><span class="line">          image: 192.168.20.114/library/nfs-client-provisioner:latest</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: nfs-client-root</span><br><span class="line">              mountPath: /persistentvolumes</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: nfs-storage  #provisioner名称,请确保该名称与 nfs-StorageClass.yaml文件中的provisioner名称保持一致</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 192.168.20.115   #NFS Server IP地址</span><br><span class="line">            - name: NFS_PATH  </span><br><span class="line">              value: /root/nfs    #NFS挂载卷</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            server: 192.168.20.115  #NFS Server IP地址</span><br><span class="line">            path: /root/nfs     #NFS 挂载卷</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat pvc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pvc</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteMany</span><br><span class="line">  storageClassName: managed-nfs-storage # StorageClass</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f rpac.yaml </span><br><span class="line">[root@master ~]# kubectl apply -f storageclass.yaml</span><br><span class="line">[root@master ~]# kubectl apply -f nfs-deployment.yaml </span><br><span class="line">[root@master ~]# kubectl apply -f pvc.yaml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get pod,sc,pvc,pv</span><br><span class="line">NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">pod/nfs-client-provisioner-8d5b467d-cr7qw   1/1     Running   0          15m</span><br><span class="line"></span><br><span class="line">NAME                                              PROVISIONER   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">storageclass.storage.k8s.io/managed-nfs-storage   nfs-storage   Delete          Immediate           false                  27m</span><br><span class="line"></span><br><span class="line">NAME                             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">persistentvolumeclaim/test-pvc   Bound    pvc-ee59105c-cc95-48ac-af39-75ab6ca51ac0   1Gi        RWX            managed-nfs-storage   2m24s</span><br><span class="line"></span><br><span class="line">NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS          REASON   AGE</span><br><span class="line">persistentvolume/pvc-ee59105c-cc95-48ac-af39-75ab6ca51ac0   1Gi        RWX            Delete           Bound    default/test-pvc   managed-nfs-storage            2m24s</span><br></pre></td></tr></table></figure>

<p>3.编写 deployment 文件 将提供的 nginx:latest 镜像上传至 harbor 镜像仓库，使用该镜像编写 dep loyment 文件，要求将已创建的 pvc 挂载至&#x2F;html 目录下，副本数 1，实现资源 限制:需求内存 300Mi，需求 CPU 300M，限制内存 450Mi，限制 CPU450M，将 POD 状态截图和 yaml 文件提交至答题框。【3 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat nginx-deployment.yaml </span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  strategy: &#123;&#125;</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      creationTimestamp: null</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: 192.168.20.114/library/nginx:latest</span><br><span class="line">        name: nginx</span><br><span class="line">        ports:</span><br><span class="line">          - name: nginx-port</span><br><span class="line">            containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">          - name: test-pvc</span><br><span class="line">            mountPath: /usr/share/nginx/html</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 450m</span><br><span class="line">            memory: 450Mi</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 300m</span><br><span class="line">            memory: 300Mi</span><br><span class="line">      volumes:</span><br><span class="line">        - name: test-pvc</span><br><span class="line">          persistentVolumeClaim:</span><br><span class="line">            claimName: test-pvc</span><br></pre></td></tr></table></figure>

<p>4.创建 service 服务，提供对外访问接口 基于 nginx 的 pod 服务，编写一个 service 名称为 nginx-svc，代理 nginx 的服务端口，端口类型为 nodeport，创建成功后可以通过该 service 访问 nginx。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat nginx-svc.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  labels:</span><br><span class="line">    app: nginx</span><br><span class="line">  name: nginx-svc</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    name: nginx-port   </span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  type: NodePort</span><br></pre></td></tr></table></figure>

<p>5.配置 metrics-server 实现资源监控 将已提供的 metrics-server 镜像上传至 harbor，修改 components.yaml， 创建 metrics-server，完成后，将 metrics-server 状态截图提交至答题框。【2 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat components.yaml </span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:aggregated-metrics-reader</span><br><span class="line">  labels:</span><br><span class="line">    rbac.authorization.k8s.io/aggregate-to-view: &quot;true&quot;</span><br><span class="line">    rbac.authorization.k8s.io/aggregate-to-edit: &quot;true&quot;</span><br><span class="line">    rbac.authorization.k8s.io/aggregate-to-admin: &quot;true&quot;</span><br><span class="line">rules:</span><br><span class="line"></span><br><span class="line">- apiGroups: [&quot;metrics.k8s.io&quot;]</span><br><span class="line">  resources: [&quot;pods&quot;, &quot;nodes&quot;]</span><br><span class="line">  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server:system:auth-delegator</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:auth-delegator</span><br><span class="line">subjects:</span><br><span class="line"></span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: RoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server-auth-reader</span><br><span class="line">  namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: Role</span><br><span class="line">  name: extension-apiserver-authentication-reader</span><br><span class="line">subjects:</span><br><span class="line"></span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apiregistration.k8s.io/v1beta1</span><br><span class="line">kind: APIService</span><br><span class="line">metadata:</span><br><span class="line">  name: v1beta1.metrics.k8s.io</span><br><span class="line">spec:</span><br><span class="line">  service:</span><br><span class="line">    name: metrics-server</span><br><span class="line">    namespace: kube-system</span><br><span class="line">  group: metrics.k8s.io</span><br><span class="line">  version: v1beta1</span><br><span class="line">  insecureSkipTLSVerify: true</span><br><span class="line">  groupPriorityMinimum: 100</span><br><span class="line"></span><br><span class="line">  versionPriority: 100</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line"></span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      k8s-app: metrics-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: metrics-server</span><br><span class="line">      labels:</span><br><span class="line">        k8s-app: metrics-server</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: metrics-server</span><br><span class="line">      volumes:</span><br><span class="line">      # mount in tmp so we can safely use from-scratch images and/or read-only containers</span><br><span class="line">      - name: tmp-dir</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      containers:</span><br><span class="line">      - name: metrics-server</span><br><span class="line">        image: 192.168.20.114/library/metrics-server-amd64:v0.3.6</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        args:</span><br><span class="line">          - --cert-dir=/tmp</span><br><span class="line">          - --secure-port=4443</span><br><span class="line">        ports:</span><br><span class="line">        - name: main-port</span><br><span class="line">          containerPort: 4443</span><br><span class="line">          protocol: TCP</span><br><span class="line">        command:</span><br><span class="line">        - /metrics-server</span><br><span class="line">        - --kubelet-insecure-tls</span><br><span class="line">        - --kubelet-preferred-address-types=InternalDNS,InternalIP,ExternalDNS,ExternalIP,Hostname</span><br><span class="line">        securityContext:</span><br><span class="line">          readOnlyRootFilesystem: true</span><br><span class="line">          runAsNonRoot: true</span><br><span class="line">          runAsUser: 1000</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: tmp-dir</span><br><span class="line">          mountPath: /tmp</span><br><span class="line">      #nodeSelector:</span><br><span class="line">        #kubernetes.io/os: linux</span><br><span class="line">        #kubernetes.io/arch: &quot;amd64&quot;</span><br><span class="line"></span><br><span class="line">      nodeName: master</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    kubernetes.io/name: &quot;Metrics-server&quot;</span><br><span class="line">    kubernetes.io/cluster-service: &quot;true&quot;</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    k8s-app: metrics-server</span><br><span class="line">  ports:</span><br><span class="line"></span><br><span class="line">  - port: 443</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: main-port</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRole</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">rules:</span><br><span class="line"></span><br><span class="line">- apiGroups:</span><br><span class="line">  - &quot;&quot;</span><br><span class="line">  resources:</span><br><span class="line">  - pods</span><br><span class="line">  - nodes</span><br><span class="line">  - nodes/stats</span><br><span class="line">  - namespaces</span><br><span class="line">  - configmaps</span><br><span class="line">  verbs:</span><br><span class="line">  - get</span><br><span class="line">  - list</span><br><span class="line">  - watch</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: system:metrics-server</span><br><span class="line">subjects:</span><br><span class="line"></span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: metrics-server</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f components.yaml </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl get apiservice</span><br><span class="line">NAME                                   SERVICE                      AVAILABLE   AGE</span><br><span class="line">v1.                                    Local                        True        25h</span><br><span class="line">v1.admissionregistration.k8s.io        Local                        True        25h</span><br><span class="line">v1.apiextensions.k8s.io                Local                        True        25h</span><br><span class="line">v1.apps                                Local                        True        25h</span><br><span class="line">v1.authentication.k8s.io               Local                        True        25h</span><br><span class="line">v1.authorization.k8s.io                Local                        True        25h</span><br><span class="line">v1.autoscaling                         Local                        True        25h</span><br><span class="line">v1.batch                               Local                        True        25h</span><br><span class="line">v1.coordination.k8s.io                 Local                        True        25h</span><br><span class="line">v1.networking.k8s.io                   Local                        True        25h</span><br><span class="line">v1.rbac.authorization.k8s.io           Local                        True        25h</span><br><span class="line">v1.scheduling.k8s.io                   Local                        True        25h</span><br><span class="line">v1.storage.k8s.io                      Local                        True        25h</span><br><span class="line">v1beta1.admissionregistration.k8s.io   Local                        True        25h</span><br><span class="line">v1beta1.apiextensions.k8s.io           Local                        True        25h</span><br><span class="line">v1beta1.authentication.k8s.io          Local                        True        25h</span><br><span class="line">v1beta1.authorization.k8s.io           Local                        True        25h</span><br><span class="line">v1beta1.batch                          Local                        True        25h</span><br><span class="line">v1beta1.certificates.k8s.io            Local                        True        25h</span><br><span class="line">v1beta1.coordination.k8s.io            Local                        True        25h</span><br><span class="line">v1beta1.discovery.k8s.io               Local                        True        25h</span><br><span class="line">v1beta1.events.k8s.io                  Local                        True        25h</span><br><span class="line">v1beta1.extensions                     Local                        True        25h</span><br><span class="line">v1beta1.metrics.k8s.io                 kube-system/metrics-server   True        56m</span><br><span class="line">v1beta1.networking.k8s.io              Local                        True        25h</span><br><span class="line">v1beta1.node.k8s.io                    Local                        True        25h</span><br><span class="line">v1beta1.policy                         Local                        True        25h</span><br><span class="line">v1beta1.rbac.authorization.k8s.io      Local                        True        25h</span><br><span class="line">v1beta1.scheduling.k8s.io              Local                        True        25h</span><br><span class="line">v1beta1.storage.k8s.io                 Local                        True        25h</span><br><span class="line">v2beta1.autoscaling                    Local                        True        25h</span><br><span class="line">v2beta2.autoscaling                    Local                        True        25h</span><br></pre></td></tr></table></figure>

<p>6.配置弹性伸缩 编写 deployment-nginx-hpa.yaml 文件，要求最小副本数 1，最大副本数 3， 当整体的资源利用率超过 80%的时候实现自动扩容，将 yaml 文件提交至答题框。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# cat deployment-nginx-hpa.yaml </span><br><span class="line">apiVersion: autoscaling/v2</span><br><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-hpa</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: nginx</span><br><span class="line">  minReplicas: 1</span><br><span class="line">  maxReplicas: 3</span><br><span class="line">  targetCPUUtilizationPercentage: 80</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# kubectl apply -f deployment-nginx-hpa.yaml</span><br><span class="line">[root@master ~]# kubectl get hpa</span><br><span class="line">NAME        REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx-hpa   Deployment/nginx-hpa   0%/80%    1         3         1          159m</span><br></pre></td></tr></table></figure>

<p>7.压力测试 安装 httpd-tools 工具，通过 service 提供的对外访问接口进行压力测试， 验证 HPA 弹性伸缩功能，将 HPA 状态和 POD 状态截图提交至答题框。【2 分】</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[root@master ~]# yum install -y httpd-tools</span><br><span class="line">[root@master ~]# kubectl get pods -o wide</span><br><span class="line">NAME                                    READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">nfs-client-provisioner-8d5b467d-5cl8s   1/1     Running   6          23h   10.244.1.3    node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-665b4cdfdd-48hwn                  1/1     Running   0          23h   10.244.2.2    node2    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx-hpa-78c58b9df7-qjdhh              1/1     Running   2          22h   10.244.0.14   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">[root@master ~]# ab -t 600 -n 1000000 -c 1000 http://10.244.0.14/path</span><br><span class="line">[root@master ~]# kubectl get hpa</span><br><span class="line">NAME        REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">nginx-hpa   Deployment/nginx-hpa   0%/80%    1         3         1          162m</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>容器云</tag>
      </tags>
  </entry>
</search>
